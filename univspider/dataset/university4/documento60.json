{
  "id": 4,
  "date": "2024-05-18T18:15:40.241196",
  "url": "https://www.cs.columbia.edu/technical-reports/",
  "university_name": "Columbia University",
  "title": "Technical Reports",
  "content": "Skip to content Skip to content Columbia University in the City of New York Toggle navigation About About the Department Department Lectures Events Newsletters Directory Research Technical Reports Research in the News Press Interviews Computing Research Facilities Faculty Faculty by Name Affiliates Faculty Achievements Open Positions Academics Undergraduate Programs MS Program PhD Program MS Bridge Program Computer Engineering Program Dual MS in Journalism and Computer Science Program Courses & Registration Academic Honesty Admissions Admissions Information Prospective Student FAQ Affiliated Programs Advising & Student Services Academic Advising Careers Student Awards Student Life and Organizations Research Areas Machine Learning Vision & Robotics Networking Computer Engineering Software Systems Computational Biology Security & Privacy NLP & Speech Theory Graphics & User Interfaces Theory Graphics & User Interfaces NLP & Speech Security & Privacy Computational Biology Software Systems Computer Engineering Networking Vision & Robotics Machine Learning Artificial Intelligence Technical Reports Search By Year All 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 2005 2004 2003 2002 2001 2000 1999 1998 1997 1996 1995 1994 1993 1992 1991 1990 1989 1988 1987 1986 1985 1984 1983 1982 1981 Title Authors Published Abstract Publication Details Computer Vision-Powered Applications for Interpreting and Interacting with Movement Basel Nitham Hindi 2023-12-24 Movement and our ability to perceive it are core elements of the human experience. To bridge the gap between artificial intelligence research and the daily lives of people, this thesis explores leveraging advancements in the field of computer vision to enhance human experiences related to movement. Through two projects, I leverage computer vision to aid Blind and Low Vision (BLV) people in perceiving sports gameplay, and provide navigation assistance for pedestrians in outdoor urban environments. I present Front Row, a system that enables BLV viewers to interpret tennis matches through immersive audio cues, along with StreetNav, a system that repurposes street cameras for real-time, precise outdoor navigation assistance and environmental awareness. User studies and technical evaluations demonstrate the potential of these systems in augmenting people’s experiences perceiving and interacting with movement. This exploration also uncovers challenges in deploying such solutions along with opportunities in the design of future technologies. (pdf) (ps) Advancing Few-Shot Multi-Label Medication Prediction in Intensive Care Units: The FIDDLE-Rx Approach Xinghe Chen 2023-12-04 Contemporary intensive care units (ICUs) are navigating the challenge of enhancing medical service quality amidst financial and resource constraints. Machine learning models have surfaced as valuable tools in this context, showcasing notable effectiveness in supporting healthcare delivery. Despite advancements, a gap remains in real-time medical interventions. To bridge this gap, we introduce FIDDLE-Rx, a novel, data-driven machine learning approach designed specifically for real-time medication recommendations in ICUs. This method leverages the eICU Collaborative Research Database (eICU-CRD) for its analysis, which encompasses diverse electronic health records from ICUs (ICU-EHRs) sourced from multiple critical care centers across the US. FIDDLE-Rx employs the Flexible Data-Driven Pipeline (FIDDLE) for transforming tabular data into binary matrix representations and standardizes medication labels using the RxNorm (Rx) API. With the processed dataset, FIDDLE-Rx applies various machine learning models to forecast the requirements for 238 medications. Compared with previous studies, FIDDLE-Rx stands out by extending the scope of the research of ICU-EHRs beyond mortality prediction, offering a more comprehensive approach to enhancing critical care. The experimental results of our models demonstrate high efficacy, evidenced by their impressive performance across two key metrics: the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). Remarkably, these results were achieved even when the model was trained with just 20% of the database, underlining its strong generalizability. By broadening the scope of ICU-EHRs research to encompass real-time medication recommendations, FIDDLE-Rx presents a scalable and effective solution for improving patient care in intensive care environments. (pdf) (ps) Koopman Constrained Policy Optimization: A Koopman operator theoretic method for differentiable optimal control in robotics Matthew Retchin 2023-05-10 Deep reinforcement learning has recently achieved state-of-the-art results for robotic control. Robots are now beginning to operate in unknown and highly nonlinear environments, expanding their usefulness for everyday tasks. In contrast, classical control theory is not suitable for these unknown, nonlinear environments. However, it retains an immense advantage over traditional deep reinforcement learning: guaranteed satisfaction of hard constraints, which is critically important for the performance and safety of robots.\r\n\r\nThis thesis introduces Koopman Constrained Policy Optimization (KCPO), combining implicitly differentiable model predictive control with a deep Koopman autoencoder. KCPO brings new optimality guarantees to robot learning in unknown and nonlinear dynamical systems.\r\n\r\nThe use of KCPO is demonstrated in Simple Pendulum and Cartpole with continuous state and action spaces and unknown environments. KCPO is shown to be able to train policies end-to-end with hard box constraints on controls. Compared to several baseline methods, KCPO exhibits superior generalization to constraints that were not part of its training. (pdf) (ps) Formal Verification of a Multiprocessor Hypervisor on Arm Relaxed Memory Hardware Runzhou Tao, Jianan Yao, Xupeng Li, Shih-Wei Li, Jason Nieh, Ronghui Gu 2021-06-01 As Arm servers are increasingly used by cloud providers, the\r\ncomplexity of its system software, such as operating systems and\r\nhypervisors, poses a growing security risk, as large codebases contain\r\nmany vulnerabilities. While formal verification offers a potential\r\nsolution for secure concurrent systems software, existing approaches\r\nhave not been able to prove the correctness of systems software on Arm\r\nrelaxed memory hardware. We introduce VRM, a new framework that can be\r\nused to verify kernel-level system software which satisfies a set of\r\nsynchronization and memory access properties such that these programs\r\ncan be mostly verified on a sequentially consistent hardware model and\r\nthe proofs will automatically hold on Arm relaxed memory hardware. VRM\r\ncan be used to verify concurrent kernel code that is not data race\r\nfree, which is typical for kernel code responsible for managing shared\r\npage tables. Using VRM, we prove for the first time the security\r\nguarantees of a retrofitted implementation of the Linux KVM\r\nmultiprocessor hypervisor on Arm. For multiple versions of KVM, we\r\nprove KVM’s security properties on a sequentially consistent model,\r\nthen prove that KVM satisfies VRM’s required program properties such\r\nthat its security proofs hold for Arm relaxed memory hardware. Our\r\nexperimental results across multiple verified KVM versions show that\r\nthe retrofit does not adversely affect the scalability of verified\r\nKVM, as it performs comparably to unmodified KVM when running many\r\nvirtual machines concurrently with real application workloads on Arm\r\nserver hardware. (pdf) (ps) Topics in Landmarking and Elementwise Mapping Mehmet Kerem Turkcan 2021-04-12 In this thesis, we consider a number of different landmarking and elementwise mapping problems and propose solutions that are thematically interconnected with each other. We consider diverse problems ranging from landmarking to deep dictionary learning, pan-sharpening, compressive sensing magnetic resonance imaging and microgrid control, introducing novelties that go beyond the state of the art for the problems we discuss.\r\n\r\nWe start by introducing a manifold landmarking approach trainable via stochastic gradient descent that allows for the consideration of structural regularization terms in the objective. We extend the approach for semi-supervised learning problems, showing that it is able to achieve comparable or better results than equivalent $k$-means based approaches. Inspired by these results, we consider an extension of this approach for general supervised and semi-supervised classification for structurally similar deep neural networks with self-modulating radial basis kernels.\r\n\r\nSecondly, we consider convolutional networks that perform image-to-image mappings for the problems of pan-sharpening and compressive sensing magnetic resonance imaging. Using extensions of deep state of the art image-to-image mapping architectures specifically tailored for these problems, we show that they could be addressed naturally and effectively.\r\n\r\nAfter this, we move on to describe a method for multilayer dictionary learning and feedforward sparse coding by formulating the dictionary learning problem using a general deep learning layer architecture inspired by analysis dictionary learning. We find this method to be significantly faster to train than classical online dictionary learning approaches and capable of addressing supervised and semi-supervised classification problems more naturally. \r\n\r\nLastly, we look at the problem of per-user power supply delivery on a microgrid powered by solar energy. Using real-world data obtained via The Earth Institute, we consider the problem of deciding the amount of power to supply to all each user for a certain period of time given their current power demand as well as past demand/supply data. We approach the problem as one of demand-to-supply mapping, providing results for a policy network trained via regular propagation for worst-case control  and classical deep reinforcement learning. (pdf) (ps) SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic Navigation Yiqing Liang, Boyuan Chen, Shuran Song 2021-04-06 This thesis focuses on visual semantic navigation, the task of producing actions for an active agent to navigate to a specified target object category in an unknown environment. To complete this task, the algorithm should simultaneously locate and navigate to an instance of the category. In comparison to the traditional point goal navigation, this task requires the agent to have a stronger contextual prior to indoor environments. This thesis introduces SSCNav, an algorithm that explicitly models scene priors using a confidence-aware semantic scene completion module to complete the scene and guide the agent’s navigation planning. Given a partial observation of the environment, SSCNav first infers a complete scene representation with semantic labels for the unobserved scene together with a confidence map associated with its own prediction. Then, a policy network infers the action from the scene completion result and confidence map. The experiments demonstrate that the proposed scene completion module improves the efficiency of the downstream navigation policies. Code and data: https://sscnav.cs.columbia.edu/ (pdf) (ps) Semantic Controllable Image Generation in Few-shot Settings Jianjin Xu 2021-04-06 Generative Adversarial Networks (GANs) are able to generate high-quality images, but it remains difficult to explicitly specify the semantics of synthesized images. In this work, we aim to better understand the semantic representation of GANs, and thereby enable semantic control in GAN’s generation process. Interestingly, we find that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way: a linear transformation of feature maps suffices to extract the generated image semantics.\r\nTo verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled images. Last but not least, leveraging our findings, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing. Given an unsupervised GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics. (pdf) (ps) SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic Navigation Yiqing Liang, Boyuan Chen, Shuran Song 2021-03-30 This paper focuses on visual semantic navigation, the task of producing actions for an active agent to navigate to a specified target object category in an unknown environment. \r\nTo complete this task, the algorithm should simultaneously locate and navigate to an instance of the category. In comparison to the traditional point goal navigation, this task requires the agent to have a stronger contextual prior to indoor environments. We introduce SSCNav, an algorithm that explicitly models scene priors using a confidence-aware semantic scene completion module to complete the scene and guide the agent's navigation planning. Given a partial observation of the environment, SSCNav first infers a complete scene representation with semantic labels for the unobserved scene together with a confidence map associated with its own prediction. Then, a policy network infers the action from the scene completion result and confidence map. Our experiments demonstrate that the proposed scene completion module improves the efficiency of the downstream navigation policies. Code and data: https://sscnav.cs.columbia.edu/ (pdf) (ps) Using Continous Logic Networks for Hardware Allocation Anthony Saieva, Dennis Roellke, Suman Jana, Gail Kaiser 2020-08-04 Increased production efficiency combined with a slowdown in Moore's law and the end of Dennard scaling have made hardware accelerators increasingly important. Accelerators have become available on many different systems from the cloud to embedded systems. This modern computing paradigm makes specialized hardware available at scale in a way it never has before. \r\n\r\nWhile accelerators have shown great efficiency in terms of power consumption and performance, matching software functions with the best available hardware remains problematic without manual selection. Since there is some software representation of each accelerator's function, selection can be automated via code analysis. Static similarity analysis has traditionally been based on solving satisfiable modulo theorems (SMT), but continuous logic networks (CLNs) have provided a faster and more efficient alternative to traditional SMT-solving by replacing boolean functions with smooth estimations. These smooth estimates create the opportunity to leverage gradient descent to learn the solution. \r\nWe present AccFinder, the first CLN-based code similarity solution and evaluate its effectiveness on a realistically complex accelerator benchmark. (pdf) (ps) SABER: Identifying SimilAr BEhavioR for Program Comprehension Aditya Sridhar, Guanming Qiao, Gail Kaiser 2020-07-06 Modern software engineering practices rely on program comprehension as the most basic underlying component for improving developer productivity and software reliability. Software developers are often tasked to work with unfamiliar code in order to remove security vulnerabilities, port and refactor legacy code, and enhance software with new features desired by users. Automatic identification of behavioral clones, or behaviorally-similar code, is one program comprehension technique that can provide developers with assistance. The idea is to identify other code that \"does the same thing\" and that may be more intuitive; better documented; or familiar to the developer, to help them understand the code at hand. Unlike the detection of syntactic or structural code clones, behavioral clone detection requires executing workloads or test cases to find code that executes similarly on the same inputs. However, a key problem in behavioral clone detection that has not received adequate attention is the \"preponderance of the evidence\" problem, which advocates for more convincing evidence from nontrivial test case executions to gain confidence in the behavioral similarities. In other words, similar outputs for some inputs matter more than for others. We present a novel system, SABER, to address the \"preponderance of the evidence\" problem, for which we adapt the legal metaphor of \"more likely to be true than not true\" burden of proof. We develop a novel test case generation methodology with three primary dynamic analysis techniques for identifying important behavioral clones. Further, we investigate filtering and weighting schemes to guide developers toward the most convincing behavioral similarities germane to specific software engineering tasks, such as code review, debugging, and introducing new features. (pdf) (ps) A Secure and Formally Verified Linux KVM Hypervisor Shih-Wei Li, Xupeng Li, Ronghui Gu, Jason Nieh, John Zhuang Hui 2020-06-27 Commodity hypervisors are widely deployed to support virtual machines (VMs) on multiprocessor hardware. Their growing complexity poses a security risk. To enable formal verification over such a large code base, we present MicroV, a microverification approach for verifying commodity multiprocessor hypervisors. MicroV retrofits an existing, full-featured hypervisor, into a large collection of untrusted hypervisor services, and a small, verifiable hypervisor core. MicroV introduces security-preserving layers to gradually prove that the implementation of the core refines its high-level layered specification, and ensure that security guarantees proven at the top layer are propagated down through all the layers, such that they hold for the entire implementation. MicroV supports proving noninterference in the sanctioned presence of encrypted data sharing, using data oracles to distinguish between intentional and unintentional information flow. Using MicroV, we retrofitted the Linux KVM hypervisor with only modest modifications to its code base and verify in Coq that the retrofitted KVM protects the confidentiality and integrity of VM data. Our work is the first machine-checked security proof for a commodity multiprocessor hypervisor. (pdf) (ps) Ad hoc Test Generation Through Binary Rewriting Anthony Saieva, Gail Kaiser 2020-04-09 When a security vulnerability or other critical bug is not detected\r\nby the developers’ test suite, and is discovered post-deployment,\r\ndevelopers must quickly devise a new test that reproduces the buggy\r\nbehavior. Then the developers need to test whether their candidate\r\npatch indeed fixes the bug, without breaking other functionality,\r\nwhile racing to deploy before cyberattackers pounce on exposed\r\nuser installations. This can be challenging when the bug discovery\r\nwas due to factors that arose, perhaps transiently, in a specific user\r\nenvironment. If recording execution traces when the bad behavior\r\noccurred, record-replay technology faithfully replays the execution,\r\nin the developer environment, as if the program were executing\r\nin that user environment under the same conditions as the bug\r\nmanifested. This includes intermediate program states dependent\r\non system calls, memory layout, etc. as well as any externally-visible\r\nbehavior. So the bug is reproduced, and many modern record-replay\r\ntools also integrate bug reproduction with interactive debuggers to\r\nhelp locate the root cause, but how do developers check whether\r\ntheir patch indeed eliminates the bug under those same conditions?\r\nState-of-the-art record-replay does not support replaying candidate patches that modify the program in ways that diverge program\r\nstate from the original recording, but successful repairs necessarily\r\ndiverge so the bug no longer manifests. This work builds on recordreplay, and binary rewriting, to automatically generate and run\r\ntests for candidate patches. These tests reflect the arbitrary (ad hoc)\r\nuser and system circumstances that uncovered the vulnerability, to\r\ncheck whether a patch indeed closes the vulnerability but does not\r\nmodify the corresponding segment of the program’s core semantics. Unlike conventional ad hoc testing, each test is reproducible\r\nand can be applied to as many prospective patches as needed until\r\ndevelopers are satisfied. The proposed approach also enables users\r\nto make new recordings of her own workloads with the original\r\nversion of the program, and automatically generate and run the\r\ncorresponding ad hoc tests on the patched version, to validate that\r\nthe patch does not introduce new problems before adopting. (pdf) (ps) Privacy Threats from Seemingly Innocuous Sensors Shirish Singh, Anthony Saieva, Gail Kaiser 2020-04-09 Smartphones incorporate a plethora of diverse and powerful sensors that enhance user experience. Two such sensors are the accelerometer and gyroscope, which measure acceleration in all three spatial dimensions and rotation along the three axes of the smartphone, respectively. These sensors are often used by gaming and fitness apps. Unlike other sensors deemed to carry sensitive user data, such as GPS, camera, and microphone, the accelerometer and gyroscope do not require user permission on Android to transmit data to apps. This paper presents our IRB-approved studies showing that the accelerometer and gyroscope gather sufficient data to quickly infer the user's gender. We started with 33 in-person participants, with 88% accuracy, and followed up with 259 on-line participants to show the effectiveness of our technique. Our unobtrusive ShakyHands technique draws on these sensors to deduce additional demographic attributes that might be considered sensitive information, notably pregnancy. We have implemented ShakyHands for Android as an app, available from Google Play store, and as a Javascript browser web-app for Android and iOS smartphones. We show that even a low-skilled attacker, without expertise in signal processing or deep learning, can succeed at inferring demographic information such as gender and pregnancy. Our approach does not require tampering with the victim's device or specialized hardware; all our study participants used their own phones. (pdf) (ps) The FHW Project: High-Level Hardware Synthesis from Haskell Programs Stephen A. Edwards 2019-08-04 The goal of the FHW project was to produce a compiler able to translate programs written in\r\na functional language (we chose Haskell) into synthesizable RTL (we chose SystemVerilog)\r\nsuitable for execution on an FPGA or ASIC that was highly parallel. We ultimately produced\r\nsuch a compiler, relying on the Glasgow Haskell Compiler (GHC) as a front-end and writing\r\nour own back-end that performed a series of lowering transformations to restructure such\r\nconstructs as recursion, polymorphism, and frst-order functions, into a form suitable for\r\nhardware, then transform the now-restricted functional IR into a datafow representation that\r\nis then finally transformed into synthesizable SystemVerilog. (pdf) (ps) Compiling Irregular Software to Specialized Hardware Richard Townsend 2019-06-05 High-level synthesis (HLS) has simplified the design process for energy-efficient hardware accelerators: a designer specifies an accelerator’s behavior in a “high-level” language, and a toolchain synthesizes register-transfer level (RTL) code from this specification. Many HLS systems produce efficient hardware designs for regular algorithms (i.e., those with limited conditionals or regular memory access patterns), but most struggle with irregular algorithms that rely on dynamic, data-dependent memory access patterns (e.g., traversing pointer-based structures like lists, trees, or graphs). HLS tools typically provide imperative, side-effectful languages to the designer, which makes it difficult to correctly specify and optimize complex, memory-bound\r\napplications.\r\n\r\nIn this dissertation, I present an alternative HLS methodology that leverages properties of functional languages to synthesize hardware for irregular algorithms. The main contribution is an optimizing compiler that translates pure functional programs into modular, parallel dataflow networks in hardware. I give an overview of this compiler, explain how its source and target together enable parallelism in the face of irregularity, and present two specific optimizations that further exploit this parallelism. Taken together, this dissertation verifies my thesis that pure functional programs exhibiting irregular memory access patterns can be compiled into specialized hardware and optimized for parallelism.\r\n\r\nThis work extends the scope of modern HLS toolchains. By relying on properties of pure functional languages, our compiler can synthesize hardware from programs containing constructs that commercial HLS tools prohibit, e.g., recursive functions and dynamic memory allocation. Hardware designers may thus use our compiler in conjunction with existing HLS systems to accelerate a wider class of algorithms than before. (pdf) (ps) Extractive Text Summarization Methods Inspired By Reinforcement Learning for Better Generalization Yan Virin 2019-05-23 This master thesis opens with a description of several text summarization methods based\r\non machine learning approaches inspired by reinforcement learning. While in many cases\r\nMaximum Likelihood Estimation (MLE) approaches work well for text summarization, they\r\ntend to suffer from poor generalization. We show that techniques which expose the model\r\nto more opportunities to learn from data tend to generalize better and generate summaries\r\nwith less lead bias. In our experiments we show that out of the box these new models do\r\nnot perform significantly better than MLE when evaluated using Rouge, however do possess\r\ninteresting properties which may be used to assemble more sophisticated and better performing\r\nsummarization systems.\r\nThe main theme of the thesis is getting machine learning models to generalize better using\r\nideas from reinforcement learning. We develop a new labeling scheme inspired by Reward\r\nAugmented Maximum Likelihood (RAML) methods developed originally for the machine\r\ntranslation task, and discuss how difficult it is to develop models which sample from their\r\nown distribution while estimating the gradient e.g. in Minimum Risk Training (MRT) and\r\nReinforcement Learning Policy Gradient methods. We show that RAML can be seen as a\r\ncompromise between direct optimization of the model towards optimal expected reward using\r\nMonte Carlo methods which may fail to converge, and standard MLE methods which fail to\r\nexplore the entire space of summaries, overfit during training by capturing prominent position\r\nfeatures and thus perform poorly on unseen data.\r\nTo that end we describe and show results of domain transfer experiments, where we train\r\nthe model on one dataset and evaluate on another, and position distribution experiments, in\r\nwhich we show how the distribution of positions of our models differ from the distribution in\r\nMLE. We also show that our models work better on documents which are less lead biased,\r\nwhile standard MLE models get significantly worse performance on those documents in particular.\r\nAnother topic covered in the thesis is Query Focused text summarization, where a search\r\nquery is used to produce a summary with the query in mind. The summary needs to be relevant\r\nto the query, rather than solely contain important information from the document. We use\r\nii\r\nthe recently published Squad dataset and adapt it for the Query Focused summarization task.\r\nWe also train deep learning Query Focused models for summarization and discuss problems\r\nassociated with that approach. Finally we describe a method to reuse an already trained QA\r\nmodel for the Query Focused text summarization by introducing a reduction of the QA task\r\ninto the Query Focused text summarization. The source code in python for all the techniques\r\nand approaches described in this thesis are available at https://github.com/yanvirin/material. (pdf) (ps) Easy Email Encryption with Easy Key Management John S. Koh, Steven M. Bellovin, Jason Nieh 2018-10-02 Email privacy is of crucial importance. Existing email encryption \r\napproaches are comprehensive but seldom used due to their complexity \r\nand inconvenience. We take a new approach to simplify email encryption \r\nand improve its usability by implementing receiver-controlled \r\nencryption: newly received messages are transparently downloaded and \r\nencrypted to a locally-generated key; the original message is then \r\nreplaced. To avoid the problem of users having to move a single private \r\nkey between devices, we implement per-device key pairs: only public \r\nkeys need be synchronized to a single device. Compromising an email account \r\nor email server only provides access to encrypted emails. We have \r\nimplemented this scheme for both Android and as a standalone daemon; we \r\nshow that it works with both PGP and S/MIME, is compatible with widely used \r\nemail services including Gmail and Yahoo! Mail, has acceptable overhead, \r\nand that users consider it intuitive and easy to use. (pdf) (ps) Analysis of the CLEAR Protocol per  the National Academies' Framework Steven M. Bellovin, Matt Blaze, Dan Boneh, Susan Landau, Ronald L. Rivest 2018-05-10 The debate over \"exceptional access\"--the government’s ability to read encrypted data--has been going on for many years and shows no signs of resolution any time soon. On the one hand, some people claim it can be accomplished safely; others dispute that. In an attempt to make progress, a National Academies study committee propounded a framework to use when analyzing proposed solutions. We apply that framework to the CLEAR protocol and show the limitations of the design. (pdf) (ps) Robot Learning in Simulation for Grasping and Manipulation Beatrice Liang 2018-05-10 Teaching a robot to acquire complex motor skills in complicated environments is one of the most ambitious problems facing roboticists today. Grasp planning is a subset of this problem which can be solved through complex geometric and physical analysis or computationally expensive data driven analysis. As grasping problems become more difficult, building analytical models becomes challenging. Consequently, we aim to learn a grasping policy through a simulation-based data driven approach. In this paper, we create and execute tests to evaluate simulator’s suitability for manipulating objects in highly constrained settings. We investigate methods for creating forward models of a robot’s dynamics, and apply a Model Free Reinforcement Learning approach with the goal of developing a grasping policy based solely on proprioception. (pdf) (ps) Partial Order Aware Concurrency Sampling Xinhao Yuan, Junfeng Yang, Ronghui Gu 2018-04-15 We present POS, a concurrency testing approach that directly samples the partial orders of a concurrent program. POS uses a novel priority-based scheduling algorithm that naturally considers partial order information dynamically, and guarantees that each partial order will be explored with significant probability. This probabilistic guarantee of error detection is exponentially better than state-of-the-art sampling approaches. Besides theoretical guarantees, POS is extremely simple and lightweight to implement. Evaluations show that POS is effective in covering the partial-order space of micro-benchmarks and finding concurrency bugs in real-world programs such as Firefox’s JavaScript engine SpiderMonkey. (pdf) (ps) Stretchcam: zooming using thin, elastic optics Daniel Sims, Oliver Cossairt, Yonghao Yue, Shree Nayar 2017-12-31 Stretchcam is a thin camera with a lens capable of zooming with small actuations.\r\nIn our design, an elastic lens array is placed on top of a sparse, rigid array of pixels. This lens\r\narray is then stretched using a small mechanical motion in order to change the field of view\r\nof the system. We present in this paper the characterization of such a system and simulations\r\nwhich demonstrate the capabilities of stretchcam. We follow this with the presentation of images\r\ncaptured from a prototype device of the proposed design. Our prototype system is able to achieve\r\n1.5 times zoom when the scene is only 300 mm away with only a 3% change of the lens array’s\r\noriginal length. (pdf) (ps) Design and Implementation of IoT Android Commissioner Andy Lianghua Xu, Jan Janak, Henning Schulzrinne 2017-09-21 As Internet of Things (IoT) devices gain more popularity, device management gradually becomes a major issue to IoT device users. To manage an IoT device, the user first needs to join it to an existing network. Then, the IoT device has to be authenticated by the user. The authentication process often requires a two-way communication between the new device and a trusted entity, which is typically a hand- held device owned by the user. To ease and standardize this process, we present the Device Enrollment Protocol (DEP) as a solution to the enrollment problem described above. Starting from DEP, we then showcase the design of an IoT device commissioner and its prototype implementation on Android, named Android Commissioner. The application allows the user to authenticate IoT devices and join them to an existing protected network. (pdf) (ps) Searching for Meaning in RNNs using Deep Neural Inspection Kevin Lin, Eugene Wu 2017-06-01 Recent variants of Recurrent Neural Networks (RNNs)---in particular, Long Short-Term Memory (LSTM) networks---have established RNNs as a deep learning staple in modeling sequential data in a variety of machine learning tasks. However, RNNs are still often used as a black box with limited understanding of the hidden representation that they learn.  Existing approaches such as visualization are limited by the manual effort to examine the visualizations and require considerable expertise, while neural attention models change, rather than interpret, the model. We propose a technique to search for neurons based on existing interpretable models, features, or programs. (pdf) Reliable Synchronization in Multithreaded Servers Rui Gu 2017-05-15 State machine replication (SMR) leverages distributed consensus protocols such as PAXOS to keep multiple replicas of a program consistent in face of replica failures or network partitions. This fault tolerance is enticing on implementing a principled SMR system that replicates general programs, especially server programs that demand high availability. Unfortunately, SMR assumes deterministic execution, but most server programs are multithreaded and thus non-deterministic. Moreover, existing SMR systems provide narrow state machine interfaces to suit specific programs, and it can be quite strenuous and error-prone to orchestrate a general program into these interfaces This paper presents CRANE, an SMR system that trans- parently replicates general server programs. CRANE achieves distributed consensus on the socket API, a common interface to almost all server programs. It leverages deterministic multithreading (specifically, our prior system PARROT) to make multithreaded replicas deterministic. It uses a new technique we call time bubbling to efficiently tackle a difficult challenge of non-deterministic network input timing. Evaluation on five widely used server programs (e.g., Apache, ClamAV, and MySQL) shows that CRANE is easy to use, has moderate overhead, and is robust. (pdf) Deobfuscating Android Applications through Deep Learning Fang-Hsiang Su, Jonathan Bell, Gail Kaiser, Baishakhi Ray 2017-05-12 Android applications are nearly always obfuscated before release, making it difficult to analyze them for malware presence or intellectual property violations. Obfuscators might hide the true intent of code by renaming variables, modifying the control flow of methods, or inserting additional code. Prior approaches toward automated deobfuscation of Android applications have relied on certain structural parts of apps remaining as landmarks, un-touched by obfuscation. For instance, some prior approaches have assumed that the structural relation- ships between identifiers (e.g. that A represents a class, and B represents a field declared directly in A) are not broken by obfuscators; others have assumed that control flow graphs maintain their structure (e.g. that no new basic blocks are added). Both approaches can be easily defeated by a motivated obfuscator. We present a new approach to deobfuscating Android apps that leverages deep learning and topic modeling on machine code, MACNETO. MACNETO makes few assumptions about the kinds of modifications that an obfuscator might perform, and we show that it has high precision when applied to two different state-of-the-art obfuscators: ProGuard and Allatori. (pdf) (ps) Analysis of Super Fine-Grained Program Phases Van Bui, Martha A. Kim 2017-04-18 Dynamic reconfiguration systems guided by coarse-grained program phases has found success in improving overall program performance and energy efficiency. These performance/energy savings are limited by the granularity that program phases are detected since phases that occur at a finer granularity goes undetected and reconfiguration opportunities are missed.  In this study, we detect program phases using interval sizes on the order of tens, hundreds, and thousands of program cycles. This is in stark contrast with prior phase detection studies where the interval size is on the order of several thousands to millions of cycles.  The primary goal of this study is to begin to fill a gap in the literature on phase detection by characterizing super fine-grained program phases and demonstrating an application where detection of these relatively short-lived phases can be instrumental. Traditional models for phase detection including basic block vectors and working set signatures are used to detect super fine-grained phases as well as a less traditional model based on microprocessor activity. Finally, we show an analytical case study where super fine-grained phases are applied to voltage and frequency scaling optimizations. (pdf) Understanding and Detecting Concurrency Attacks Rui Gu, Bo Gan, Jason Zhao, Yi Ning, Heming Cui, Junfeng Yang 2016-12-30 Just like bugs in single-threaded programs can lead to vulnerabilities, bugs in multithreaded programs can also lead to concurrency attacks. Unfortunately, there is little quantitative data on how well existing tools can detect these attacks. This paper presents the first quantitative study on concurrency attacks and their implications on tools. Our study on 10 widely used programs reveals 26 concurrency attacks with broad threats (e.g., OS privilege escalation), and we built scripts to successfully exploit 10 attacks. Our study further reveals that, only extremely small portions of inputs and thread interleavings (or schedules) can trigger these attacks, and existing concurrency bug detectors work poorly because they lack help to identify the vulnerable inputs and schedules. Our key insight is that the reports in existing detectors have implied moderate hints on what inputs and schedules will likely lead to attacks and what will not (e.g., benign bug reports). With this insight, this paper presents a new directed concurrency attack detection approach and its implementation, OWL. It extracts hints from the reports with static analysis, augments existing detectors by pruning out the benign inputs and schedules, and then directs detectors and its own runtime vulnerability verifiers to work on the remaining, likely vulnerable inputs and schedules. Evaluation shows that OWL reduced 94.3% reports caused by benign inputs or schedules and detected 7 known concurrency attacks. OWL also detected 3 previously unknown concurrency attacks, including a use-after-free attack in SSDB confirmed as CVE-2016-1000324, an integer overflow, HTML integrity violation in Apache and three new MySQL data races confirmed with bug ID 84064, 84122, 84241. All OWL source code, exploit scripts, and results are available at https://github.com/ruigulala/ConAnalysis. (pdf) (ps) Mysterious Checks from Mauborgne to Fabyan Steven M. Bellovin 2016-11-28 It has long been known that George Fabyan's Riverbank Laboratories provided the U.S. military with cryptanalytic and training services during World War I. The relationship has always be seen as voluntary.  Newly discovered evidence raises the question of whether Fabyan was in fact paid, at least in part, for his services, but available records do not provide a definitive answer. (pdf) Further Information on Miller's 1882 One-Time Pad Steven M. Bellovin 2016-11-25 New information has been discovered about Frank Miller's 1882 one-time pad.  These documents explain Miller's threat model and show that he had a reasonably deep understanding of the problem; they also suggest that his scheme was used more than had been supposed. (pdf) Kensa: Sandboxed, Online Debugging of Production Bugs with No Overhead Nipun Arora, Jonathan Bell, Franjo Ivancic, Gail Kaiser 2016-10-27 Short time-to-bug localization and resolution is extremely important for any 24x7 service-oriented application. In this work, we present a novel-mechanism which allows debugging of production systems on-the-fly. We leverage user-space virtualization technology (OpenVZ/LXC), to launch replicas from running instances of a production application, thereby having two containers: prod uction (which provides the real output), and debug (for debugging). The debug container provides a sandbox environment for debugging without any perturbation to the production environment. Customized network-proxy agents asynchronously replicate and replay network inputs from clients to both the production and debug-container, as well as safely discard all network output from the debug-container. We evaluated this low-overhead record and replay technique on five real-world applications, finding that it was effective at reproducing real bugs. In comparison to existing monitoring solutions which can slow-down production applications, Kensa allows application monitoring at â€œzero-overheadâ€. (pdf) (ps) Discovering Functionally Similar Code with Taint Analysis Fang-Hsiang Su, Jonathan Bell, Gail Kaiser, Simha Sethumadhavan 2016-09-30 Identifying similar code in software systems can assist many software engineering tasks such as program understanding and software refactoring. While most approaches focus on identifying code that looks alike, some techniques aim at detecting code that functions alike. Detecting these functional clones â€” code that functions alike â€” in object oriented languages remains an open question because of the difficulty in exposing and comparing programsâ€™ functionality effectively, in general cases undecidable. We propose a novel technique, In-Vivo Clone Detection, which detects functional clones in arbitrary programs by identifying and mining their inputs and outputs. The key insight is to use existing workloads to execute programs and then measure functional similarities between programs based on their inputs and outputs. Further, to identify inputs and outputs of programs appropriately, we use the techniques of static and dynamic data flow analysis. These enhancements mitigate the problems in object oriented languages with respect to identifying program I/Os as reported by prior work. We implement such techniques in our system, HitoshiIO, which is open source and freely available. Our experimental results show that HitoshiIO detects âˆ¼ 900 and âˆ¼ 2, 000 functional clones by static and dynamic data flow analysis, respectively, across a corpus of 118 projects. In a random sample of the detected clones by the static data flow analysis, HitoshiIO achieves 68+% true positive rate with only 15% false positive rate. (pdf) (ps) Heterogeneous Multi-Mobile Computing Naser AlDuaij, Alexander Van't Hof, Jason Nieh 2016-08-02 As smartphones and tablets proliferate, there is a growing need to provide ways to combine multiple mobile systems into more capable ones, including using multiple hardware devices such as cameras, displays, speakers, microphones, sensors, and input. We call this multi-mobile computing. However, the tremendous device, hardware, and software heterogeneity of mobile systems makes this difficult in practice. We present M2, a system for multi-mobile computing across heterogeneous mobile systems that enable new ways of sharing and combining multiple devices. M2 leverages higher-level device abstractions and encoding and decoding hardware in mobile systems to define a client-server device stack that shares devices seamlessly across heterogeneous systems. M2 introduces device transformation, a new technique to mix and match heterogeneous input and output device data including rich media content. Example device transformations for transparently making existing unmodified apps multi-mobile include fused devices, which combine multiple devices into a more capable one, and translated devices, which can substitute use of one type of device for another. We have implemented an M2 prototype on Android that operates across heterogeneous hardware and software, including multiple versions of Android and iOS devices, the latter allowing iOS users to also run Android apps. Our results using unmodified apps from Google Play show that M2 can enable apps to be combined in new ways, and can run device-intensive apps across multiple mobile systems with modest overhead and qualitative performance indistinguishable from using local device hardware. (pdf) (ps) Why Are We Permanently Stuck in an Elevator? A Software Engineering Perspective on Game Bugs Iris Zhang 2016-06-01 In the past decade, the complexity of video games have increased dramatically and so have the complexity of software systems behind them. The difficulty in designing and testing games invariably leads to bugs that manifest themselves across funny video reels on graphical glitches and millions of submitted support tickets. This paper presents an analysis of game developers and their teams who have knowingly released bugs to see what factors may motivate them in doing so. It examines different development environments as well as inquiring into varied types of game platforms and play-style. Above all, it seeks out how established research on software development best practices and challenges should inform understanding of these bugs. These findings may lead to targeted efforts to mitigate some of the factors leading to glitches, tailored to the specific needs of the game development team. (pdf) Software Engineering Methodologies and Life Scott Lennon 2016-06-01 The paradigms of design patterns and software engineering methodologies are methods that apply to areas outside the software space. As a business owner and student, I implement many software principles daily in both my work and personal life. After experiencing the power of Agile methodologies outside the scope of software engineering, I always think about how I can integrate the computer science skills that I am learning at Columbia in my life. For my study, I seek to learn about other software engineering development processes that can be useful in life. I theorize that if a model such as Agile can provide me with useful tools, then a model that the government and most of the world trusts should have paradigms I can learn with as well. The software model I will study is open source software (OSS). My research examines the lateral software standards of (OSS) and closed source software (CSS). For the scope of this paper, I will focus on research primarily on Linux as the OSS model and Agile as the CSS model. OSS has had an extraordinary impact on the software revolution [1], and CSS models have gained such popularity that itâ€™s paradigms extend far beyond the software engineering space. Before delving into research, I thought the methodologies of OSS and CSS would be radically different. My study shall describe the similarities that exist between these two methodologies. In the process of my research, I was able to implement the values and paradigms that define the OSS development model to work more productively in my business. Software engineering core values and models can be used as a tool to improve our lives. (pdf) User Study: Programming Understanding from Similar Code Anush Ramsurat 2016-06-01 The aim of the user study conducted is primarily threefold:\r\nâ€¢\tTo accurately judge, based on a number of parameters, whether showing similar code helps in code comprehension.\r\nâ€¢\tTo investigate separately, a number of cases involving dynamic code, static code, the effect of options on accuracy of responses, and so on.\r\nâ€¢\tTo distribute the user survey, collect data, responses and feedback from the user study and draw conclusions. (pdf) YOLO: A New Security Architecture for Cyber-Physical Systems Miguel Arroyo, Jonathan Weisz, Simha Sethumadhavan, Hidenori Kobayashi, Junfeng Yang 2016-05-24 Cyber-physical systems (CPS) are defined by their unique characteristics involving both the cyber and physical domains. Their hybrid nature introduces new attack vectors but also provides an opportunity to design of new security architectures. In this work, we present YOLO,--- You Only Live Once --- a security architecture that leverages two unique physical properties of a CPS, inertia: the tendency of objects to stay at rest or in motion, and its built-in reliability to intermittent faults to survive CPS attacks. \r\n\r\nAt a high level, YOLO aims to use a new diversified variant for every new sensor input to the CPS'.  The delays involved in YOLO, viz., the delays for rebooting and diversification,  are easily absorbed by the CPS because of the inherent inertia and their ability to withstand minor perturbations.  We implement YOLO on an open source Car Engine Control Unit, and with measurements from a real race car engine show that YOLO is imminently practical. (pdf) (ps) Identifying Functionally Similar Code in Complex Codebases Fang-Hsiang Su, Jonathan Bell, Gail Kaiser, Simha Sethumadhavan 2016-02-18 Identifying similar code in software systems can assist many software engineering tasks, including program understand- ing. While most approaches focus on identifying code that looks alike, some researchers propose to detect instead code that functions alike, which are known as functional clones. However, previous work has raised the technical challenges to detect these functional clones in object oriented languages such as Java. We propose a novel technique, In-Vivo Clone Detection, a language-agnostic technique that detects functional clones in arbitrary programs by observing and mining inputs and outputs. We implemented this technique targeting programs that run on the JVM, creating HitoshiIO (available freely on GitHub), a tool to detect functional code clones. Our experimental results show that it is powerful in detecting these functional clones, finding 185 methods that are functionally similar across a corpus of 118 projects, even when there are only very few inputs available. In a random sample of the detected clones, HitoshiIO achieves 68+% true positive rate, while the false positive rate is only 15%. (pdf) Cambits: A Reconfigurable Camera System Makoto Odamaki, Shree K. Nayar 2016-02-11 Cambits is a set of physical blocks that can be used to build a wide variety of cam-eras with different functionalities. A unique feature of Cambits is that it is easy and quick to reconfigure. The blocks are assembled using magnets, without any screws or cables. When two blocks are attached, they are electrically connected by spring-loaded pins that carry power, data and control signals. With this novel architecture we can use Cambits to configure various imaging systems. The host computer al-ways knows the current configuration and presents the user with a menu of functionalities that the configuration can perform. We demonstrate a wide range of computational photography methods including HDR, wide angle, panoramic, collage, kaliedoscopic, post-focus, light field and stereo imaging. Cambits can even be used to configure a microscope. Cambits is a scalable system, allowing new blocks and accompanying software to be added to the existing set. (pdf) (ps) Grandet: A Unified, Economical Object Store for Web Applications Yang Tang, Gang Hu, Xinhao Yuan, Lingmei Weng, Junfeng Yang 2016-02-02 Web applications are getting ubiquitous every day because they offer many useful services to consumers and businesses. Many of these web applications are quite storage-intensive. Cloud computing offers attractive and economical choices for meeting their storage needs. Unfortunately, it remains challenging for developers to best leverage them to minimize cost. This paper presents Grandet, a storage system that greatly reduces storage cost for web applications deployed in the cloud. Grandet provides both a key-value interface and a file system interface, supporting a broad spectrum of web applications. Under the hood, it supports multiple heterogeneous stores, and unifies them by placing each data object at the store deemed most economical. We implemented Grandet on Amazon Web Services and evaluated Grandet on a diverse set of four popular open-source web applications. Our results show that Grandet reduces their cost by an average of 42.4%, and it is fast, scalable, and easy to use. The source code of Grandet is at http://columbia.github.io/grandet. (pdf) A Measurement Study of ARM Virtualization Performance Christoffer Dall, Shih-Wei Li, Jintack Lim, Jason Nieh 2015-11-30 ARM servers are becoming increasingly common, making server technologies such as virtualization for ARM of grow- ing importance. We present the first in-depth study of ARM virtualization performance on ARM server hardware, including measurements of two popular ARM and x86 hypervisors, KVM and Xen. We show how the ARM hardware support for virtualization can support much faster transitions between the VM and the hypervisor, a key hypervisor operation. However, current hypervisor designs, including both KVM (Type 1) and Xen (Type 2), are not able to lever- age this performance benefit in practice for real application workloads. We discuss the reasons why and show that other factors related to hypervisor software design and implementation have a larger role in overall performance than the speed of micro architectural operations. Based on our measurements, we discuss changes to ARMâ€™s hardware virtualization support that can potentially bridge the gap to bring its faster virtual machine exit mechanism to modern Type 2 hypervisors running real applications. These changes have been incorporated into the latest ARM architecture. (pdf) Use of Fast Multipole to Accelerate Discrete Circulation-Preserving Vortex Sheets for Soap Films and Foams Fang Da, Christopher Batty, Chris Wojtan, Eitan Grinspun 2015-11-07 We report the integration of a FMM (Fast Multipole Method) template library â€œFMMTLâ€ into the discrete circulation-preserving vortex sheets method to accelerate the Biot-Savart integral. We measure the speed-up on a bubble oscillation test with varying mesh resolution. We also report a few examples with higher complexity than previously achieved. (pdf) Hardware in Haskell: Implementing Memories in a Stream-Based World Richard Townsend, Martha Kim, Stephen Edwards 2015-09-21 Recursive functions and data types pose significant challenges for a Haskell-to-hardware compiler. Directly translating these structures yields infinitely large circuits; a subtler approach is required. We propose a sequence of abstraction-lowering transformations that exposes time and memory in a Haskell program. producing a simpler form for hardware translation. This paper outlines these transformations on a specific example; future research will focus on generalizing and automating them in our group's compiler. (pdf) Improving System Reliability for Cyber-Physical Systems Leon Wu 2015-09-14 Cyber-physical systems (CPS) are systems featuring a tight combination of, and coordination between, the systemâ€™s computational and physical elements. Cyber-physical systems include systems ranging from critical infrastructure such as a power grid and transportation system to health and biomedical devices. System reliability, i.e., the ability of a system to perform its intended function under a given set of environmental and operational conditions for a given period of time, is a fundamental requirement of cyber-physical systems. An unreliable system often leads to disruption of service, financial cost and even loss of human life. An important and prevalent type of cyber-physical system meets the following criteria: processing large amounts of data; employing software as a system component; running online continuously; having operator-in-the-loop because of human judgment and an accountability requirement for safety critical systems. This thesis aims to improve system reliability for this type of cyber-physical system.\r\n\r\nTo improve system reliability for this type of cyber-physical system, I present a system evaluation approach entitled automated online evaluation (AOE), which is a data-centric runtime monitoring and reliability evaluation approach that works in parallel with the cyber-physical system to conduct automated evaluation along the workflow of the system continuously using computational intelligence and self-tuning techniques and provide operator-in-the-loop feedback on reliability improvement. For example, abnormal input and output data at or between the multiple stages of the system can be detected and flagged through data quality analysis. As a result, alerts can be sent to the operator-in-the-loop. The operator can then take actions and make changes to the system based on the alerts in order to achieve minimal system downtime and increased system reliability. One technique used by the approach is data quality analysis using computational intelligence, which applies computational intelligence in evaluating data quality in an automated and efficient way in order to make sure the running system perform reliably as expected. Another technique used by the approach is self-tuning which automatically self-manages and self-configures the evaluation system to ensure that it adapts itself based on the changes in the system and feedback from the operator. To implement the proposed approach, I further present a system architecture called autonomic reliability improvement system (ARIS).\r\n\r\nThis thesis investigates three hypotheses. First, I claim that the automated online evaluation empowered by data quality analysis using computational intelligence can effectively improve system reliability for cyber-physical systems in the domain of interest as indicated above. In order to prove this hypothesis, a prototype system needs to be developed and deployed in various cyber-physical systems while certain reliability metrics are required to measure the system reliability improvement quantitatively. Second, I claim that the self-tuning can effectively self-manage and self-configure the evaluation system based on the changes in the system and feedback from the operator-in-the-loop to improve system reliability. Third, I claim that the approach is effcient. It should not have a large impact on the overall system performance and introduce only minimal extra overhead to the cyberphysical system. Some performance metrics should be used to measure the effciency and added overhead quantitatively.\r\n\r\nAdditionally, in order to conduct efficient and cost-effective automated online evaluation for data-intensive CPS, which requires large volumes of data and devotes much of its processing time to I/O and data manipulation, this thesis presents COBRA, a cloud-based reliability assurance framework. COBRA provides automated multi-stage runtime reliability evaluation along the CPS workflow using data relocation services, a cloud data store, data quality analysis and process scheduling with self-tuning to achieve scalability, elasticity and efficiency.\r\n\r\nFinally, in order to provide a generic way to compare and benchmark system reliability for CPS and to extend the approach described above, this thesis presents FARE, a reliability benchmark framework that employs a CPS reliability model, a set of methods and metrics on evaluation environment selection, failure analysis, and reliability estimation.\r\n\r\nThe main contributions of this thesis include validation of the above hypotheses and empirical studies of ARIS automated online evaluation system, COBRA cloud-based reliability assurance framework for data-intensive CPS, and FARE framework for benchmarking reliability of cyber-physical systems. This work has advanced the state of the art in the CPS reliability research, expanded the body of knowledge in this field, and provided some useful studies for further research. (pdf) Exploiting Visual Perception for Sampling-Based Approximation on Aggregate Queries Daniel Alabi 2015-09-07 Efficient sampling algorithms have been developed for approximating answers to aggregate queries on large data sets. In some formulations of the problem, concentration inequalities (such as Hoeffdingâ€™s inequality) are used to estimate the confidence interval for an approximated aggregated value. Samples are usually chosen until the confidence interval is arbitrarily small enough regardless of how the approximated query answers will be used (for example, in interactive visualizations). In this report, we show how to exploit visualization-specific properties to reduce the sampling complexity of a sampling-based approximate query processing algorithm while preserving certain visualization guarantees (the visual property of relative ordering) with a very high probability. (pdf) Code Relatives: Detecting Similar Software Behavior Fang-Hsiang Su, Kenneth Harvey, Simha Sethumadhavan, Gail Kaiser, Tony Jebara 2015-08-28 Detecting â€œsimilar codeâ€ is fundamental to many software engineering tasks. Current tools can help detect code with statically similar syntactic features (code clones). Unfortunately, some code fragments that behave alike without similar syntax may be missed. In this paper, we propose the term â€œcode relativesâ€ to refer to code with dynamically similar execution features. Code relatives can be used for such tasks as implementation-agnostic code search and classification of code with similar behavior for human understanding, which code clone detection cannot achieve. To detect code relatives, we present DyCLINK, which constructs an approximate runtime representation of code using a dynamic instruction graph. With our link analysis based subgraph matching algorithm, DyCLINK detects fine-grained code relatives efficiently. In our experiments, DyCLINK analyzed 290+ million prospective subgraph matches. The results show that DyCLINK detects not only code relatives, but also code clones that the state-of-the-art system is unable to identify. In a code classification problem, DyCLINK achieved 96% precision on average compared with the competitorâ€™s 61%. (pdf) Dynamic Taint Tracking for Java with Phosphor (Formal Tool Demonstration) Jonathan Bell, Gail Kaiser 2015-04-07 Dynamic taint tracking is an information flow analysis that can be applied to many areas of testing. Phosphor is the first portable, accurate and performant dynamic taint tracking system for Java. While previous systems for performing general-purpose taint tracking in the JVM required specialized research JVMs, Phosphor works with standard off-the-shelf JVMs (such as Oracle's HotSpot and OpenJDK's IcedTea). Phosphor also differs from previous portable JVM taint tracking systems that were not general purpose (e.g. tracked only tags on Strings and no other type), in that it tracks tags on all variables. We have also made several enhancements to Phosphor, allowing it to track taint tags through control flow (in addition to data flow), as well as allowing it to track an arbitrary number of relationships between taint tags (rather than be limited to only 32 tags). In this demonstration, we show how developers writing testing tools can benefit from Phosphor, and explain briefly how to interact with it. (pdf) Hardware Synthesis from a Recursive Functional Language Kuangya Zhai, Richard Townsend, Lianne Lairmore, Martha A. Kim, Stephen A. Edwards 2015-04-01 Abstraction in hardware description languages stalled at the\r\nregister-transfer level decades ago, yet few alternatives have had\r\nmuch success, in part because they provide only modest gains in\r\nexpressivity.  We propose to make a much larger jump: a compiler that\r\nsynthesizes hardware from behavioral functional specifications. Our\r\ncompiler translates general Haskell programs into a restricted\r\nintermediate representation before applying a series of\r\nsemantics-preserving transformations, concluding with a simple\r\nsyntax-directed translation to SystemVerilog.  Here, we present the\r\noverall framework for this compiler, focusing on the IRs involved and\r\nour method for translating general recursive functions into equivalent\r\nhardware. We conclude with experimental results that depict the\r\nperformance and resource usage of the circuitry generated with our\r\ncompiler. (pdf) M2: Multi-Mobile Computing Naser AlDuaij, Alexander Van't Hof, Jason Nieh 2015-03-31 With the widespread use of mobile systems, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems, including hardware device features such as cameras, displays, speakers, microphones, sensors, and input. We present M2, a system for multi-mobile computing by enabling remote sharing and combining of devices across multiple mobile systems. M2 leverages higher-level device abstractions and encoding and decoding hardware in mobile systems to define a cross-platform interface for remote device sharing to operate seamlessly across heterogeneous mobile hardware and software. M2 can be used to build new multi-mobile apps as well as make existing unmodified apps multi-mobile aware through the use of fused devices, which transparently combine multiple devices into a more capable one. We have implemented an M2 prototype on Android that operates across heterogeneous hardware and software, including using Android and iOS remote devices, the latter allowing iOS users to also run Android apps. Our results using unmodified apps from Google Play show that M2 can enable even display-intensive 2D and 3D games to use remote devices across multiple mobile systems with modest overhead and qualitative performance indistinguishable from using local device hardware. (pdf) Dynamic Inference of Likely Metamorphic Properties to Support Differential Testing Fang-Hsiang Su, Jonathan Bell, Christian Murphy, Gail Kaiser 2015-02-27 Metamorphic testing is an advanced technique to test programs and applications without a test oracle such as machine learning applications. Because these programs have no general oracle to identify their correctness, traditional testing techniques such as unit testing may not be helpful for developers to detect potential bugs. This paper presents a novel system, KABU, which can dynamically infer properties that describe the characteristics of a program before and after transforming its input at the method level. Metamorphic Properties (MPs) are pivotal to detecting potential bugs in programs without test oracles, but most previous work relies solely on human effort do identify them. This paper also proposes a new testing concept, Metamorphic Differential Testing (MDT). By comparing the MPs between different versions of the same application, KABU can detect potential bugs in the program. We have performed a preliminary evaluation of KABU by comparing the MPs detected by humans with the MPs detected by KABU. Our preliminary results are very promising: KABU can find more MPs as human developers, and its differential testing mechanism is effective at detecting function changes in programs. (pdf) DisCo: Displays that Communicate Kensei Jo, Mohit Gupta, Shree Nayar 2014-12-16 We present DisCo, a novel display-camera communication system. DisCo enables displays and cameras to communicate with each other, while also displaying and capturing images for human consumption. Messages are transmitted by temporally modulating the display brightness at high frequencies so that they are imperceptible to humans. Messages are received by a rolling shutter camera which converts the temporally modulated incident light into a spatial flicker pattern. In the captured image, the flicker pattern is superimposed on the pattern shown on the display. The flicker and the display pattern are separated by capturing two images with different exposures. The proposed system performs robustly in challenging real-world situations such as occlusion, variable display size, defocus blur, perspective distortion and camera rotation. Unlike several existing visible light communication methods, DisCo works with off-the-shelf image sensors. It is compatible with a variety of sources (including displays, single LEDs), as well as reflective surfaces illuminated with light sources. We have built hardware prototypes that demonstrate DisCoâ€™s performance in several scenarios. Because of its robustness, speed, ease of use and generality, DisCo can be widely deployed in several CHI applications, such as advertising, pairing of displays with cell-phones, tagging objects in stores and museums, and indoor navigation. (pdf) The Internet is a Series of Tubes Henning Schulzrinne 2014-11-28 This is a contribution for the November 2014 Dagstuhl workshop on affordable Internet access. The contribution describes the issues of availability, affordability and relevance, with a particular focus on the experience with providing universal broadband Internet access in the United States. (pdf) Making Lock-free Data Structures Verifiable with Artificial Transactions Xinhao Yuan, David Williams-King, Junfeng Yang, Simha Sethumadhavan 2014-11-11 Among all classes of parallel programming abstractions, lock-free data structures are considered one of the most scalable and efficient because of their fine-grained style of synchronization. However, they are also challenging for developers and tools to verify because of the huge number of possible interleavings that result from fine-grained synchronizations.\r\n\r\nThis paper address this fundamental problem between performance and verifiability of lock-free data structures. We present TXIT, a system that greatly reduces the set of possible interleavings by inserting transactions into the implementation of a lock-free data structure. We leverage hardware transactional memory support from Intel Haswell processors to enforce these artificial transactions. Evaluation on six popular lock-free data structures shows that TXIT makes it easy to verify lock-free data structures while incurring acceptable runtime overhead. Further analysis shows that two inefficiencies in Haswell are the largest contributors to this overhead. (pdf) Metamorphic Runtime Checking of Applications Without Test Oracles Jonathan Bell, Chris Murphy, Gail Kaiser 2014-10-20 For some applications, it is impossible or impractical to know what the correct output should be for an arbitrary input, making testing difficult. Many machine-learning applications for â€œbig dataâ€, bioinformatics and cyberphysical systems fall in this scope: they do not have a test oracle. Metamorphic Testing, a simple testing technique that does not require a test oracle, has been shown to be effective for testing such applications. We present Metamorphic Runtime Checking, a novel approach that conducts metamorphic testing of both the entire application and individual functions during a programâ€™s execution. We have applied Metamorphic Runtime Checking to 9 machine-learning applications, finding it to be on average 170% more effective than traditional metamorphic testing at only the full application level. (pdf) Repeatable Reverse Engineering for the Greater Good with PANDA Brendan Dolan-Gavitt, Josh Hodosh, Patrick Hulin, Tim Leek, Ryan Whelan 2014-10-01 We present PANDA, an open-source tool that has been purpose-built to support whole system reverse engineering. It is built upon the QEMU whole system emulator, and so analyses have access to all code executing in the guest and all data. PANDA adds the ability to record and replay executions, enabling iterative, deep, whole system analyses. Further, the replay log files are compact and shareable, allowing for repeatable experiments. A nine billion instruction boot of FreeBSD, e.g., is represented by only a few hundred MB. Further, PANDA leverages QEMU's support of thirteen different CPU architectures to make analyses of those diverse instruction sets possible within the LLVM IR. In this way, PANDA can have a single dynamic taint analysis, for example, that precisely supports many CPUs. PANDA analyses are written in a simple plugin architecture which includes a mechanism to share functionality between plugins, increasing analysis code re-use and simplifying complex analysis development. We demonstrate PANDA's effectiveness via a number of use cases, including enabling an old but legitimate version of Starcraft to run despite a lost CD key, in-depth diagnosis of an Internet Explorer crash, and uncovering the censorship activities and mechanisms of a Chinese IM client. (pdf) Detecting, Isolating and Enforcing Dependencies Between and Within Test Cases Jonathan Bell 2014-07-06 Testing stateful applications is challenging, as it can be difficult to identify hidden dependencies on program state. These dependencies may manifest between several test cases, or simply within a single test case. When it's left to developers to document, understand, and respond to these dependencies, a mistake can result in unexpected and invalid test results. Although current testing infrastructure does not currently leverage state dependency information, we argue that it could, and that by doing so testing can be improved. Our results thus far show that by recovering dependencies between test cases and modifying the popular testing framework, JUnit, to utilize this information, we can optimize the testing process, reducing time needed to run tests by 62% on average. Our ongoing work is to apply similar analyses to improve existing state of the art test suite prioritization techniques and state of the art test case generation techniques. This work is advised by Professor Gail Kaiser. (pdf) Phasor Imaging: A Generalization of Correlation-Based Time-of-Flight Imaging Mohit Gupta, Shree Nayar, Matthias Hullin, Jaime Martin 2014-06-26 In correlation-based time-of-flight (C-ToF) imaging systems, light sources with temporally varying intensities illuminate the scene. Due to global illumination, the temporally varying radiance received at the sensor is a combination of light received along multiple paths. Recovering scene properties (e.g., scene depths) from the received radiance requires separating these contributions, which is challenging due to the complexity of global illumination and the additional temporal dimension of the radiance.\r\n\r\nWe propose phasor imaging, a framework for performing fast inverse light transport analysis using C-ToF sensors. Phasor imaging is based on the idea that by representing light transport quantities as phasors and light transport events as phasor transformations, light transport analysis can be simplified in the temporal frequency domain. We study the effect of temporal illumination frequencies on light transport, and show that for a broad range of scenes, global radiance (multi-path interference) vanishes for frequencies higher than a scene-dependent threshold. We use this observation for developing two novel scene recovery techniques. First, we present Micro ToF imaging, a ToF based shape recovery technique that is robust to errors due to global illumination. Second, we present a technique for separating the direct  and global components of radiance. Both techniques require capturing as few as 3-4 images and minimal computations. We demonstrate the validity of the presented techniques via simulations and experiments performed with our hardware prototype. (pdf) Schur complement trick for positive semi-definite energies Alec Jacobson 2014-06-12 The â€œSchur complement trickâ€ appears sporadically in numerical optimization methods [Schur 1917; Cottle 1974]. The trick is especially useful for solving Lagrangian saddle point problems when minimizing quadratic energies subject to linear equality constraints [Gill et al. 1987]. Typically, to apply the trick, the energyâ€™s Hessian is assumed positive definite. I generalize this technique for positive semi-definite Hessians. (pdf) Model Aggregation for Distributed Content Anomaly Detection Sean Whalen, Nathaniel Boggs, Salvatore J. Stolfo 2014-06-02 Cloud computing offers a scalable, low-cost, and resilient platform for critical applications. Securing these applications against attacks targeting unknown vulnerabilities is an unsolved challenge. Network anomaly detection addresses such zero-day attacks by modeling attributes of attack-free application traffic and raising alerts when new traffic deviates from this model. Content anomaly detection (CAD) is a variant of this approach that models the payloads of such traffic instead of higher level attributes. Zero-day attacks then appear as outliers to properly trained CAD sensors. In the past, CAD was unsuited to cloud environments due to the relative overhead of content inspection and the dynamic routing of content paths to geographically diverse sites. We challenge this notion and introduce new methods for efficiently aggregating content models to enable scalable CAD in dynamically-pathed environments such as the cloud. These methods eliminate the need to exchange raw content, drastically reduce network and CPU overhead, and offer varying levels of content privacy. We perform a comparative analysis of our methods using Random Forest, Logistic Regression, and Bloom Filter-based classifiers for operation in the cloud or other distributed settings such as wireless sensor networks. We find that content model aggregation offers statistically significant improvements over non-aggregate models with minimal overhead, and that distributed and non-distributed CAD have statistically indistinguishable performance. Thus, these methods enable the practical deployment of accurate CAD sensors in a distributed attack detection infrastructure. (pdf) Vernam, Mauborgne, and Friedman:  The One-Time Pad and the Index of Coincidence Steven M. Bellovin 2014-05-13 The conventional narrative for the invention of the AT&T\r\none-time pad was related by David Kahn.  Based on the evidence available\r\nin the AT&T patent files and from interviews and correspondence,\r\nhe concluded that  Gilbert Vernam came up with the need for randomness, while\r\nJoseph Mauborgne realized the need for a non-repeating key.  Examination\r\nof other documents\r\nsuggests a different narrative.\r\nIt is most likely that Vernam came up with the need for non-repetition;\r\nMauborgne, though, apparently contributed materially to the\r\ninvention of the\r\ntwo-tape variant.  Furthermore, there is reason to suspect that\r\nhe suggested the need for randomness to Vernam.\r\nHowever, neither Mauborgne, Herbert Yardley, nor\r\nanyone at AT&T really understood\r\nthe security advantages of the true one-time tape.  Col. Parker Hitt\r\nmay have; William Friedman definitely did.  Finally, we\r\nshow that Friedman's attacks on the two-tape variant\r\nlikely led to his invention of the index of coincidence, arguably\r\nthe single most important publication in the history of \r\ncryptanalysis. (pdf) Exploring Societal Computing based on the Example of Privacy Swapneel Sheth 2014-04-25 Data privacy when using online systems like Facebook and Amazon has become an increasingly popular topic in the last few years. This thesis will consist of the following four projects that aim to address the issues of privacy and software engineering.\r\n\r\nFirst, only a little is known about how users and developers perceive privacy and which concrete measures would mitigate their privacy concerns. To investigate privacy requirements, we conducted an online survey with closed and open questions and collected 408 valid responses. Our results show that users often reduce privacy to security, with data sharing and data breaches being their biggest concerns. Users are more concerned about the content of their documents and their personal data such as location than about their interaction data. Unlike users, developers clearly prefer technical measures like data anonymization and think that privacy laws and policies are less effective. We also observed interesting differences between people from different geographies. For example, people from Europe are more concerned about data breaches than people from North America. People from Asia/Pacific and Europe believe that content and metadata are more critical for privacy than people from North America. Our results contribute to developing a user-driven privacy framework that is based on empirical evidence in addition to the legal, technical, and commercial perspectives.\r\n\r\nSecond, a related challenge to above, is to make privacy more understandable in complex systems that may have a variety of user interface options, which may change often. As social network platforms have evolved, the ability for users to control how and with whom information is being shared introduces challenges concerning the configuration and comprehension of privacy settings. To address these concerns, our crowd sourced approach simplifies the understanding of privacy settings by using data collected from 512 users over a 17 month period to generate visualizations that allow users to compare their personal settings to an arbitrary subset of individuals of their choosing. To validate our approach we conducted an online survey with closed and open questions and collected 59 valid responses after which we conducted follow-up interviews with 10 respondents. Our results showed that 70% of respondents found visualizations using crowd sourced data useful for understanding privacy settings, and 80% preferred a crowd sourced tool for configuring their privacy settings over current privacy controls.\r\n\r\nThird, as software evolves over time, this might introduce bugs that breach usersâ€™ privacy. Further, there might be system-wide policy changes that could change usersâ€™ settings to be more or less private than before. We present a novel technique that can be used by end-users for detecting changes in privacy, i.e., regression testing for privacy. Using a social approach for detecting privacy bugs, we present two prototype tools. Our evaluation shows the feasibility and utility of our approach for detecting privacy bugs. We highlight two interesting case studies on the bugs that were discovered using our tools. To the best of our knowledge, this is the first technique that leverages regression testing for detecting privacy bugs from an end-user perspective.\r\n\r\nFourth, approaches to addressing these privacy concerns typically require substantial extra computational resources, which might be beneficial where privacy is concerned, but may have significant negative impact with respect to Green Computing and sustainability, another major societal concern. Spending more computation time results in spending more energy and other resources that make the software system less sustainable. Ideally, what we would like are techniques for designing software systems that address these privacy concerns but which are also sustainable â€” systems where privacy could be achieved â€œfor free,â€ i.e., without having to spend extra computational effort. We describe how privacy can indeed be achieved for free â€” an accidental and beneficial side effect of doing some existing computation â€” in web applications and online systems that have access to user data. We show the feasibility, sustainability, and utility of our approach and what types of privacy threats it can mitigate.\r\n\r\nFinally, we generalize the problem of privacy and its tradeoffs. As Social Computing has increasingly captivated the general public, it has become a popular research area for computer scientists. Social Computing research focuses on online social behavior and using artifacts derived from it for providing recommendations and other useful community knowledge. Unfortunately, some of that behavior and knowledge incur societal costs, particularly with regards to Privacy, which is viewed quite differently by different populations as well as regulated differently in different locales. But clever technical solutions to those challenges may impose additional societal costs, e.g., by consuming substantial resources at odds with Green Computing, another major area of societal concern. We propose a new crosscutting research area, Societal Computing, that focuses on the technical tradeoffs among computational models and application domains that raise significant societal issues. We highlight some of the relevant research topics and open problems that we foresee in Societal Computing. We feel that these topics, and Societal Computing in general, need to gain prominence as they will provide useful avenues of research leading to increasing benefits for society as a whole. (pdf) A Convergence Study of Multimaterial Mesh-based Surface Tracking Fang Da, Christopher Batty, Eitan Grinspun 2014-04-14 We report the results from experiments on the convergence of the multimaterial mesh-based surface tracking method introduced by the same authors. Under mesh refinement, approximately first order convergence or higher in L1 and L2 is shown for vertex positions, face normals and non-manifold junction curves in a number of scenarios involving the new operations proposed in the method. (pdf) The Economics of Cyberwar Steven M. Bellovin 2014-04-11 Cyberwar is very much in the news these days.\r\nIt is tempting to try to understand the economics of such an activity,\r\nif only qualitatively.\r\nWhat effort is required?  What can such attacks accomplish?\r\nWhat does this say, if anything, about the likelihood of cyberwar? (pdf) Energy Exchanges: Internal Power Oversight for Applications Melanie Kambadur, Martha A. Kim 2014-04-08 This paper introduces energy exchanges, a set of abstractions that allow applications to help hardware and operating systems manage power and energy consumption. Using annotations, energy exchanges dictate when, where, and how to trade performance or accuracy for power in ways that only an applicationâ€™s developer can decide. In particular, the abstractions offer audits and budgets which watch and cap the power or energy of some piece of the application. The interface also exposes energy and power usage reports which an application may use to change its behavior. Such information complements existing system-wide energy management by operating systems or hardware, which provide global fairness and protections, but are unaware of the internal dynamics of an application.\r\n\r\nEnergy exchanges are implemented as a user-level C++ library. The library employs an accounting technique to attribute  shares of system-wide energy consumption (provided by system-wide hardware energy meters available on newer hardware platforms) to individual application threads. With these per-thread meters and careful tracking of an applicationâ€™s activity, the library exposes energy and power usage for program regions of interest via the energy exchange abstractions with negligible runtime or power overhead. We use the library to demonstrate three applications of energy exchanges: (1) the prioritization of a mobile gameâ€™s energy use over third-party advertisements, (2) dynamic adaptations of the framerate of a video tracking benchmark that maximize performance and accuracy within the confines of a given energy allotment, and (3) the triggering of computational sprints and corresponding cooldowns, based on time, system TDP, and power consumption. (pdf) Phosphor: Illuminating Dynamic Data Flow in the JVM Jonathan Bell, Gail Kaiser 2014-03-25 Dynamic taint analysis is a well-known information flow analysis problem with many possible applications. Taint tracking allows for analysis of application data flow by assigning labels to inputs, and then propagating those labels through data flow. Taint tracking systems traditionally compromise among performance, precision, accuracy, and portability. Performance can be critical, as these systems are typically intended to be deployed with software, and hence must have low overhead. To be deployed in security-conscious settings, taint tracking must also be accurate and precise.\r\nDynamic taint tracking must be portable in order to be easily deployed and adopted for real world purposes, without requiring recompilation of the operating system or language interpreter, and without requiring access to application source code.\r\n\r\nWe present Phosphor, a dynamic taint tracking system for the Java Virtual Machine (JVM) that simultaneously achieves our goals of performance, accuracy, precision, and portability. Moreover, to our knowledge, it is the first portable general purpose taint tracking system for the JVM. We evaluated Phosphor's performance on two commonly used JVM languages (Java and Scala), on two versions of two commonly used JVMs (Oracle's HotSpot and OpenJDK's IcedTea) and on Android's Dalvik Virtual Machine, finding its performance to be impressive: as low as 3% (53% on average), using the DaCapo macro benchmark suite. This paper describes the approach that Phosphor uses to achieve portable taint tracking in the JVM. (pdf) Enhancing Security by Diversifying Instruction Sets Kanad Sinha, Vasileios Kemerlis, Vasilis Pappas, Simha Sethumadhavan, Angelos Keromytis 2014-03-20 Despite the variety of choices regarding hardware and software, to date a large number of computer\r\nsystems remain identical. Characteristic examples of this trend are Windows on x86 and Android on ARM.\r\nThis homogeneity, sometimes referred to as â€œcomputing oligoculture\", provides a fertile ground for malware\r\nin the highly networked world of today.\r\nOne way to counter this problem is to diversify systems so that attackers cannot quickly and easily\r\ncompromise a large number of machines. For instance, if each system has a different ISA, the attacker\r\nhas to invest more time in developing exploits that run on every system manifestation. It is not that each\r\nindividual attack gets harder, but the spread of malware slows down. Further, if the diversified ISA is kept\r\nsecret from the attacker, the bar for exploitation is raised even higher.\r\nIn this paper, we show that system diversification can be realized by enabling diversity at the lowest\r\nhardware/software interface, the ISA, with almost zero performance overhead. We also describe how prac-\r\ntical development and deployment problems of diversified systems can be handled easily in the context of\r\npopular software distrbution models, such as the mobile app store model. We demonstrate our proposal with\r\nan OpenSPARC FPGA prototype (pdf) Teaching Microarchitecture through Metaphors Julianna Eum, Simha Sethumadhavan 2014-03-19 Students traditionally learn microarchitecture by studying textual\r\n\r\ndescriptions with diagrams but few analogies. Several popular \r\n\r\ntextbooks on this topic introduce concepts such as pipelining and \r\n\r\ncaching in the context of simple paper-only architectures. While \r\n\r\nthis instructional style allows important concepts to be covered \r\n\r\nwithin a given class period, students have difficulty bridging the \r\n\r\ngap between what is covered in classes and real-world \r\n\r\nimplementations. Discussing concrete implementations and \r\n\r\ncomplications would, however, take too much time. \r\n\r\nIn this paper, we propose a technique of representing \r\n\r\nmicroarchitecture building blocks with animated metaphors to \r\n\r\naccelerate the process of learning about complex \r\n\r\nmicroarchitectures. We represent hardware implementations as \r\n\r\nroad networks that include specific patterns of traffic flow found \r\n\r\nin microarchitectural behavior. Our experiences indicate an 83% \r\n\r\nimprovement to understanding memory system microarchitecture. \r\n\r\nWe believe the mental models developed by these students will\r\n\r\nserve them in remembering microarchitectural behavior and \r\n\r\nextend to learning new microarchitectures more easily. (pdf) A Red Team/Blue Team Assessment of Functional Analysis Methods for Malicious Circuit Identification Adam Waksman, Jeyavijayan Rajendran, Matthew Suozzo, Simha Sethumadhavan 2014-03-05 Recent advances in hardware security have led to the development of\r\nFANCI (Functional Analysis for Nearly-Unused Circuit Identification), an\r\nanalysis tool that identifies stealthy, malicious circuits within hardware designs that can perform malicious backdoor behavior. Evaluations of such tools against benchmarks and academic attacks are not always equivalent to the dynamic attack scenarios that can arise in the real world. For this reason, we apply a red team/blue team approach to stress-test FANCI's abilities to efficiently detect malicious backdoor circuits within hardware designs.\r\n\r\nIn the Embedded Systems Challenge (ESC) 2013, teams from research groups\r\nfrom multiple continents created designs with malicious backdoors hidden\r\nin them as part of a red team effort to circumvent FANCI. Notably, these\r\nbackdoors were not placed into a priori known designs. The red\r\nteam was allowed to create arbitrary, unspecified designs. Two interesting results came out of this effort. The first was that FANCI was surprisingly resilient to this wide variety of attacks and was not circumvented by any of the stealthy backdoors created by the red teams. The second result is that frequent-action backdoors, which are backdoors that are not made stealthy, were often successful. These results emphasize the importance of combining FANCI with a reasonable degree of validation testing. The blue team efforts also exposed some aspects of the FANCI prototype that make analysis time-consuming in some cases, which motivates further development of the prototype in the future. (pdf) (ps) Unsupervised Anomaly-based Malware Detection using Hardware Features Adrian Tang, Simha Sethumadhavan, Salvatore Stolfo 2014-03-01 Recent works have shown promise in using microarchitectural execution patterns to detect malware programs. These detectors belong to a class of detectors known as signature-based detectors as they catch malware by comparing a program's execution pattern (signature) to execution patterns of known malware programs.  In this work, we propose a new class of detectors - anomaly-based hardware malware detectors - that do not require signatures for malware detection, and thus can catch a wider range of malware including potentially novel ones. We use unsupervised machine learning to build profiles of normal program execution based on data from performance counters, and use these profiles to detect significant deviations in program behavior that occur as a result of malware exploitation. We show that real-world exploitation of popular programs such as IE and Adobe PDF Reader on a Windows/x86 platform can be detected with nearly perfect certainty. We also examine the limits and challenges in implementing this approach in face of a sophisticated adversary attempting to evade anomaly-based detection. The proposed detector is complementary to previously proposed signature-based detectors and can be used together to improve security. (pdf) Enabling the Virtual Phones to remotely sense the Real Phones in real-time ~ A Sensor Emulation initiative for virtualized Android-x86 ~ Raghavan Santhanam 2014-02-13 Smartphones nowadays have the ground-breaking features that were only a figment of oneâ€™s imagination. For the ever-demanding cellphone users, the exhaustive list of features that a smartphone supports just keeps getting more exhaustive with time. These features aid oneâ€™s personal and professional uses as well. Extrapolating into the future the features of a present-day smartphone, the lives of us humans using smartphones are going to be unimaginably agile.\r\n\r\nWith the above said emphasis on the current and future potential of a smartphone, the ability to virtualize smartphones with all their real-world features into a virtual platform, is a boon for those who want to rigorously experiment and customize the virtualized smartphone hardware without spending an extra penny. Once virtualizable independently on a larger scale, the idea of virtualized smartphones with all the virtualized pieces of hardware takes an interesting turn with the sensors being virtualized in a way thatâ€™s closer to the real-world behavior.\r\n\r\nWhen accessible remotely with the real-time responsiveness, the above mentioned real-world behavior will be a real dealmaker in many real-world systems, namely, the life-saving systems like the ones that instantaneously get alerts about harmful magnetic radiations in the deep mining areas, etc. And these life-saving systems would be installed on a large scale on the desktops or large servers as virtualized smartphones having the added support of virtualized sensors which remotely fetch the real hardware sensor readings from a real smartphone in real-time. Based on these readings the lives working in the affected areas can be alerted and thus saved by the people who are operating the at the desktops or large servers hosting the virtualized smartphones.\r\n\r\nIn addition, the direct and one of the biggest advantages of such a real hardware sensor driven Sensor Emulation in an emulated Android(-x86) environment is that the Android applications that use sensors can now run on the emulator and act under the\r\ninfluence of real hardware sensorsâ€™ due to the emulated sensors. The current work of Sensor Emulation is quite unique when compared to the existing and past sensor-related works. The uniqueness comes from the full-fledged sensoremulation in a virtualized smartphone environment as opposed to building some sophisticated physical systems that usually aggregate the sensor readings from the real hardware sensors, might be in a remote manner and in real-time. For example, wireless sensor networks based remote-sensing systems that install real hardware sensors in remote places and have the sensor readings from all those sensors at a centralized server or something similar, for the necessary real-time or offline analysis. In these systems, apart from collecting mere real hardware sensor readings into a centralized entity, nothing more is being achieved unlike in the current work of Sensor Emulation wherein the emulated sensors behave exactly like the remote real hardware sensors. The emulated sensors can be calibrated, speeded up or slowed down(in terms of their sampling frequency), influence the sensor-based application running inside the virtualized smartphone environment exactly as the real hardware sensors of a real phone would do to the sensor-based application running in that real phone. In essence, the current work is more about generalizing the sensors with all its real-world characteristics as far as possible in a virtualized platform than just a\r\nframework to send and receive sensor readings over the network between the real and virtual phones.\r\n\r\nRealizing the useful advantages of Sensor Emulation which is about adding virtualized sensors support to emulated environments, the current work emulates a total of ten sensors present in the real smartphone, Samsung Nexus S, an Android device. Virtual phones run Android-x86 while real phones run Android. The real reason behind choosing Android-x86 for virtual phone is that x86-based Android\r\ndevices are feature-rich over ARM based ones, for example a full-fledged x86 desktop or a tablet has more features than a relatively small smartphone. Out of the ten, five are real sensors and the rest are virtual or synthetic ones. The real ones are Accelerometer, Magnetometer, Light, Proximity, and Gyroscope whereas the virtual ones are Corrected Gyroscope, Linear Acceleration, Gravity, Orientation, and Rotation Vector. The emulated Android-x86 is of Android release version Jelly Bean 4.3.1 which differs only very slightly in terms of bug fixes from Android Jelly Bean 4.3 running on the real smartphone.\r\n\r\nOne of the noteworthy aspects of the Sensor Emulation accomplished is being demand-less - exactly the same sensor-based Android applications will be able to use the sensors on the real and virtual phones, with absolutely no difference in terms of their sensor-based behavior.\r\n\r\nThe emulationâ€™s core idea is the socket-communication between two modules of Hardware Abstraction Layer(HAL) which is driver-agnostic, remotely over a wireless network in real-time. Apart from a Paired real-device scenario from which the real hardware sensor readings are fetched, the Sensor Emulation also is compatible with a Remote Server Scenario wherein the artificially generated sensor readings are fetched from a remote server. Due to the Sensor Emulation having been built on mereend-to-end socket-communication, itâ€™s logical and obvious to see that the real and virtual phones can run different Android(-x86) releases with no real impact on the\r\nSensor Emulation being accomplished.\r\n\r\nSensor Emulation once completed was evaluated for each of the emulated sensors using applications from Android Market as well as Amazon Appstore. The applications category include both the basic sensor-test applications that show raw sensor readings, as well as the advanced 3D sensor-driven games which are emulator compatible, especially in terms of the graphics. The evaluations proved the current work of Sensor Emulation to be generic, efficient, robust, fast, accurate, and real.\r\n\r\nAs of this writing i.e., January 2014, the current work of Sensor Emulation is the sole system-level sensor virtualization work that embraces remoteness in real-time for the emulated Android-x86 systems. It is important to note that though the current work is\r\ntargeted for Android-x86, the code written for the current work makes no assumptions about underlying platform to be an x86 one. Hence, the work is also logically seen as compatible with ARM based emulated Android environment though not actually tested. (pdf) Towards A Dynamic QoS-aware Over-The-Top Video Streaming in LTE Hyunwoo Nam, Kyung Hwa Kim, Bong Ho Kim, Doru Calin, Henning Schulzrinne 2014-01-16 We present a study of traffic behavior of two popular over-the-top (OTT) video streaming services (YouTube and Netflix). Our analysis is conducted on different mobile devices (iOS and Android) over various wireless networks (Wi-Fi, 3G and LTE) under dynamic network conditions. Our measurements show that the video players frequently discard a large amount of video content although it is successfully delivered to a client.\r\n\r\nWe first investigate the root cause of this unwanted behavior. Then, we propose a Quality-of-Service (QoS)-aware video streaming architecture in Long Term Evolution (LTE) networks to reduce the waste of network resource and improve user experience. The architecture includes a selective packet discarding mechanism, which can be placed in packet data network gateways (P-GW). In addition, our QoS-aware rules assist video players in selecting an appropriate resolution under a fluctuating channel condition. We monitor network condition and configure QoS parameters to control availability of the maximum bandwidth in real time. In our experimental setup, the proposed platform shows up to 20.58% improvement in saving downlink bandwidth and improves user experience by reducing buffer underflow period to an average of 32 seconds. (pdf) Towards Dynamic Network Condition-Aware Video Server Selection Algorithms over Wireless Networks Hyunwoo Nam, Kyung-Hwa Kim, Doru Calin, Henning Schulzrinne 2014-01-16 We investigate video server selection algorithms in a distributed video-on-demand system. We conduct a detailed study of the YouTube Content Delivery Network (CDN) on PCs and mobile devices over Wi-Fi and 3G networks under varying network conditions. We proved that a location-aware video server selection algorithm assigns a video content server\r\nbased on the network attachment point of a client. We found out that such distance-based algorithms carry the risk of directing a client to a less optimal content server, although there may exist other better performing video delivery servers. In order to solve this problem, we propose to use dynamic network information such as packet loss rates and Round Trip Time (RTT)between an edge node of an wireless network (e.g., an Internet Service Provider (ISP) router in a Wi-Fi network and a Radio\r\nNetwork Controller (RNC) node in a 3G network) and video content servers, to find the optimal video content server when a video is requested. Our empirical study shows that the proposed architecture can provide higher TCP performance, leading to better viewing quality compared to location-based video server selection algorithms. (pdf) Approximating the Bethe partition function Adrian Weller, Tony Jebara 2013-12-30 When belief propagation (BP) converges, it does so to a stationary point of the Bethe free energy $\\F$, and is often strikingly accurate. However, it may converge only to a local optimum or may not converge at all. An algorithm was recently introduced for attractive binary pairwise MRFs which is guaranteed to return an $\\eps$-approximation to the global minimum of $\\F$ in polynomial time provided the maximum degree $\\Delta=O(\\log n)$, where $n$ is the number of variables. Here we significantly improve this algorithm and derive several results including a new approach based on analyzing first derivatives of $\\F$, which leads to performance that is typically far superior and yields a fully polynomial-time approximation scheme (FPTAS) for attractive models without any degree restriction. Further, the method applies to general (non-attractive) models, though with no polynomial time guarantee in this case, leading to the important result that approximating $\\log$ of the Bethe partition function, $\\log Z_B=-\\min \\F$, for a general model to additive $\\epsilon$-accuracy may be reduced to a discrete MAP inference problem.  We explore an application to predicting equipment failure on an urban power network and demonstrate that the Bethe approximation can perform well even when BP fails to converge. (pdf) A Gameful Approach to Teaching Software Design and Software Testing Swapneel Sheth, Jonathan Bell, Gail Kaiser 2013-12-13 Introductory computer science courses traditionally focus on exposing students to basic programming and computer science theory, leaving little or no time to teach students about software testing. A lot of studentsâ€™ mental model when they start learning programming is that â€œif it compiles and runs without crashing, it must work fine.â€ Thus exposure to testing, even at a very basic level, can be very beneficial to the students. In the short term, they will do better on their assignments as testing before submission might help them discover bugs in their implementation that they hadnâ€™t realized. In the long term, they will appreciate the importance of testing as part of the software development life cycle. (pdf) A Gameful Approach to Teaching Software Design and Software Testing --- Assignments and Quests Swapneel Sheth, Jonathan Bell, Gail Kaiser 2013-12-11 We describe how we used HALO in a CS2 classroom and include the assignments and quests created. (pdf) Heterogeneous Access: Survey and Design Considerations Amandeep Singh, Gaston Ormazabal, Sateesh Addepalli, Henning Schulzrinne 2013-10-25 As voice, multimedia, and data services are converging to IP, there is a need for a new networking architecture to support future innovations and applications. Users are consuming Internet services from multiple devices that have multiple network interfaces such as Wi-Fi, LTE, Bluetooth, and possibly wired LAN. Such diverse network connectivity can be used to increase both reliability and performance by running applications over multiple links, sequentially for seamless user experience, or in parallel for bandwidth and performance enhancements. The existing networking stack, however, offers almost no support for intelligently exploiting such network, device, and location diversity.\r\n\r\nIn this work, we survey recently proposed protocols and architectures that enable heterogeneous networking support. Upon evaluation, we abstract common design patterns and propose a unified networking architecture that makes better use of a heterogeneous dynamic environment, both in terms of networks and devices. The architecture enables mobile nodes to make intelligent decisions about how and when to use each or a combination of networks, based on access policies. With this new architecture, we envision a shift from current applications, which support a single network, location, and device at a time to applications that can support multiple networks, multiple locations, and multiple devices. (pdf) Functioning Hardware from Functional Programs Stephen A. Edwards 2013-10-08 To provide high performance at practical power levels, tomorrow's\r\nchips will have to consist primarily of application-specific logic\r\nthat is only powered on when needed.  This paper discusses\r\nsynthesizing such logic from the functional language Haskell.  The\r\nproposed approach, which consists of rewriting steps that ultimately\r\ndismantle the source program into a simple dialect that enables a\r\nsyntax-directed translation to hardware, enables aggressive\r\nparallelization and the synthesis of application-specific distributed\r\nmemory systems.  Transformations include scheduling arithmetic\r\noperations onto specific data paths, replacing recursion with\r\niteration, and improving data locality by inlining recursive types.  A\r\ncompiler based on these principles is under development. (pdf) N Heads are Better than One Morris Hopkins, Mauricio Castaneda, Swapneel Sheth, Gail Kaiser 2013-10-04 Social network platforms have transformed how people communicate and share information. However, as these platforms have evolved, the ability for users to control how and with whom information is being shared introduces challenges concerning the configuration and comprehension of privacy settings. To address these concerns, our crowd sourced approach simplifies the understanding of privacy settings by using data collected from 512 users over a 17 month period to generate visualizations that allow users to compare their personal settings to an arbitrary subset of individuals of their choosing. To validate our approach we conducted an online survey with closed and open questions and collected 50 valid responses after which we conducted follow-up interviews with 10 respondents. Our results showed that 80% of users found visualizations using crowd sourced data useful for understanding privacy settings, and 70% preferred a crowd sourced tool for configuring their privacy settings over current privacy controls. (pdf) Us and Them --- A Study of Privacy Requirements Across North America, Asia, and Europe Swapneel Sheth, Gail Kaiser, Walid Maalej 2013-09-15 Data privacy when using online systems like Facebook and Amazon has\r\nbecome an increasingly popular topic in the\r\nlast few years. However, only a little is known about how users and\r\ndevelopers perceive privacy and which concrete measures would mitigate privacy concerns. To investigate privacy requirements,\r\nwe conducted an online survey with closed and open questions and\r\ncollected 408 valid responses.\r\nOur results show that users often reduce privacy to security, with\r\ndata sharing and data breaches being their biggest concerns. Users are more\r\nconcerned about the content of their documents and personal data such as location than\r\ntheir interaction data.\r\nUnlike users, developers clearly prefer technical measures like data\r\nanonymization and think that privacy laws and policies are less effective.\r\nWe also observed interesting differences between people from different\r\ngeographies. For example, people from Europe are more concerned about\r\ndata breaches than people from North America. People from Asia/Pacific\r\nand Europe believe that content and\r\nmetadata are more critical for privacy than people from North America.\r\nOur results contribute to developing a user-driven privacy framework\r\nthat is based on empirical evidence in addition to the legal,\r\ntechnical, and commercial perspectives. (pdf) Unit Test Virtualization with VMVM Jonathan Bell, Gail Kaiser 2013-09-13 Testing large software packages can become very time intensive. To address this problem, researchers have investigated techniques such as Test Suite Minimization. Test Suite Minimization reduces the number of tests in a suite by removing tests that appear redundant, at the risk of a reduction in fault-finding ability since it can be difficult to identify which tests are truly redundant. We take a completely different approach to solving the same problem of long running test suites by instead reducing the time needed to execute each test, an approach that we call Unit Test Virtualization. With Unit Test Virtualization, we reduce the overhead of isolating each unit test with a lightweight virtualization container. We describe the empirical analysis that grounds our approach and provide an implementation of Unit Test Virtualization targeting Java applications. We evaluated our implementation, VMVM, using 20 real-world Java applications and found that it reduces test suite execution time by up to 97% (on average, 62%) when compared to traditional unit test execution. We also compared VMVM to a well known Test Suite Minimization technique, finding the reduction provided by VMVM to be four times greater, while still executing every test with no loss of fault-finding ability. (pdf) Metamorphic Runtime Checking of Applications without Test Oracles Christian Murphy, Gail Kaiser, Jonathan Bell, Fang-Hsiang Su 2013-09-13 Challenges arise in testing applications that do not have test oracles, i.e., for which it is impossible or impractical to know what the correct output should be for general input. Metamorphic testing, introduced by Chen et al., has been shown to be a simple yet effective technique in testing these types of applications: test inputs are transformed in such a way that it is possible to predict the expected change to the output, and if the output resulting from this transformation is not as expected, then a fault must exist.\r\n\r\nHere, we improve upon previous work by presenting a new technique called Metamorphic Runtime Checking, which automatically conducts metamorphic testing of both the entire application and individual functions during a program's execution. This new approach improves the scope, scale, and sensitivity of metamorphic testing by allowing for the identification of more properties and execution of more tests, and increasing the likelihood of detecting faults not found by application-level properties. We also present the results of new mutation analysis studies that demonstrate that Metamorphic Runtime Checking can kill an average of 170% more mutants than traditional, application-level metamorphic testing alone, and advances the state of the art in testing applications without oracles. (pdf) Effectiveness of Teaching Metamorphic Testing, Part II Kunal Swaroop Mishra, Gail E. Kaiser, Swapneel K. Sheth 2013-07-31 We study the ability of students in a senior/graduate software engineering course to understand and\r\napply metamorphic testing, a relatively recently invented advance in software testing research that\r\ncomplements conventional approaches such as equivalence partitioning and boundary analysis. We\r\npreviously reported our investigation of the fall 2011 offering of the Columbia University course COMS\r\nW4156 Advanced Software Engineering, and here report on the fall 2012 offering and contrast it to\r\nthe previous year. Our main findings are: 1) Although the students in the second offering did not do\r\nvery well on the newly added individual assignment specifically focused on metamorphic testing,\r\nthereafter they were better able to find metamorphic properties for their team projects than the\r\nstudents from the previous year who did not have that preliminary homework and, perhaps most\r\nsignificantly, did not have the solution set for that homework. 2) Students in the second offering did\r\nreasonably well using the relatively novel metamorphic testing technique vs. traditional black box\r\ntesting techniques in their projects (such comparison data is not available for the first offering). 3)\r\nFinally, in both semesters, the majority of the student teams were able to apply metamorphic testing to\r\ntheir team projects after only minimal instruction, which would imply that metamorphic testing is a\r\nviable strategy for student testers. (pdf) On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow Records Sambuddho Chakravarty, Marco V. Barbera, Georgios Portokalidis, Michalis Polychronakis, Angelos D. Keromytis 2013-07-18 Low-latency anonymous communication networks, such as Tor, are geared towards web browsing, instant messaging, and other semi-interactive\r\napplications. To achieve acceptable quality of service, these systems\r\nattempt to preserve packet inter-arrival characteristics, such as inter-packet delay. Consequently, a powerful adversary can mount traffic analysis attacks by observing similar traffic patterns at various points of the network, linking together otherwise unrelated network connections. Previous research has shown that having access to\r\na few Internet exchange points is enough for monitoring a significant percentage of the network paths from Tor nodes to destination servers.\r\nAlthough the capacity of current networks makes packet-level monitoring at such a scale quite challenging, adversaries could potentially use less accurate but readily available traffic monitoring functionality, such as Cisco's NetFlow, to mount large-scale traffic analysis attacks.\r\n\r\nIn this paper, we assess the feasibility and effectiveness of practical\r\ntraffic analysis attacks against the Tor network using NetFlow data.\r\nWe present an active traffic analysis method based on deliberately\r\nperturbing the characteristics of user traffic at the server side,\r\nand observing a similar perturbation at the client side through\r\nstatistical correlation. We evaluate the accuracy of our method using both in-lab testing, as well as data gathered from a public Tor relay serving hundreds of users. Our method revealed the actual sources of anonymous traffic with 100% accuracy for the in-lab tests,\r\nand achieved an overall accuracy of about 81.4% for the real-world experiments, with an average false positive rate of 6.4%. (pdf) A Mobile Video Traffic Analysis: Badly Designed Video Clients Can Waste Network Bandwidth Hyunwoo Nam, Bong Ho Kim, Doru Calin, Henning Schulzrinne 2013-07-08 Video streaming on mobile devices is on the rise. According to recent reports, mobile video streaming traffic accounted for 52.8% of total mobile data traffic in 2011, and it is forecast to reach 66.4% in 2015. We analyzed the network traffic behaviors of the two most popular HTTP-based video streaming services: YouTube and Netflix. Our research indicates that the network traffic behavior depends on factors such as the type of device, multimedia applications in use and network conditions. Furthermore, we found that a large part of the downloaded\r\nvideo content can be unaccepted by a video player even though it is successfully delivered to a client. This unwanted behavior often occurs when the video player changes the resolution in a fluctuating network condition and the playout buffer is full while downloading a video. Some of the measurements show that the discarded data may exceed 35% of the total video content. (pdf) Energy Secure Architecture: A wish list Simha Sethumadhavan 2013-06-23 Energy optimizations are being aggressively pursued today. Can these optimizations open up security vulnerabilities? In this invited talk at the Energy Secure System Architectures Workshop (run by Pradip Bose from IBM Watson research center) I discussed security implications of energy optimizations, capabilities of attackers, ease of exploitation, and potential payoff to the attacker. I presented a mini tutorial on security for computer architects, and a personal research wish list for this emerging topic. (pdf) Principles and Techniques of Schlieren Imaging Systems Amrita Mazumdar 2013-06-19 This paper presents a review of modern-day schlieren optics system and its application. Schlieren imaging systems provide a powerful technique to visualize changes or nonuniformities in refractive index of air or other transparent media. With the popularization of computational imaging techniques and widespread availability of digital imaging systems, schlieren systems provide novel methods of viewing transparent fluid dynamics. This paper presents a historical background of the technique, describes the methodology behind the system, presents a mathematical proof of schlieren fundamentals, and lists various recent applications and advancements in schlieren studies. (pdf) WiSlow: A WiFi Network Performance Troubleshooting Tool for End Users Kyung Hwa Kim, Hyunwoo Nam, Henning Schulzrinne 2013-05-29 The increasing number of 802.11 APs and wireless devices results in more contention, which causes unsatisfactory WiFi network performance. In addition, non-WiFi devices sharing the same spectrum with 802.11 networks such as microwave ovens, cordless phones, and baby monitors severely interfere with WiFi networks. Although the problem sources can be easily removed in many cases, it is difficult for end users to identify the root cause. \r\n\r\nWe introduce WiSlow, a software tool that diagnoses the root causes of poor WiFi performance with user-level network probes and leverages peer collaboration to identify the location of the causes. We elaborate on two main methods: packet loss analysis and 802.11 ACK pattern analysis. (pdf) Connecting the Physical World with Arduino in SECE Hyunwoo Nam, Jan Janak, Henning Schulzrinne 2013-05-23 The Internet of Things (IoT) enables the physical world to be connected and controlled over the Internet. This paper presents a smart gateway platform that connects everyday objects such as lights, thermometers, and TVs over the Internet. The proposed hardware architecture is implemented on an Arduino platform with a variety of off the shelf home automation technologies such as Zigbee and X10. Using the microcontroller-based platform, the SECE (Sense Everything, Control Everything) system allows users to create various IoT services such as monitoring sensors, controlling actuators, triggering action events, and periodic sensor reporting. We give an overview of the Arduino-based smart gateway architecture and its integration into SECE. (pdf) Chameleon: Multi-Persona Binary Compatibility for Mobile Devices Jeremy Andrus, Alexander Van't Hof, Naser AlDuaij, Christoffer Dall, Nicolas Viennot, Jason Nieh 2013-04-08 Mobile devices are vertically integrated systems that are powerful, useful platforms, but unfortunately limit user choice and lock users and developers into a particular mobile ecosystem, such as iOS or Android. We present Chameleon, a multi-persona binary compatibility architecture that allows mobile device users to run applications built for different mobile ecosystems together on the same smartphone or tablet. Chameleon enhances the domestic operating system of a device with personas to mimic the application binary interface of a foreign operating system to run unmodified foreign binary applications. To accomplish this without reimplementing the entire foreign operating system from scratch, Chameleon provides four key mechanisms. First, a multi-persona binary interface is used that can load and execute both domestic and foreign applications that use different sets of system calls. Second, compile-time code adaptation makes it simple to reuse existing unmodified foreign kernel code in the domestic kernel. Third, API interposition and passport system calls make it possible to reuse foreign user code together with domestic kernel facilities to support foreign kernel functionality in user space. Fourth, schizophrenic processes allow foreign applications to use domestic libraries to access proprietary software and hardware interfaces on the device. We have built a Chameleon prototype and demonstrate that it imposes only modest performance overhead and can run iOS applications from the Apple App Store together with Android applications from Google Play on a Nexus 7 tablet running the latest version of Android. (pdf) KVM/ARM: Experiences Building the Linux ARM Hypervisor Christoffer Dall, Jason Nieh 2013-04-05 As ARM CPUs become increasingly common in mobile devices and servers, there is a\r\ngrowing demand for providing the benefits of virtualization for ARMbased\r\ndevices. We present our experiences building the Linux ARM hypervisor, KVM/ARM,\r\nthe first full system ARM virtualization solution that can run unmodified guest\r\noperating systems on ARM multicore hardware.  KVM/ARM introduces split-mode\r\nvirtualization, allowing a hypervisor to split its execution across CPU modes to\r\ntake advantage of CPU mode-specific features. This allows KVM/ARM to leverage\r\nLinux kernel services and functionality to simplify hypervisor development and\r\nmaintainability while utilizing recent ARM hardware virtualization extensions to\r\nrun application workloads in guest operating systems with comparable performance\r\nto native execution. KVM/ARM has been successfully merged into the mainline\r\nLinux 3.9 kernel, ensuring that it will gain wide adoption as the virtualization\r\nplatform of choice for ARM. We provide the first measurements on real hardware\r\nof a complete hypervisor using ARM hardware virtualization support. Our results\r\ndemonstrate that KVM/ARM has modest virtualization performance and power costs,\r\nand can achieve lower performance and power costs compared to x86-based Linux\r\nvirtualization on multicore hardware. (pdf) FARE: A Framework for Benchmarking Reliability of Cyber-Physical Systems Leon Wu, Gail Kaiser 2013-04-01 A cyber-physical system (CPS) is a system featuring a tight combination of, and coordination between, the systemâ€™s computational and physical elements. System reliability is a critical requirement of cyber-physical systems. An unreliable CPS often leads to system malfunctions, service disruptions, financial losses and even human life. Improving CPS reliability requires an objective measurement, estimation and comparison of the CPS system reliability. This paper describes FARE (Failure Analysis and Reliability Estimation), a framework for benchmarking reliability of cyber-physical systems. Some prior researches have proposed reliability benchmark for some specific CPS such as wind power plant and wireless sensor networks. There were also some prior researches on the components of CPS such as software and some specific hardware. But according to the best of our knowledge, there isnâ€™t any reliability benchmark framework for CPS in general. FARE framework provides a CPS reliability model, a set of methods and metrics on the evaluation environment selection, failure analysis and reliability estimation for benchmarking CPS reliability. It not only provides a retrospect evaluation and estimation of the CPS system reliability using the past data, but also provides a mechanism for continuous monitoring and evaluation of CPS reliability for runtime enhancement. The framework is extensible for accommodating new reliability measurement techniques and metrics. It is also generic and applicable to a wide range of CPS applications. For empirical study, we applied the FARE framework on a smart building management system for a large commercial building in New York City. Our experiments showed that FARE is easy to implement, accurate for comparison and can be used for building useful industry benchmarks and standards after accumulating enough data. (pdf) Additional remarks on designing category-level attributes for discriminative visual recognition Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang 2013-03-10 This is the supplementary material for the paper Designing Category-Level Attributes for Discriminative Visual Recognition. (pdf) Make Parallel Programs Reliable with Stable Multithreading Junfeng Yang, Heming Cui, Jingyue Wu, John Gallagher, Chia-Che Tsai, Huayang Guo 2013-02-20 Our accelerating computational demand and the rise of multicore\r\nhardware have made parallel programs increasingly pervasive and\r\ncritical. Yet, these programs remain extremely difficult to write,\r\ntest, analyze, debug, and verify. In this article, we provide our view\r\non why parallel programs, specifically multithreaded programs, are\r\ndifficult to get right.We present a promising approach we call stable\r\nmultithreading to dramatically improve reliability, and summarize\r\nour last four yearsâ€™ research on building and applying stable multithreading\r\nsystems. (pdf) A Finer Functional Fibonacci on a Fast FPGA Stephen A. Edwards 2013-02-13 Through a series of mechanical, semantics-preserving transformations,\r\nI show how a three-line recursive Haskell program (Fibonacci) can be\r\ntransformed to a hardware description language -- Verilog -- that can be\r\nsynthesized on an FPGA.  This report lays groundwork for a\r\ncompiler that will perform this transformation automatically. (pdf) Cost and Scalability of Hardware Encryption Techniques Adam Waksman, Simha Sethumadhavan 2013-02-06 We discuss practical details and basic scalability for two recent ideas for hardware encryption for trojan prevention. The broad idea is to encrypt the data used as inputs to hardware circuits to make it more difficult for malicious attackers to exploit hardware trojans. The two methods we discuss are data obfuscation and fully homomorphic encryption (FHE). Data obfuscation is a technique wherein specific data inputs are encrypted so that they can be operated on within a hardware module without exposing the data itself to the hardware. FHE is a technique recently discovered to be theoretically possible. With FHE, not only the data but also the operations and the entire circuit are encrypted. FHE primarily exists as a theoretical construct currently. It has been shown that it can theoretically be applied to any program or circuit. It has also been applied in a limited respect to some software. Some initial algorithms for hardware applications have been proposed. We find that data obfuscation is efficient enough to be immediately practical, while FHE is not yet in the practical realm. There are also scalability concerns regarding current algorithms for FHE. (pdf) Societal Computing - Thesis Proposal Swapneel Sheth 2013-01-30 As Social Computing has increasingly captivated the general public, it\r\nhas become a popular research area for computer scientists. Social\r\nComputing research focuses on online social behavior and using\r\nartifacts derived from it for providing recommendations and other\r\nuseful community knowledge. Unfortunately, some of that behavior and\r\nknowledge incur societal costs, particularly with regards to Privacy,\r\nwhich is viewed quite differently by different populations as well as\r\nregulated differently in different locales. But clever technical\r\nsolutions to those challenges may impose additional societal costs,\r\ne.g., by consuming substantial resources at odds with Green Computing,\r\nanother major area of societal concern. We propose a new crosscutting\r\nresearch area, \\emph{Societal Computing}, that focuses on the\r\ntechnical tradeoffs among computational models and application domains\r\nthat raise significant societal issues.  We highlight some of the\r\nrelevant research topics and open problems that we foresee in Societal\r\nComputing.\r\nWe feel that\r\nthese topics, and Societal Computing in general, need to gain prominence\r\nas they will provide useful avenues of research leading to\r\nincreasing benefits for society as a whole.\r\nThis thesis will consist of the following four projects that aim to address the issues of Societal Computing.\r\n\r\nFirst, privacy in the context of ubiquitous social computing systems has become a\r\nmajor concern for society at large.\r\nAs the number of online social computing systems that collect user data\r\ngrows, concerns with privacy are further exacerbated.\r\nExamples of such online systems include social networks, recommender systems, and so on.\r\nApproaches to addressing these privacy concerns typically require\r\nsubstantial extra computational resources, which might be beneficial where\r\nprivacy is concerned, but may have significant negative impact with respect\r\nto Green Computing and sustainability, another major societal concern.\r\nSpending more computation time results in spending more energy and other\r\nresources that make the software system less sustainable.\r\nIdeally, what we would like are techniques for designing software systems\r\nthat address these privacy concerns but which are also sustainable --- systems\r\nwhere privacy could be achieved ``for free,'' \\ie without having to spend\r\nextra computational effort.\r\nWe describe how privacy can indeed be achieved for free --- an\r\naccidental and beneficial side effect of doing some existing computation --- in web applications and online systems that have access to user data.\r\nWe show the feasibility, sustainability, and utility of our approach and what types of privacy threats it can mitigate.\r\n\r\nSecond, we aim to understand what the expectations and needs to end-users and software developers are, with respect to privacy in social systems.\r\nSome questions that we want to answer are:\r\nDo end-users care about privacy?\r\nWhat aspects of privacy are the most important to end-users?\r\nDo we need different privacy mechanisms for technical vs. non-technical users?\r\nShould we customize privacy settings and systems based on the geographic location of the users?\r\nWe have created a large scale user study using an online questionnaire to gather privacy requirements from a variety of stakeholders.\r\nWe also plan to conduct follow-up semi-structured interviews.\r\nThis user study will help us answer these questions.\r\n\r\nThird, a related challenge to above, is to make privacy more understandable in complex systems that may have a variety of user interface options, which may change often.\r\nOur approach is to use crowdsourcing to find out how other users deal with privacy and what settings are commonly used to give users feedback on aspects like how public/private their settings are, what common settings are typically used by others, where do a certain users' settings differ from a trusted group of friends, etc.\r\nWe have a large dataset of privacy settings for over 500 users on Facebook and we plan to create a user study that will use the data to make privacy settings more understandable.\r\n\r\nFinally, end-users of such systems find it increasingly hard to understand complex privacy settings.\r\nAs software evolves over time, this might introduce bugs that breach users' privacy.\r\nFurther, there might be system-wide policy changes that could change users' settings to be more or less private than before.\r\nWe present a novel technique that can be used by \\emph{end-users} for detecting changes in privacy, \\ie regression testing for privacy.\r\nUsing a social approach for detecting privacy bugs, we present two prototype tools.\r\nOur evaluation shows the feasibility and utility of our approach for detecting privacy bugs.\r\nWe highlight two interesting case studies on the bugs that were discovered using our tools.\r\nTo the best of our knowledge, this is the first technique that leverages regression testing for detecting privacy bugs from an end-user perspective. (pdf) Finding 9-1-1 Callers in Tall Buildings Wonsang Song, Jae Woo Lee, Byung Suk Lee, Henning Schulzrinne 2013-01-23 Accurately determining a user's floor location is essential for\r\nminimizing delays in emergency response.  This paper presents a floor\r\nlocalization system intended for emergency calls.  We aim to provide\r\nfloor-level accuracy with minimum infrastructure support.  Our\r\napproach is to use multiple sensors, all available in today's\r\nsmartphones, to trace a user's vertical movements inside buildings.\r\n\r\nWe make three contributions.  First, we present a hybrid architecture\r\nfor floor localization with emergency calls in mind.  The architecture\r\ncombines beacon-based infrastructure and sensor-based dead reckoning,\r\nstriking the right balance between accurately determining a user's\r\nlocation and minimizing the required infrastructure.  Second, we\r\npresent the elevator module for tracking a user's movement in an\r\nelevator.  The elevator module addresses three core challenges that\r\nmake it difficult to accurately derive displacement from acceleration.\r\nThird, we present the stairway module which determines the number of\r\nfloors a user has traveled on foot.  Unlike previous systems that\r\ntrack users' foot steps, our stairway module uses a novel landing\r\ncounting technique. (pdf) Effective Dynamic Detection of Alias Analysis Errors Jingyue Wu, Gang Hu, Yang Tang, Junfeng Yang 2013-01-23 Alias analysis is perhaps one of the most crucial and widely used analyses, and has attracted tremendous research efforts over the years. Yet, advanced alias analyses are extremely difficult to get right, and the bugs in these analyses are most likely the reason that they have not been adopted to production compilers. This paper presents NEONGOBY, a system for effectively detecting errors in alias analysis implementations, improving their correctness and hopefully widening their adoption. NEONGOBY works by dynamically observing pointer addresses during the execution of a test program and then checking these addresses against an alias analysis for errors. It is explicitly designed to (1) be agnostic to the alias analysis it checks for maximum applicability and ease of use and (2) detect alias analysis errors that manifest on real-world programs and workloads. It reduces false positives and performance overhead using a practical selection of techniques. Evaluation on three popular alias analyses and real-world programs Apache and MySQL shows that NEONGOBY effectively finds 29 alias analysis bugs with only 2 false positives and reasonable overhead. To enable alias analysis builders to start using NEONGOBY today, we have released it open-source at https://github.com/wujingyue/neongoby, along with our error detection results and proposed patches. (pdf) Bethe Bounds and Approximating the Global Optimum Adrian Weller, Tony Jebara 2012-12-31 Inference in general Markov random fields (MRFs) is NP-hard, though identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with submodular cost functions is efficiently solvable using graph cuts. Marginal inference, however, even for this restricted class, is in \\#P. We prove new formulations of derivatives of the Bethe free energy, provide bounds on the derivatives and bracket the locations of stationary points, introducing a new technique called Bethe bound propagation. Several results apply to pairwise models whether associative or not. Applying these to discretized pseudo-marginals in the associative case we present a polynomial time approximation scheme for global optimization provided the maximum degree is $O(\\log n)$, and discuss several extensions. (pdf) Reconstructing Pong on an FPGA Stephen A. Edwards 2012-12-27 I describe in detail the circuitry of the original 1972 Pong video\r\narcade game and how I reconstructed it on an FPGA -- a modern-day\r\nprogrammable logic device. In the original circuit, I discover some\r\nsloppy timing and a previously unidentified bug that subtly affected\r\ngameplay.  I emulate the quasi-synchronous behavior of the original\r\ncircuit by running a synchronous ``simulation'' circuit with a\r\n2X clock and replacing each flip-flop with a circuit that\r\neffectively simulates one.  The result is an accurate reproduction\r\nthat exhibits many idiosyncracies of the original. (pdf) Focal Sweep Camera for Space-Time Refocusing Changyin Zhou, Daniel Miau, Shree K. Nayar 2012-11-29 A conventional camera has a limited depth of field (DOF), which often results in defocus blur and loss of image detail. The technique of image refocusing allows a user\r\nto interactively change the plane of focus and DOF of an image after it is captured. One way to achieve refocusing is to capture the entire light field. But this requires a significant compromise of spatial resolution. This is because of the dimensionality gap - the captured information (a light field) is 4-D, while the information required for refocusing (a focal stack) is only 3-D. \r\n\r\nIn this paper, we present an imaging system that directly captures a focal stack by physically sweeping the focal plane. We first describe how to sweep the focal plane \r\nso that the aggregate DOF of the focal stack covers the entire desired depth range without gaps or overlaps. Since the focal stack is captured in a duration of time when scene objects can move, we refer\r\nto the captured focal stack as a duration focal stack. We then propose an algorithm for computing a space-time in-focus index map from the focal stack, which represents the time at which each pixel is best focused. The algorithm is designed to enable a seamless refocusing experience, even for textureless regions and at depth discontinuities. \r\n\r\nWe have implemented two prototype focal-sweep cameras and captured several duration focal stacks. Results obtained using our method can be viewed at www.focalsweep.com. (pdf) Effectiveness of Teaching Metamorphic Testing Kunal Swaroop Mishra, Gail Kaiser 2012-11-15 This paper is an attempt to understand the effectiveness of teaching metamorphic properties in a senior/graduate software engineering course classroom environment through gauging the success achieved by students in identifying these properties on the basis of the lectures and materials provided in class. The main findings were: (1) most of the students either misunderstood what metamorphic properties are or fell short of identifying all the metamorphic properties in their respective projects, (2) most of the students that were successful in finding all the metamorphic properties in their respective projects had incorporated certain arithmetic rules into their project logic, and (3) most of the properties identified were numerical metamorphic properties. A possible reason for this could be that the two relevant lectures given in class cited examples of metamorphic properties that were based on numerical properties. Based on the findings of the case study, pertinent suggestions were made in order to improve the impact of lectures provided for Metamorphic Testing. (pdf) A Competitive-Collaborative Approach for Introducing Software Engineering in a CS2 Class Swapneel Sheth, Jonathan Bell, Gail Kaiser 2012-11-05 Introductory Computer Science (CS) classes are typically competitive in nature.\r\nThe cutthroat nature of these classes comes from students attempting to get as high a grade as possible, which may or may not correlate with actual learning.\r\nFurther, there is very little collaboration allowed in most introductory CS classes.\r\nMost assignments are completed individually since many educators feel that students learn the most, especially in introductory classes, by working alone.\r\nIn addition to completing ``normal'' individual assignments, which have many benefits, we wanted to expose students to collaboration early (via, for example, team projects).\r\nIn this paper, we describe how we leveraged competition and collaboration in a CS2 to help students learn aspects of computer science better --- in this case, good software design and software testing --- and summarize student feedback. (pdf) Increasing Student Engagement in Software Engineering with Gamification Swapneel Sheth, Jonathan Bell, Gail Kaiser 2012-11-05 Gamification, or the use of game elements in non-game contexts, has become an increasingly popular approach to increasing end-user engagement in many contexts, including employee productivity, sales, recycling, and education.\r\nOur preliminary work has shown that gamification can be used to boost student engagement and learning in basic software testing.\r\nWe seek to expand our gamified software engineering approach to motivate other software engineering best practices.\r\nWe propose to build a game layer on top of traditional continuous integration technologies to increase student engagement in development, documentation, bug reporting, and test coverage.\r\nThis poster describes to our approach and presents some early results showing feasibility. (pdf) Improving Vertical Accuracy of Indoor Positioning for Emergency Communication Wonsang Song, Jae Woo Lee, Byung Suk Lee, Henning Schulzrinne 2012-10-30 The emergency communication systems are undergoing a transition from\r\nthe PSTN-based legacy system to an IP-based next generation system.\r\nIn the next generation system, GPS accurately provides a user's\r\nlocation when the user makes an emergency call outdoors using a mobile\r\nphone.  Indoor positioning, however, presents a challenge because GPS\r\ndoes not generally work indoors.  Moreover, unlike outdoors, vertical\r\naccuracy is critical indoors because an error of few meters will send\r\nemergency responders to a different floor in a building. \r\n\r\nThis paper presents an indoor positioning system which focuses on\r\nimproving the accuracy of vertical location.  We aim to provide\r\nfloor-level accuracy with minimal infrastructure support.  Our\r\napproach is to use multiple sensors available in today's smartphones\r\nto trace users' vertical movements inside buildings.\r\n\r\nWe make three contributions.  First, we present the elevator module\r\nfor tracking a user's movement in elevators.  The elevator module\r\naddresses three core challenges that make it difficult to accurately\r\nderive displacement from acceleration.  Second, we present the\r\nstairway module which determines the number of floors a user has\r\ntraveled on foot.  Unlike previous systems that track users' foot\r\nsteps, our stairway module uses a novel landing counting technique.\r\nThird, we present a hybrid architecture that combines the sensor-based\r\ncomponents with minimal and practical infrastructure.  The\r\ninfrastructure provides initial anchor and periodic corrections of a\r\nuser's vertical location indoors.  The architecture strikes the right\r\nbalance between the accuracy of location and the feasibility of\r\ndeployment for the purpose of emergency communication. (pdf) An Autonomic Reliability Improvement System for Cyber-Physical Systems Leon Wu, Gail Kaiser 2012-09-17 System reliability is a fundamental requirement of cyber-physical systems. Unreliable systems can lead to disruption of service, financial cost and even loss of human life. Typical cyber-physical systems are designed to process large amounts of data, employ software as a system component, run online continuously and retain an operator-in-the-loop because of human judgment and accountability requirements for safety-critical systems. This paper describes a data-centric runtime monitoring system named ARIS (Autonomic Reliability Improvement System) for improving the reliability of these types of cyber-physical systems. ARIS employs automated online evaluation, working in parallel with the cyber-physical system to continuously conduct automated evaluation at multiple stages in the system workflow and provide real-time feedback for reliability improvement. This approach enables effective evaluation of data from cyber-physical systems. For example, abnormal input and output data can be detected and flagged through data quality analysis. As a result, alerts can be sent to the operator-in-the-loop, who can then take actions and make changes to the system based on these alerts in order to achieve minimal system downtime and higher system reliability. We have implemented ARIS in a large commercial building cyber-physical system in New York City, and our experiment has shown that it is effective and efficient in improving building system reliability. (pdf) Hardware-Accelerated Range Partitioning Lisa Wu, Raymond J. Barker, Martha A. Kim, Kenneth A. Ross 2012-09-05 With global pool of data growing at over 2.5 quinitillion bytes per day and over 90% of all data in existence created in the last two years alone [23], there can be little doubt that we have entered the big data era. This trend has brought database performance to the forefront of high throughput, low energy system design. This paper explores targeted deploy- ment of hardware accelerators to improve the throughput and efficiency of database processing. Partitioning, a critical operation when manipulating large data sets, is often the limiting factor in database performance, and represents a significant amount of the overall runtime of database processing workloads.\r\nThis paper describes a hardware-software streaming framework and a hardware accelerator for range partitioning, or HARP. The streaming framework offers seamless execution environment for database processing elements such as HARP. HARP offers performance, as well as orders of magnitude gains in power and area efficiency. A detailed analysis of a 32nm physical design shows 9.3 times the throughput of a highly optimized and optimistic software implementation, while consuming just 3.6% of the area and 2.6% of the power of a single Xeon core in the same technology generation. (pdf) End-User Regression Testing for Privacy Swapneel Sheth, Gail Kaiser 2012-08-25 Privacy in social computing systems has become a major concern.\r\nEnd-users of such systems find it increasingly hard to understand complex privacy settings.\r\nAs software evolves over time, this might introduce bugs that breach users' privacy.\r\nFurther, there might be system-wide policy changes that could change users' settings to be more or less private than before.\r\n\r\nWe present a novel technique that can be used by end-users for detecting changes in privacy, i.e., regression testing for privacy.\r\nUsing a social approach for detecting privacy bugs, we present two prototype tools.\r\nOur evaluation shows the feasibility and utility of our approach for detecting privacy bugs.\r\nWe highlight two interesting case studies on the bugs that were discovered using our tools.\r\nTo the best of our knowledge, this is the first technique that leverages regression testing for detecting privacy bugs from an end-user perspective. (pdf) Chronicler: Lightweight Recording to Reproduce Field Failures Jonathan Bell, Nikhil Sarda, Gail Kaiser 2012-08-23 When programs fail in the field, developers are often left with limited information to diagnose the failure. Automated error reporting tools can assist in bug report generation but without precise steps from the end user it is often difficult for developers to recreate the failure. Advanced remote debugging tools aim to capture sufficient information from field executions to recreate failures in the lab but often have too much overhead to practically deploy. We present CHRONICLER, an approach to remote debugging that captures non-deterministic inputs to applications in a lightweight manner, assuring faithful reproduction of client executions. We evaluated CHRONICLER by creating a Java implementation, CHRONICLERJ, and then by using a set of benchmarks mimicking real world applications and workloads, showing its runtime overhead to be under 10% in most cases (worst case 86%), while an existing tool showed overhead over 100% in the same cases (worst case 2,322%). (pdf) Statically Unrolling Recursion to Improve Opportunities for Parallelism Neil Deshpande, Stephen A. Edwards 2012-07-13 We present an algorithm for unrolling recursion in the Haskell\r\nfunctional language.  Adapted from a similar algorithm proposed by\r\nRugina and Rinard for imperative languages, it essentially inlines a\r\nfunction in itself as many times as requested.  This algorithm aims to\r\nincrease the available parallelism in recursive functions, with an eye\r\ntoward its eventual application in a Haskell-to-hardware compiler. We\r\nfirst illustrate the technique on a series of examples, then describe\r\nthe algorithm, and finally show its Haskell source, which operates as\r\na plug-in for the Glasgow Haskell Compiler. (pdf) Functional Fibonacci to a Fast FPGA Stephen A. Edwards 2012-06-16 Through a series of mechanical transformation, I show how a three-line\r\nrecursive Haskell function (Fibonacci) can be translated into a\r\nhardware description language -- VHDL -- for efficient execution on an\r\nFPGA.  The goal of this report is to lay the groundwork for a compiler\r\nthat will perform these transformations automatically, hence the\r\ndevelopment is deliberately pedantic. (pdf) High Throughput Heavy Hitter Aggregation Orestis Polychroniou, Kenneth A. Ross 2012-05-15 Heavy hitters are data items that occur at high frequency\r\nin a data set. Heavy hitters are among the most important\r\nitems for an organization to summarize and understand dur-\r\ning analytical processing. In data sets with sufficient skew,\r\nthe number of heavy hitters can be relatively small. We\r\ntake advantage of this small footprint to compute aggregate\r\nfunctions for the heavy hitters in fast cache memory.\r\nWe design cache-resident, shared-nothing structures that\r\nhold only the most frequent elements from the table. Our\r\napproach works in three phases. It first samples and picks\r\nheavy hitter candidates. It then builds a hash table and\r\ncomputes the exact aggregates of these candidates. Finally,\r\nif necessary, a validation step identifies the true heavy hitters\r\nfrom among the candidates based on the query specification.\r\nWe identify trade-offs between the hash table capacity and\r\nperformance. Capacity determines how many candidates\r\ncan be aggregated. We optimize performance by the use of\r\nperfect hashing and SIMD instructions. SIMD instructions\r\nare utilized in novel ways to minimize cache accesses, be-\r\nyond simple vectorized operations. We use bucketized and\r\ncuckoo hash tables to increase capacity, to adapt to different\r\ndatasets and query constraints.\r\nThe performance of our method is an order of magnitude\r\nfaster than in-memory aggregation over a complete set of\r\nitems if those items cannot be cache resident. Even for item\r\nsets that are cache resident, our SIMD techniques enable\r\nsignificant performance improvements over previous work. (pdf) Improving Efficiency and Reliability of Building Systems Using Machine Learning and Automated Online Evaluation Leon Wu, Gail Kaiser, David Solomon, Rebecca Winter, Albert Boulanger, Roger Anderson 2012-05-04 A high percentage of newly-constructed commercial office buildings experience energy consumption that exceeds specifications and system failures after being put into use. This problem is even worse for older buildings. We present a new approach, ‘predictive building energy optimization’, which uses machine learning (ML) and automated online evaluation of historical and real-time building data to improve efficiency and reliability of building operations without requiring large amounts of additional capital investment. Our ML approach uses a predictive model to generate accurate energy demand forecasts and automated analyses that can guide optimization of building operations. In parallel, an automated online evaluation system monitors efficiency at multiple stages in the system workflow and provides building operators with continuous feedback. We implemented a prototype of this application in a large commercial building in Manhattan. Our predictive machine learning model applies Support Vector Regression (SVR) to the building’s historical energy use and temperature and wet-bulb humidity data from the building’s interior and exterior in order to model performance for each day. This predictive model closely approximates actual energy usage values, with some seasonal and occupant-specific variability, and the dependence of the data on day-of-the-week makes the model easily applicable to different types of buildings with minimal adjustment. In parallel, an automated online evaluator monitors the building’s internal and external conditions, control actions and the results of those actions. Intelligent real-time data quality analysis components quickly detect anomalies and automatically transmit feedback to building management, who can then take necessary preventive or corrective actions. Our experiments show that this evaluator is responsive and effective in further ensuring reliable and energy-efficient operation of building systems. (pdf) Aperture Evaluation for Defocus Deblurring and Extended Depth of Field Changyin Zhou, Shree Nayar 2012-04-17 For a given camera setting, scene points that lie outside of depth of field (DOF) will appear defocused (or blurred). Defocus causes the loss of image details. To recover scene details from a defocused region, deblurring techniques must be employed. It is well known that the deblurring quality is closely related to the defocus kernel or point-spread-function (PSF), whose shape is largely determined by the aperture pattern of the camera. In this paper, we propose a comprehensive framework of aperture evaluation for the purpose of defocus deblurring, which takes the effects of image noise, deblurring algorithm, and the structure of natural images into account. By using the derived evaluation criterion, we are able to solve for the optimal coded aperture patterns. Extensive simulations and experiments are then conducted to compare the optimized coded apertures with previously proposed ones.\r\n\r\nThe proposed framework of aperture evaluation is further extended to evaluate and optimize extended depth of field (EDOF) cameras. EDOF cameras (e.g., focal sweep and wavefront coding camera) are designed to produce PSFs which are less sensitive to depth variation, so that people can deconvolve the whole image using a single PSF without knowing scene depth. Different choices of camera parameters or the PSF to deconvolve with lead to different deblurring qualities. With the derived evaluation criterion, we are able to derive the optimal PSF to deconvolve with in a closed-form and optimize camera parameters for the best deblurring results. (pdf) Partitioned Blockmap Indexes for Multidimensional Data Access Kenneth Ross, Evangelia Sitaridi 2012-04-16 Given recent increases in the size of main memory in modern machines, it is now common to to store large data sets in RAM for faster processing. Multidimensional access methods aim to provide efficient access to large data sets when queries apply predicates to some of the data dimensions. We examine multidimensional access methods in the context of an in-memory column store tuned for on-line analytical processing or scientific data analysis. We propose a multidimensional data structure that contains a novel combination of a grid array and several bitmaps. The base data is clustered in an order matching that of the index structure. The bitmaps contain one bit per block of data, motivating the\r\nterm ``blockmap.'' The proposed data structures are compact, typically taking less than one bit of space per row of data. Partition boundaries can be chosen in a way that reflects both the query workload and the data distribution, and boundaries are not required to evenly divide the data if there is a bias in the query distribution. We examine the theoretical performance of the data structure and experimentally measure its performance on three modern CPUs and one GPU processor. We demonstrate that efficient multidimensional access can be achieved with minimal space overhead. (pdf) Experiments of Image Retrieval Using Weak Attributes Felix X. Yu, Rongrong Ji, Ming-Hen Tsai, Guangnan Ye, Shih-Fu Chang 2012-04-06 Searching images based on descriptions of image attributes is an intuitive process that can be easily understood by humans and recently made feasible by a few promising works in both the computer vision and multimedia communities. In this report, we describe some experiments of image retrieval methods that utilize weak attributes. (pdf) When Does Computational Imaging Improve Performance? Oliver Cossairt, Mohit Gupta, Shree Nayar 2012-03-24 A number of computational imaging techniques have been introduced to improve image quality by increasing light throughput. These techniques use optical coding to measure a stronger signal level. However, the performance of these techniques is limited by the decoding step, which amplifies noise. While it is well understood that optical coding can increase performance at low light levels, little is known about the quantitative performance advantage of computational imaging in general settings. In this paper, we derive the performance bounds for various computational imaging techniques. We then discuss the implications of these bounds for several real-world scenarios (illumination conditions, scene properties and sensor noise characteristics). Our results show that computational imaging techniques provide a significant performance advantage in a surprisingly small set of real world settings. These results can be readily used by practitioners to design the most suitable imaging systems given the application at hand. (pdf) High Availability for Carrier-Grade SIP Infrastructure on Cloud Platforms Jong Yul Kim, Henning Schulzrinne 2012-03-19 SIP infrastructure on cloud platforms has the potential to be both scalable and highly available. In our previous project, we focused on the scalability aspect of SIP services on cloud platforms; the focus of this project is on the high availability aspect. We investigated the effects of component fault on service availability with the goal of understanding how high availability can be guaranteed even in the face of component faults. The experiments were conducted empirically on a real system that runs on Amazon EC2. Our analysis shows that most component faults are masked with a simple automatic failover technique. However, we have also identified fundamental problems that cannot be addressed by simple failover techniques; a problem involving DNS cache in resolvers and a problem involving static failover configurations. Recommendations on how to solve these problems are included in the report. (pdf) Automatic Detection of Metamorphic Properties of Software Sahar Hasan 2012-03-14 The goal of this project is to demonstrate the feasibility of automatic detection of\r\nmetamorphic properties of individual functions. Properties of interest here, as described\r\nin Murphy et al.’s SEKE 2008 paper “Properties of Machine Learning Applications for Use\r\nin Metamorphic Testing”, include:\r\n1. Permutation of the order of the input data\r\n2. Addition of numerical values by a constant\r\n3. Multiplication of numerical values by a constant\r\n4. Reversal of the order of the input data\r\n5. Removal of part of the data\r\n6. Addition of data to the dataset\r\nWhile focusing on permutative, additive, and multiplicative properties in functions and\r\napplications, I have sought to identify common programming constructs and code\r\nfragments that strongly indicate that these properties will hold, or fail to hold, along an\r\nexecution path in which the code is evaluated. I have constructed a syntax for\r\nexpressions representing these common constructs and have also mapped a collection\r\nof these expressions to the metamorphic properties they uphold or invalidate. I have\r\nthen developed a general framework to evaluate these properties for programs as a\r\nwhole. (pdf) CloudFence: Enabling Users to Audit the Use of their Cloud-Resident Data Vasilis Pappas, Vasileios P. Kemerlis, Angeliki Zavou, Michalis Polychronakis, Angelos D. Keromytis 2012-01-24 One of the primary concerns of users of cloud-based services and applications is the risk of unauthorized access to their private information. For the common setting in which the infrastructure provider and the online service provider are different, end users have to trust their data to both parties, although they interact solely with the service provider. This paper presents CloudFence, a framework that allows users to independently audit the treatment of their private data by third-party online services, through the intervention of the cloud provider that hosts these services.\r\nCloudFence is based on a fine-grained data flow tracking platform exposed by the cloud provider to both developers of cloud-based applications, as well as their users. Besides data auditing for end users, CloudFence allows service providers to confine the use of sensitive data in well-defined domains using data tracking at arbitrary granularity, offering additional protection against inadvertent leaks and unauthorized access. The results of our experimental evaluation with real-world applications, including an e-store platform and a cloud-based backup service, demonstrate that CloudFence requires just a few changes to existing application code, while it can detect and prevent a wide range of security breaches, ranging from data leakage attacks using SQL injection, to personal data disclosure due to missing or erroneously implemented access control checks. (pdf) Failure Analysis of the New York City Power Grid Leon Wu, Roger Anderson, Albert Boulanger, Cynthia Rudin, Gail Kaiser 2012-01-12 As U.S. power grid transforms itself into Smart Grid, it has become less reliable in the past years. Power grid\r\nfailures lead to huge financial cost and affect peopleâ€™s life. Using\r\na statistical analysis and holistic approach, this paper analyzes\r\nthe New York City power grid failures: failure patterns and\r\nclimatic effects. Our findings include: higher peak electrical load\r\nincreases likelihood of power grid failure; increased subsequent\r\nfailures among electrical feeders sharing the same substation;\r\nunderground feeders fail less than overhead feeders; cables and\r\njoints installed during certain years are more likely to fail; higher\r\nweather temperature leads to more power grid failures. We further\r\nsuggest preventive maintenance, intertemporal consumption,\r\nand electrical load optimization for failure prevention. We also\r\nestimated that the predictability of the power grid component\r\nfailures correlates with the cycles of the North Atlantic Oscillation\r\n(NAO) Index. (pdf) NetServ: Reviving Active Networks Jae Woo Lee, Roberto Francescangeli, Wonsang Song, Emanuele Maccherani, Jan Janak, Suman Srinivasan 2012-01-05 In 1996, Tennenhouse and Wetherall proposed active net- works, where users can inject code modules into network nodes. The proposal sparked intense debate and follow- on research, but ultimately failed to win over the net- working community. Fifteen years later, the problems that motivated the active networks proposal persist.\r\nWe call for a revival of active networks. We present NetServ, a fully integrated active network system that provides all the necessary functionality to be deployable, addressing the core problems that prevented the practical success of earlier approaches.\r\nWe make the following contributions. We present a hybrid approach to active networking, which combines the best qualities from the two extreme approachesï¿½ integrated and discrete. We built a working system that strikes the right balance between security and perfor- mance by leveraging current technologies. We suggest an economic model based on NetServ between content providers and ISPs. We built four applications to illus- trate the model. (pdf) A Large-Scale, Longitudinal Study of User Profiles in World of Warcraft Jonathan Bell, Swapneel Sheth, Gail Kaiser 2011-12-29 We present a survey of usage of the popular Massively Multiplayer Online Role Playing Game, World of Warcraft.\r\nPlayers within this game often self-organize into communities with similar interests and/or styles of play. By mining publicly available data, we collected a dataset consisting of the complete player history for approximately six million characters, with partial data for another six million characters. The paper provides a thorough description of the distributed approach used to collect this massive community data set, and then focuses on an analysis of player achievement data in particular, exposing trends in play from this highly successful game.\r\nFrom this data, we present several findings regarding player profiles. We correlate achievements with motivations based upon a previously-defined motivation model, and then classify players based on the categories of achievements that they pursued. Experiments show players who fall within each of these buckets can play differently, and that as players progress through game content, their play style evolves as well. (pdf) GRAND: Git Revisions As Named Data Jan Janak, Jae Woo Lee, Henning Schulzrinne 2011-12-12 GRAND is an experimental extension of Git, a distributed\r\nrevision control system, which enables the synchronization\r\nof Git repositories over Content-Centric Networks (CCN).\r\nGRAND brings some of the benefits of CCN to Git, such as\r\ntransparent caching, load balancing, and the ability to fetch\r\nobjects by name rather than location. Our implementation\r\nis based on CCNx, a reference implementation of content\r\nrouter. The current prototype consists of two components:\r\ngit-daemon-ccnx allows a node to publish its local Git\r\nrepositories to CCNx Content Store; git-remote-ccnx implements\r\nCCNx transport on the client side. This adds CCN\r\nto the set of transport protocols supported by Git, alongside\r\nHTTP and SSH. (pdf) ActiveCDN: Cloud Computing Meets Content Delivery Networks Suman Srinivasan, Jae Woo Lee, Dhruva Batni, Henning Schulzrinne 2011-11-15 Content  delivery  networks  play  a  crucial  role  in today’s  Internet.  They  serve  a  large  portion  of  the  multimedia on the Internet and solve problems of scalability and indirectly network congestion (at a price). However, most content delivery networks  rely  on  a  statically  deployed  configuration  of  nodes and  network  topology  that  makes  it  hard  to  grow  and  scale dynamically. We present ActiveCDN, a novel CDN architecture that allows a content publisher to dynamically scale their content delivery services using network virtualization and cloud computing techniques. (pdf) libdft: Practical Dynamic Data Flow Tracking for Commodity Systems Vasileios P. Kemerlis, Georgios Portokalidis, Kangkook Jee, Angelos D. Keromytis 2011-10-27 Dynamic data flow tracking (DFT) deals with the tagging and tracking of \"interesting\" data as they propagate during program execution. DFT has been repeatedly implemented by a variety of tools for numerous purposes, including protection from zero-day and cross-site scripting attacks, detection and prevention of information leaks, as well as for the analysis of legitimate and malicious software. We present libdft, a dynamic DFT framework that unlike previous work is at once fast, reusable, and works with commodity software and hardware. libdft provides an API, which can be used to painlessly deliver DFT-enabled tools that can be applied on unmodified binaries, running on common operating systems and hardware, thus facilitating research and rapid prototyping.\r\n\r\nWe explore different approaches for implementing the low-level aspects of instruction-level data tracking, introduce a more efficient and 64-bit capable shadow memory, and identify (and avoid) the common pitfalls responsible for the excessive performance overhead of previous studies. We evaluate libdft using real applications with large codebases like the Apache and MySQL servers, and the Firefox web browser. We also use a series of benchmarks and utilities to compare libdft with similar systems. Our results indicate that it performs at least as fast, if not faster, than previous solutions, and to the best of our knowledge, we are the first to evaluate the performance overhead of a fast dynamic DFT implementation in such depth. Finally, our implementation is freely available as open source software. (pdf) Money for Nothing and Privacy for Free? Swapneel Sheth, Tal Malkin, Gail Kaiser 2011-10-10 Privacy in the context of ubiquitous social computing systems has become a major concern for the society at large.\r\nAs the number of online social computing systems that collect user data grows, this privacy threat is further exacerbated.\r\nThere has been some work (both, recent and older) on addressing these privacy concerns.\r\nThese approaches typically require extra computational resources, which might be beneficial where privacy is concerned, but when dealing with Green Computing and sustainability, this is not a great option.\r\nSpending more computation time results in spending more energy and more resources that make the software system less sustainable.\r\nIdeally, what we would like are techniques for designing software systems that address these privacy concerns but which are also sustainable - systems where privacy could be achieved ``for free,'' i.e., without having to spend extra computational effort.\r\nIn this paper, we describe how privacy can be achieved for free - an accidental and beneficial side effect of doing some existing computation - and what types of privacy threats it can mitigate.\r\nMore precisely, we describe a ``Privacy for Free'' design pattern and show its feasibility, sustainability, and utility in building complex social computing systems. (pdf) Forecasting Energy Demand in Large Commercial Buildings Using Support Vector Machine Regression David Solomon, Rebecca Winter, Albert Boulanger, Roger Anderson, Leon Wu 2011-09-24 As our society gains a better understanding of how humans have negatively impacted the environment, research related to reducing carbon emissions and overall energy consumption has become increasingly important. One of the simplest ways to reduce energy usage is by making current buildings less wasteful. By improving energy efficiency, this method of lowering our carbon footprint is particularly worthwhile because it reduces energy costs of operating the building, unlike many environmental initiatives that require large monetary investments. In order to improve the efficiency of the heating, ventilation, and air conditioning (HVAC) system of a Manhattan skyscraper, 345 Park Avenue, a predictive computer model was designed to forecast the amount of energy the building will consume. This model uses Support Vector Machine Regression (SVMR), a method that builds a regression based purely on historical data of the building, requiring no knowledge of its size, heating and cooling methods, or any other physical properties. SVMR employs time-delay coordinates as a representation of the past to create the feature vectors for SVM training. This pure dependence on historical data makes the model very easily applicable to different types of buildings with few model adjustments. The SVM regression model was built to predict a week of future energy usage based on past energy, temperature, and dew point temperature data. (pdf) Privacy Enhanced Access Control for Outsourced Data Sharing Mariana Raykova, Hang Zhao, Steven Bellovin 2011-09-20 Traditional access control models often assume that the entity enforcing access control\r\npolicies is also the owner of data and resources. This assumption no longer holds \r\nwhen data is outsourced to a third-party storage provider, such as the \\emph{cloud}.\r\nExisting access control solutions mainly focus on preserving confidentiality of stored \r\ndata from unauthorized access and the storage provider. \r\nHowever, in this setting, access control policies as \r\nwell as users' access patterns also become privacy sensitive information that \r\nshould be protected from the cloud. We propose a two-level access control scheme\r\nthat combines coarse-grained access control enforced at the cloud, which allows to get \r\nacceptable communication overhead and at the same time limits the information that \r\nthe cloud learns from his partial view of the access rules and the access patterns, \r\nand fine-grained cryptographic access control enforced at the user's side, which provides the desired expressiveness \r\nof the access control policies. Our solution handles both \\emph{read} and \\emph{write} access control. (pdf) Stable Flight and Object Tracking with a Quadricopter using an Android Device Benjamin Bardin, William Brown, Paul S. Blaer 2011-09-09 We discuss a novel system architecture for quadricopter control, the Robocopter platform, in which the quadricopter can behave near-autonomously and processing is handled by an Android device on the quadricopter. The Android device communicates with a laptop, receiving commands from the host and sending imagery and sensor data back. We also discuss the results of a series of tests of our platform on our first hardware iteration, named Jabberwock. (pdf) NetServ on OpenFlow 1.0 Emanuele Maccherani, Jae Woo Lee, Mauro Femminella, Gianluca Reali, Henning Schulzrinne 2011-09-03 We describe the initial prototype implementation of OpenFlow-based NetServ. (pdf) Improving System Reliability for Cyber-Physical Systems Leon Wu 2011-08-31 System reliability is a fundamental requirement of Cyber-Physical System, i.e., a system featuring\r\na tight combination of, and coordination between, the systems computational and physical\r\nelements. Cyber-physical system includes systems ranging from the critical infrastructure such as\r\npower grid and transportation system to the health and biomedical devices. An unreliable system\r\noften leads to disruption of service, financial cost and even loss of human life. This thesis aims\r\nto improve system reliability for cyber-physical systems that meet following criteria: processing\r\nlarge amount of data; employing software as a system component; running online continuously;\r\nhaving operator-in-the-loop because of human judgment and accountability requirement for safety\r\ncritical systems. The reason that I limit the system scope to this type of cyber-physical system is\r\nthat this type of cyber-physical systems are important and becoming more prevalent.\r\nTo improve system reliability for this type of cyber-physical systems, I propose a system\r\nevaluation approach named automated online evaluation. It works in parallel with the cyber-physical\r\nsystem to conduct automated evaluation at the multiple stages along the workflow of\r\nthe system continuously and provide operator-in-the-loop feedback on reliability improvement.\r\nIt is an approach whereby data from cyber-physical system is evaluated. For example, abnormal\r\ninput and output data can be detected and flagged through data quality analysis. As a result, alerts\r\ncan be sent to the operator-in-the-loop. The operator can then take actions and make changes to\r\nthe system based on the alerts in order to achieve minimal system downtime and higher system\r\nreliability. To implement the proposed approach, I further propose a system architecture named\r\nARIS (Autonomic Reliability Improvement System).\r\nOne technique used by the approach is data quality analysis using computational intelligence\r\nthat applies computational intelligence in evaluating data quality in some automated and efficient\r\nway to ensure data quality and make sure the running system to perform as expected reliably.\r\nThe computational intelligence is enabled by machine learning, data mining, statistical and probabilistic\r\nanalysis, and other intelligent techniques. In a cyber-physical system, the data collected\r\nfrom the system, e.g., software bug reports, system status logs and error reports, are stored in\r\nsome databases. In my approach, these data are analyzed via data mining and other intelligent\r\ntechniques so that useful information on system reliability including erroneous data and abnormal\r\nsystem state can be concluded. These reliability related information are directed to operators so\r\nthat proper actions can be taken, sometimes proactively based on the predictive results, to ensure\r\nthe proper and reliable execution of the system.\r\nAnother technique used by the approach is self-tuning that automatically self-manages and\r\nself-configures the evaluation system to ensure it adapts itself based on the changes in the system\r\nand feedback from the operator. The self-tuning adapts the evaluation system to ensure its proper\r\nfunctioning, which leads to a more robust evaluation system and improved system reliability.\r\nFor feasibility study of the proposed approach, I first present NOVA (Neutral Online Visualization-aided\r\nAutonomic) system, a data quality analysis system for improving system reliability for\r\npower grid cyber-physical system. I then present a feasibility study on effectiveness of some\r\nself-tuning techniques, including data classification, redundancy checking and trend detection.\r\nThe self-tuning leads to an adaptive evaluation system that works better under system changes\r\nand operator feedback, which will lead to improved system reliability.\r\nThe contribution of the work is an automated online evaluation approach that is able to improve\r\nsystem reliability for cyber-physical systems in the domain of interest as indicated above. It\r\nenables online reliability assurance of the deployed systems that are not possible to perform robust\r\ntests prior to actual deployment. (pdf) Describable Visual Attributes for Face Images Neeraj Kumar 2011-08-01 We introduce the use of describable visual attributes for face images.                                                                   \r\nDescribable visual attributes are labels that can be given to an image to describe its appearance. This thesis focuses mostly on images of faces and the attributes used to describe them, although the concepts also apply to other domains.  Examples of face attributes include gender, age, jaw shape, nose size, etc.  The advantages of an attribute-based representation for vision tasks are manifold: they can be composed to create descriptions at various levels of specificity; they are generalizable, as they can be learned once and then applied to recognize new objects or categories without any further training; and they are efficient, possibly requiring exponentially fewer attributes (and training data) than explicitly naming each category. We show how one can create and label large datasets of real-world images to train classifiers which measure the presence, absence, or degree to which an attribute is expressed in images.  These classifiers can then automatically label new images.\r\n\r\nWe demonstrate the current effectiveness and explore the future potential of using attributes for image search, automatic face replacement in images, and face verification, via both human and computational experiments. To aid other researchers in studying these problems, we introduce two new large face datasets, named FaceTracer and PubFig, with labeled attributes and identities, respectively.\r\n\r\nFinally, we also show the effectiveness of visual attributes in a completely different domain: plant species identification. To this end, we have developed and publicly released the Leafsnap system, which has been downloaded by over half a million users. The mobile phone application is a flexible electronic field guide with high-quality images of the tree species in the Northeast US. It also gives users instant access to our automatic recognition system, greatly simplifying the identification process. (pdf) ICOW: Internet Access in Public Transit Systems Se Gi Hong, SungHoon Seo, Henning Schulzrinne, Prabhakar Chitrapu 2011-07-27 When public transportation stations have access points to provide Internet access to passengers, public transportation becomes a more attractive travel and commute option. However, the Internet connectivity is intermittent because passengers can access the Internet only when a bus or a train is within the networking coverage of an access point at a stop. To efficiently handle this intermittent network for the public transit system, we propose Internet Cache on Wheels (ICOW), a system that provides a low-cost way for bus and train operators to offer access to Internet content. Each bus or train car is equipped with a smart cache that serves popular content to passengers. The cache updates its content based on passenger requests when it is within range of Internet access points placed at bus stops, train stations or depots. We have developed a system architecture and built a prototype of the ICOW system.  Our evaluation and analysis show that ICOW is significantly more efficient than having passengers contact Internet access points individually and ensures continuous availability of content throughout the journey. (pdf) Data Quality Assurance and Performance Measurement of Data Mining for Preventive Maintenance of Power Grid Leon Wu, Gail Kaiser, Cynthia Rudin, Roger Anderson 2011-07-01 Ensuring reliability as the electrical grid morphs into the \"smart grid\" will require innovations in how we assess the state of the grid, for the purpose of proactive maintenance, rather than reactive maintenance; in the future, we will not only react to failures, but also try to anticipate and avoid them using predictive modeling (machine learning and data mining) techniques. To help in meeting this challenge, we present the Neutral Online Visualization-aided Autonomic evaluation framework (NOVA) for evaluating machine learning and data mining algorithms for preventive maintenance on the electrical grid. NOVA has three stages provided through a unified user interface: evaluation of input data quality, evaluation of machine learning and data mining results, and evaluation of the reliability improvement of the power grid. A prototype version of NOVA has been deployed for the power grid in New York City, and it is able to evaluate machine learning and data mining systems effectively\r\nand efficiently. (pdf) Columbia University WiMAX Campus Deployment and Installation SungHoon Seo, Jan Janak, Henning Schulzrinne 2011-06-27 This report describes WiMAX campus deployment and installation at Columbia University. (pdf) The Benefits of Using Clock Gating in the Design of Networks-on-Chip Michele Petracca, Luca P. Carloni 2011-06-21 Networks-on-chip (NoC) are critical to the design\r\nof complex multi-core system-on-chip (SoC) architectures. Since\r\nSoCs are characterized by a combination of high performance\r\nrequirements and stringent energy constraints, NoCs must be\r\nrealized with low-power design techniques. Since the use of semicustom\r\ndesign flow based on standard-cell technology libraries is\r\nessential to cope with the SoC design complexity challenges under\r\ntight time-to-market constraints, NoC must be implemented\r\nusing logic synthesis. In this paper we analyze the major power\r\nreduction that clock gating can deliver when applied to the\r\nsynthesis of a NoC in the context of a semi-custom automated\r\ndesign flow. (pdf) Secret Ninja Testing with HALO Software Engineering Jonathan Bell, Swapneel Sheth, Gail Kaiser 2011-06-21 Software testing traditionally receives little attention in early computer science courses. However, we believe that if exposed to testing early, students will develop positive habits for future work. As we have found that students typically are not keen on testing, we propose an engaging and socially-oriented approach to teaching software testing in introductory and intermediate computer science courses. Our proposal leverages the power of gaming utilizing our previously described system HALO. Unlike many previous approaches, we aim to present software testing in disguise - so that students do not recognize (at first) that they are being exposed to software testing. We describe how HALO could be integrated into course assignments as well as the benefits that HALO creates. (pdf) Markov Models for Network-Behavior Modeling and Anonymization Yingbo Song, Salvatore J. Stolfo, Tony Jebara 2011-06-15 Modern network security research has demonstrated a clear need for open sharing of traffic datasets between organizations, a need that has so far been superseded by the challenge of removing sensitive content beforehand. Network Data Anonymization (NDA) is emerging as a field dedicated to this problem, with its main direction focusing on removal of identifiable artifacts that might pierce privacy, such as usernames and IP addresses. However, recent research has demonstrated that more subtle statistical artifacts, also present, may yield fingerprints that are just as differentiable as the former. This result highlights certain shortcomings in current anonymization frameworks -- particularly, ignoring the behavioral idiosyncrasies of network protocols, applications, and users. Recent anonymization results have shown that the extent to which utility and privacy can be obtained is mainly a function of the information in the data that one is aware and not aware of. This paper leverages the predictability of network behavior in our favor to augment existing frameworks through a new machine-learning-driven anonymization technique. Our approach uses the substitution of individual identities with group identities where members are divided based on behavioral similarities, essentially providing anonymity-by-crowds in a statistical mix-net. We derive time-series models for network traffic behavior which quantifiably models the discriminative features of network \"behavior\" and introduce a kernel-based framework for anonymity which fits together naturally with network-data modeling. (pdf) Concurrency Attacks Junfeng Yang, Ang Cui, John Gallagher, Sal Stolfo, Simha Sethumadhavan 2011-06-02 Just as errors in sequential programs can lead to security\r\nexploits, errors in concurrent programs can lead to concurrency\r\nattacks. In this paper, we present an in-depth\r\nstudy of concurrency attacks and how they may affect existing\r\ndefenses. Our study yields several interesting findings.\r\nFor instance, we find that concurrency attacks can\r\ncorrupt non-pointer data, such as user identifiers, which\r\nexisting memory-safety defenses cannot handle. Inspired\r\nby our findings, we propose new defense directions and\r\nfixes to existing defenses. (pdf) Constructing Subtle Concurrency Bugs Using Synchronization-Centric Second-Order Mutation Operators Leon Wu, Gail Kaiser 2011-06-01 Mutation testing applies mutation operators to modify program source code or byte code in small ways, and then runs these modified programs (i.e., mutants) against a test suite in order to evaluate the quality of the test suite. In this paper, we first describe a general fault model for con- current programs and some limitations of previously developed sets of first-order concurrency mutation operators. We then present our new mutation testing approach, which em- ploys synchronization-centric second-order mutation operators that are able to generate subtle concurrency bugs not represented by the first-order mutation. These operators are used in addition to the synchronization-centric first-order mutation operators to form a small set of effective concurrency mutation operators for mutant generation. Our empirical study shows that our set of operators is effective in mutant generation with limited cost and demonstrates that this new approach is easy to implement. (pdf) BUGMINER: Software Reliability Analysis Via Data Mining of Bug Reports Leon Wu, Boyi Xie, Gail Kaiser, Rebecca Passonneau 2011-06-01 Software bugs reported by human users and automatic error reporting software are often stored in some bug track- ing tools (e.g., Bugzilla and Debbugs). These accumulated bug reports may contain valuable information that could be used to improve the quality of the bug reporting, reduce the quality assurance effort and cost, analyze software re- liability, and predict future bug report trend. In this paper, we present BUGMINER, a tool that is able to derive useful information from historic bug report database using data mining, use these information to do completion check and redundancy check on a new or given bug report, and to estimate the bug report trend using statistical analysis. Our empirical studies of the tool using several real-world bug report repositories show that it is effective, easy to implement, and has relatively high accuracy despite low quality data. (pdf) Evaluating Machine Learning for Improving Power Grid Reliability Leon Wu, Gail Kaiser, Cynthia Rudin, David Waltz, Roger Anderson, Albert Boulanger 2011-06-01 Ensuring reliability as the electrical grid morphs into the “smart grid” will require innovations in how we assess the state of the grid, for the purpose of proactive maintenance, rather than reactive maintenance – in the future, we will not only react to failures, but also try to anticipate and avoid them using predictive modeling (ma- chine learning) techniques. To help in meeting this challenge, we present the Neutral Online Visualization-aided Autonomic evaluation framework (NOVA) for evaluating machine learning algorithms for preventive maintenance on the electrical grid. NOVA has three stages provided through a unified user interface: evaluation of input data quality, evaluation of machine learning results, and evaluation of the reliability improvement of the power grid. A prototype version of NOVA has been deployed for the power grid in New York City, and it is able to evaluate machine learning systems effectively and efficiently. (pdf) Entropy, Randomization, Derandomization, and Discrepancy Michael Gnewuch 2011-05-24 Abstract The star discrepancy is a measure of how uniformly distributed a finite\r\npoint set is in the d-dimensional unit cube. It is related to high-dimensional numerical\r\nintegration of certain function classes as expressed by the Koksma-Hlawka\r\ninequality. A sharp version of this inequality states that the worst-case error of approximating\r\nthe integral of functions from the unit ball of some Sobolev space by\r\nan equal-weight cubature is exactly the star discrepancy of the set of sample points.\r\nIn many applications, as, e.g., in physics, quantum chemistry or finance, it is essential\r\nto approximate high-dimensional integrals. Thus with regard to the Koksma-\r\nHlawka inequality the following three questions are very important:\r\n(i) What are good bounds with explicitly given dependence on the dimension d for\r\nthe smallest possible discrepancy of any n-point set for moderate n?\r\n(ii) How can we construct point sets efficiently that satisfy such bounds?\r\n(iii) How can we calculate the discrepancy of given point sets efficiently?\r\nWe want to discuss these questions and survey and explain some approaches to\r\ntackle them relying on metric entropy, randomization, and derandomization. (pdf) (ps) A NEW RANDOMIZED ALGORITHM TO APPROXIMATE THE STAR DISCREPANCY BASED ON THRESHOLD ACCEPTING MICHAEL GNEWUCH, MAGNUS WAHLSTROM, CAROLA WINZEN 2011-05-24 Abstract. We present a new algorithm for estimating the star discrepancy of arbitrary point\r\nsets. Similar to the algorithm for discrepancy approximation of Winker and Fang [SIAM J. Numer.\r\nAnal. 34 (1997), 2028{2042] it is based on the optimization algorithm threshold accepting. Our\r\nimprovements include, amongst others, a non-uniform sampling strategy which is more suited for\r\nhigher-dimensional inputs and additionally takes into account the topological characteristics of given\r\npoint sets, and rounding steps which transform axis-parallel boxes, on which the discrepancy is to be\r\ntested, into critical test boxes. These critical test boxes provably yield higher discrepancy values, and\r\ncontain the box that exhibits the maximum value of the local discrepancy. We provide comprehensive\r\nexperiments to test the new algorithm. Our randomized algorithm computes the exact discrepancy\r\nfrequently in all cases where this can be checked (i.e., where the exact discrepancy of the point set\r\ncan be computed in feasible time). Most importantly, in higher dimension the new method behaves\r\nclearly better than all previously known methods. (pdf) (ps) Cells: A Virtual Mobile Smartphone Architecture Jeremy Andrus, Christoffer Dall, Alexander Van't Hoff, Oren Laadan, Jason Nieh 2011-05-24 Cellphones are increasingly ubiquitous, so much so that many users are\r\ninconveniently forced to carry multiple cellphones to accommodate\r\nwork, personal, and geographic mobility needs.  We present Cells, a\r\nvirtualization architecture for enabling multiple virtual\r\nsmartphones to run simultaneously on the same physical cellphone\r\ndevice in a securely isolated manner.  Cells introduces a usage model\r\nof having one foreground virtual phone and multiple background\r\nvirtual phones.  This model enables a new device namespace mechanism\r\nand novel device proxies that integrate with lightweight operating system virtualization to efficiently and securely multiplex phone hardware devices across multiple virtual phones while providing native hardware device performance to all applications.  Virtual phone features\r\ninclude fully-accelerated graphics for gaming, complete power\r\nmanagement features, and full telephony functionality with separately \r\nassignable telephone numbers and caller ID support.  We have\r\nimplemented a Cells prototype that supports multiple Android virtual\r\nphones on the same phone hardware.  Our performance results\r\ndemonstrate that Cells imposes only modest runtime and memory \r\noverhead, works seamlessly across multiple hardware devices including\r\nGoogle Nexus 1 and Nexus S phones and an NVIDIA tablet, and transparently runs all existing Android applications without any modifications. (pdf) Towards Diversity in Recommendations using Social Networks Swapneel Sheth, Jonathan Bell, Nipun Arora, Gail Kaiser 2011-05-17 While there has been a lot of research towards improving the accuracy of recommender systems, the resulting systems have tended to become increasingly narrow in suggestion variety.\r\nAn emerging trend in recommendation systems is to actively seek out diversity in recommendations, where the aim is to provide unexpected, varied, and serendipitous recommendations to the user.\r\nOur main contribution in this paper is a new approach to diversity in recommendations called ``Social Diversity,'' a technique that uses social network information to diversify recommendation results.\r\nSocial Diversity utilizes social networks in recommender systems to leverage the diverse underlying preferences of different user communities to introduce diversity into recommendations.\r\nThis form of diversification ensures that users in different social networks (who may not collaborate in real life, since they are in a different network) share information, helping to prevent siloization of knowledge and recommendations.\r\nWe describe our approach and show its feasibility in providing diverse recommendations for the MovieLens dataset. (pdf) Combining a Baiting and a User Search Profiling Techniques for Masquerade Detection Malek Ben Salem, Salvatore J. Stolfo 2011-05-06 Masquerade attacks are characterized by an adversary stealing\r\na legitimate user's credentials and using them to impersonate the victim\r\nand perform malicious activities, such as stealing information. Prior work\r\non masquerade attack detection has focused on pro\fling legitimate user behavior\r\nand detecting abnormal behavior indicative of a masquerade attack.\r\nLike any anomaly-detection based techniques, detecting masquerade attacks\r\nby pro\fling user behavior su\u000bers from a signi\fcant number of false positives.\r\nWe extend prior work and provide a novel integrated detection approach in\r\nthis paper. We combine a user behavior pro\fling technique with a baiting\r\ntechnique in order to more accurately detect masquerade activity. We show\r\nthat using this integrated approach reduces the false positives by 36% when\r\ncompared to user behavior pro\fling alone, while achieving almost perfect detection\r\nresults.We also show how this combined detection approach serves as\r\na mechanism for hardening the masquerade attack detector against mimicry\r\nattacks. (pdf) DYSWIS: Collaborative Network Fault Diagnosis - Of End-users, By End-users, For End-users Kyung Hwa Kim, Vishal Singh, Henning Schulzrinne 2011-05-05 With increase in application complexity, the need for network\r\nfaults diagnosis for end-users has increased. However,\r\nexisting failure diagnosis techniques fail to assist the endusers\r\nin accessing the applications and services.\r\nWe present DYSWIS, an automatic network fault detection\r\nand diagnosis system for end-users. The key idea is\r\ncollaboration of end-users; a node requests multiple nodes\r\nto diagnose a network fault in real time to collect diverse information\r\nfrom different parts of the networks and infer the\r\ncause of failure. DYSWIS leverages DHT network to search\r\nthe collaborating nodes with appropriate network properties\r\nrequired to diagnose a failure. The framework allows dynamic\r\nupdating of rules and probes into a running system.\r\nAnother key aspect is contribution of expert knowledge (rules\r\nand probes) by application developers, vendors and network\r\nadministrators; thereby enabling crowdsourcing of diagnosis\r\nstrategy for growing set of applications.\r\nWe have implemented the framework and the software\r\nand tested them using our test bed and PlanetLab to show\r\nthat several complex commonly occurring failures can be\r\ndetected and diagnosed successfully using DYSWIS, while\r\nsingle-user probe with traditional tools fails to pinpoint the\r\ncause of such failures. We validate that our base modules\r\nand rules are sufficient to detect infrastructural failures causing\r\nmajority of application failures. (pdf) NetServ Framework Design and Implementation 1.0 Jae Woo Lee, Roberto Francescangeli, Wonsang Song, Jan Janak, Suman Srinivasan, Michael S. Kester 2011-05-04 Eyeball ISPs today are under-utilizing an important asset: edge\r\nrouters.  We present NetServ, a programmable node architecture aimed\r\nat turning edge routers into distributed service hosting platforms.\r\nThis allows ISPs to allocate router resources to content publishers\r\nand application service pro\\-vi\\-ders motivated to deploy content and\r\nservices at the network edge.  This model provides important benefits\r\nover currently available solutions like CDN.  Content and services can\r\nbe brought closer to end users by dynamically installing and removing\r\ncustom modules as needed throughout the network.\r\n\r\nUnlike previous programmable router proposals which focused on\r\ncustomizing features of a router, NetServ focuses on deploying content\r\nand services.  All our design decisions reflect this change in focus.\r\nWe set three main design goals: a wide-area deployment, a multi-user\r\nexecution environment, and a clear economic benefit.  We built a\r\nprototype using Linux, NSIS signaling, and the Java OSGi framework.\r\nWe also implemented four prototype applications: ActiveCDN provides\r\npublisher-specific content distribution and processing; KeepAlive\r\nResponder and Media Relay reduce the infrastructure needs of telephony\r\nproviders; and Overload Control makes it possible to deploy more\r\nflexible algorithms to handle excessive traffic. (pdf) Estimation of System Reliability Using a Semiparametric Model Leon Wu, Timothy Teravainen, Gail Kaiser, Roger Anderson, Albert Boulanger, Cynthia Rudin 2011-04-20 An important problem in reliability engineering is\r\nto predict the failure rate, that is, the frequency with which\r\nan engineered system or component fails. This paper presents a\r\nnew method of estimating failure rate using a semiparametric\r\nmodel with Gaussian process smoothing. The method is able to\r\nprovide accurate estimation based on historical data and it does\r\nnot make strong a priori assumptions of failure rate pattern (e.g.,\r\nconstant or monotonic). Our experiments of applying this method\r\nin power system failure data compared with other models show\r\nits efficacy and accuracy. This method can be used in estimating\r\nreliability for many other systems, such as software systems or\r\ncomponents. (pdf) Beyond Trending Topics: Real-World Event Identification on Twitter Hila Becker, Mor Naaman, Luis Gravano 2011-03-25 User-contributed messages on social media sites such as Twitter have emerged as powerful, real-time means of information sharing on the Web. These short messages tend to reflect a variety of events in real time, earlier than other social media sites such as Flickr or YouTube, \r\nmaking Twitter particularly well suited as a source of real-time event content. In this paper, we explore approaches for analyzing the stream of Twitter messages to distinguish between messages about real-world events and non-event messages. Our approach relies on a rich family of aggregate statistics of topically similar message clusters, including temporal, social, topical, and Twitter-centric features. Our large-scale experiments over millions of Twitter messages show the effectiveness of our approach for surfacing real-world event content on Twitter. (pdf) Efficient, Deterministic and Deadlock-free Concurrency Nalini Vasudevan 2011-03-25 Concurrent programming languages are growing in importance with the advent\r\nof multicore systems. Two major concerns in any concurrent program are data\r\nraces and deadlocks. Each are potentially subtle bugs that can be caused by nondeterministic\r\nscheduling choices in most concurrent formalisms. Unfortunately,\r\ntraditional race and deadlock detection techniques fail on both large programs, and\r\nsmall programs with complex behaviors.\r\nWe believe the solution is model-based design, where the programmer is presented\r\nwith a constrained higher-level language that prevents certain unwanted\r\nbehavior. We present the SHIM model that guarantees the absence of data races by\r\neschewing shared memory.\r\nThis dissertation provides SHIM based techniques that aid determinism - models\r\nthat guarantee determinism, compilers that generate deterministic code and\r\nlibraries that provide deterministic constructs. Additionally, we avoid deadlocks,\r\na consequence of improper synchronization. A SHIM program may deadlock if it\r\nviolates a communication protocol. We provide efficient techniques for detecting\r\nand deterministically breaking deadlocks in programs that use the SHIM model.\r\nWe evaluate the efficiency of our techniques with a set of benchmarks. We\r\nhave also extended our ideas to other languages. The ultimate goal is to provide\r\ndeterministic deadlock-free concurrency along with efficiency. Our hope is that\r\nthese ideas will be used in the future while designing complex concurrent systems. (pdf) Implementing Zeroconf in Linphone Abhishek Srivastava, Jae Woo Lee, Henning Schulzrinne 2011-03-05 This report describes the motivation behind implementing Zeroconf in a open source SIP phone(Linphone) and the architecture of the solution implemented. It also describes the roadblocks encountered and how they were tackled in the implementation. It concludes with a few mentions about future enhancements that may be implemented on a later date. (pdf) Frank Miller: Inventor of the One-Time Pad Steven M. Bellovin 2011-03-01 The invention of the one-time pad is generally credited to Gilbert S. Vernam and\r\nJoseph O. Mauborgne.  We show that it was invented about 35 years earlier by a\r\nSacramento banker named\r\nFrank Miller.  We provide a tentative identification of which Frank Miller it\r\nwas, and speculate on whether or not Mauborgne might have known of Miller's work,\r\nespecially via his colleague Parker Hitt. (pdf) The Failure of Online Social Network Privacy Settings Michelle Madejski, Maritza Johnson, Steven M. Bellovin 2011-02-23 Increasingly, people are sharing \r\nsensitive personal information via online social networks (OSN).\r\nWhile such networks do permit users to control what they share with whom,\r\naccess control policies are notoriously difficult to configure correctly;\r\nthis raises the question of whether OSN users' privacy settings match their sharing intentions. \r\nWe present the results of an empirical evaluation that measures privacy attitudes and intentions\r\nand compares these against the privacy settings on Facebook. Our results indicate a serious mismatch: every one of the 65 participants\r\nin our study confirmed that at least one of the identified violations was in fact a sharing violation.\r\nIn other words, OSN users' privacy settings are incorrect.\r\nFurthermore, a majority of users cannot or will not fix such errors.\r\nWe conclude that the current approach to privacy settings is fundamentally flawed\r\nand cannot be fixed;\r\na fundamentally different approach is needed. We present recommendations to\r\nameliorate the current problems, as well as provide suggestions for future\r\nresearch. (pdf) HALO (Highly Addictive, sociaLly Optimized) Software Engineering Swapneel Sheth, Jonathan Bell, Gail Kaiser 2011-02-08 In recent years, computer games have become increasingly social and collaborative in nature.\r\nMassively multiplayer online games, in which a large number of players collaborate with each other to achieve common goals in the game, have become extremely pervasive.\r\nBy working together towards a common goal, players become more engrossed in the game.\r\nIn everyday work environments, this sort of engagement would be beneficial, and is often sought out.\r\nWe propose an approach to software engineering called HALO that builds upon the properties found in popular games, by turning work into a game environment.\r\nOur proposed approach can be viewed as a model for a family of prospective games that would support the software development process.\r\nUtilizing operant conditioning and flow theory, we create an immersive software development environment conducive to increased productivity.\r\nWe describe the mechanics of HALO and how it could fit into typical software engineering processes. (pdf) On Effective Testing of Health Care Simulation Software Christian Murphy, M.S. Raunak, Andrew King, Sanjien Chen, Christopher Imbriano, Gail Kaiser 2011-02-04 Health care professionals rely on software to simulate anatomical and\r\nphysiological elements of the human body for purposes of training, prototyping, and decision making. Software can also be used to simulate medical processes and protocols to measure cost effectiveness and resource utilization. Whereas much of the software engineering research into simulation software focuses on validation (determining that the simulation accurately models real-world activity), to date there has been little investigation into the testing of simulation software itself, that is, the ability to effectively search for errors in the implementation. This is particularly challenging because often there is no test oracle to indicate whether the results of the simulation are correct.  In this paper, we present an approach to systematically testing simulation software in the absence of test oracles, and evaluate the effectiveness of the technique. (pdf) Protocols and System Design, Reliability, and Energy Efficiency in Peer-to-Peer Communication Systems Salman Abdul Baset 2011-02-04 Modern Voice-over-IP (VoIP) communication systems provide a bundle of services to their users. These services range from the most basic voice-based services such as voice calls and voicemail to more advanced ones such as conferencing, voicemail-to-text, and online address books. Besides voice, modern VoIP systems provide video calls and video conferencing, presence, instant messaging (IM), and even desktop sharing services. These systems also let their users establish a voice, video, or a text session with devices in cellular, public switched telephone network (PSTN), or other VoIP networks.\r\n\r\nThe peer-to-peer (p2p) paradigm for building VoIP systems involves minimal or no use of managed servers and is therefore attractive from an administrative and economic perspective. However, the benefits of using p2p paradigm in VoIP systems are not without their challenges. First, p2p communication (VoIP) systems can be deployed in environ-\r\nments with varying requirements of scalability, connectivity, security, interoperability, and performance. These requirements bring forth the question of designing open and standardized protocols for diverse deployments. Second, the presence of restrictive network address\r\ntranslators (NATs) and firewalls prevents machines from directly exchanging packets and is problematic from the perspective of establishing direct media sessions. The p2p communication systems address this problem by using an intermediate peer with unrestricted\r\nconnectivity to relay the session or by preferring the use of TCP. This technique for addressing connectivity problems raises questions about the reliability and session quality of p2p communication systems compared with the traditional client-server VoIP systems. Third,\r\nwhile administrative overheads are likely to be lower in running p2p communication systems as compared to client-server, can the same be said about the energy efficiency? Fourth, what type of techniques can be used to gain insights into the performance of a deployed\r\np2p VoIP system like Skype?\r\n\r\nThe thesis addresses the challenges in designing, building, and analyzing peer-to-peer communication systems. The thesis presents Peer-to-Peer Protocol (P2PP), an open protocol for building p2p communication systems with varying operational requirements. P2PP\r\nis now part of the IETF's P2PSIP protocol and is on track to become an RFC. The thesis describes the design and implementation of OpenVoIP, a proof-of-concept p2p communication system to demonstrate the feasibility of P2PP and to explore issues in building p2p communication systems. The thesis introduces a simple and novel analytical model for analyzing the reliability of peer-to-peer communication systems and analyzes the feasibility of TCP for sending real-time traffic. The thesis then analyzes the energy efficiency of peer-to-peer and client-server VoIP systems and shows that p2p VoIP systems are less energy efficient than client-server even if the peers consume a small amount of energy for running the p2p network. Finally, the thesis presents an analysis of the Skype protocol which indicates that Skype is free-riding on the network bandwidth of universities. (pdf) Detecting Traffic Snooping in Anonymity Networks Using Decoys Sambuddho Chakravarty, Georgios Portokalidis, Michalis Polychronakis, Angelos D. Keroymtis 2011-02-03 Anonymous communication networks like Tor\r\npartially protect the confidentiality of their users' traffic by\r\nencrypting all intra-overlay communication.\r\nHowever, when the relayed traffic\r\nreaches the boundaries of the overlay network towards its actual\r\ndestination, the original user traffic is inevitably exposed.  At this\r\npoint, unless end-to-end encryption is used, sensitive user data can\r\nbe snooped by a malicious or compromised exit node, or by any other\r\nrogue network entity on the path towards the actual destination.\r\n\r\nWe explore the use of decoy traffic for the detection of traffic\r\ninterception on anonymous proxying systems.  Our approach is based on\r\nthe injection of traffic that exposes bait credentials for decoy\r\nservices that require user authentication.  Our aim is to entice\r\nprospective eavesdroppers to access decoy accounts on servers under\r\nour control using the intercepted credentials.  We have deployed our\r\nprototype implementation in the Tor network using decoy IMAP and SMTP\r\nservers. During the course of six months, our system detected eight\r\ncases of traffic interception that involved eight different Tor exit\r\nnodes.  We provide a detailed analysis of the detected incidents,\r\ndiscuss potential improvements to our system, and outline how our\r\napproach can be extended for the detection of HTTP session hijacking\r\nattacks. (pdf) POWER: Parallel Optimizations With Executable Rewriting Nipun Arora, Jonathan Bell, Martha Kim, Vishal Singh, Gail Kaiser 2011-02-01 The hardware industryï¿½s rapid development of multicore and many core hardware has outpaced the software industryï¿½s transition from sequential to parallel programs. Most applications are still sequential, and many cores on parallel machines remain unused. We propose a tool that uses data-dependence profiling and binary rewriting to parallelize executables without access to source code. Our technique uses Bernsteinï¿½s conditions to identify independent sets of basic blocks that\r\ncan be executed in parallel, introducing a level of granularity between fine-grained instruction level and coarse grained task level parallelism. We analyze dynamically generated control and data dependence graphs to find independent sets of basic blocks which can be parallelized.\r\nWe then propose to parallelize these candidates using binary rewriting techniques. Our technique aims to demonstrate the parallelism that remains in serial application by exposing concrete opportunities for parallelism. (pdf) Decoy Document Deployment for Eff\u000bective Masquerade Attack Detection Malek Ben Salem, Salvatore J. Stolfo 2011-01-30 Masquerade attacks pose a grave security problem that is a consequence of identity theft. Detecting masqueraders is very hard. Prior work has focused on pro\fling legitimate user behavior and detecting deviations from that normal behavior that could potentially signal an ongoing\r\nmasquerade attack. Such approaches suff\u000ber from high false positive rates.\r\nOther work investigated the use of trap-based mechanisms as a means\r\nfor detecting insider attacks in general. In this paper, we investigate the use of such trap-based mechanisms for the detection of masquerade at\r\ntacks. We evaluate the desirable properties of decoys deployed within a\r\nuser's \ffile space for detection.We investigate the trade-o\u000bs between these properties through two user studies, and propose recommendations for eff\u000bective masquerade detection using decoy documents based on findings from our user studies. (pdf) Data Collection and Analysis for Masquerade Attack Detection: Challenges and Lessons Learned Malek Ben Salem, Salvatore J. Stolfo 2011-01-30 Real-world large-scale data collection poses an important\r\nchallenge in the security fi\feld. Insider and masquerader attack data collection poses even a greater challenge. Very few organizations acknowledge such breaches because of liability concerns and potential implications on their market value. This caused the scarcity of real-world data sets that could be used to study insider and masquerader attacks. In this paper, we present the design, technical, and procedural challenges encountered during our own masquerade data gathering project. We also share some lessons learned from this several-year project related to the Institutional Review Board process and to user study design. (pdf) On Accelerators: Motivations, Costs, and Outlook Simha Sethumadhavan 2011-01-30 Some notes on accelerators. (pdf) Computational Cameras: Appraoches, Benefits and Limits Shree K. Nayar 2011-01-15 A computational camera uses a combination of optics and software to produce images that cannot be taken with traditional cameras. In the last decade, computational imaging has emerged as a vibrant field of research. A wide variety of computational cameras have been demonstrated - some designed to achieve new imaging functionalities and others to reduce the complexity of traditional imaging.\r\nIn this article, we describe how computational cameras have evolved and present a taxonomy for the technical approaches they use. We explore the benefits and limits of computational imaging, and describe how it is related to the adjacent and overlapping fields of digital imaging, computational photography and computational image sensors. (pdf) Weighted Geometric Discrepancies and Numerical Integration on Reproducing Kernel Hilbert Spaces Michael Gnewuch 2010-12-22 We extend the notion of L2-B-discrepancy introduced in [E. Novak, H. Wo´zniakowski, L2 discrepancy and multivariate integration, in: Analytic number theory. Essays in honour of Klaus Roth. W. W. L. Chen, W. T. Gowers, H. Halberstam, W. M. Schmidt, and R. C. Vaughan (Eds.), Cambridge University Press, Cambridge, 2009, 359 – 388] to what we want to call weighted geometric L2-discrepancy. This extended notion allows us to consider weights to moderate the importance of different groups of variables, and additionally volume measures different from the Lebesgue measure as well as classes of test sets different from measurable subsets of Euclidean spaces. \r\n\r\nWe relate the weighted geometric L2-discrepancy to numerical integration defined over weighted reproducing kernel Hilbert spaces and settle in this way an open problem posed by Novak and Wo´zniakowski. \r\n\r\nFurthermore, we prove an upper bound for the numerical integration error for cubature formulas that use admissible sample points. The set of admissible sample points may actually be a subset of the integration domain of measure zero. We illustrate that particularly in infinite dimensional numerical integration it is crucial to distinguish between the whole integration domain and the set of those sample points that actually can be used by algorithms. (pdf) (ps) A Comprehensive Survey of Voice over IP Security Research Angelos D. Keromytis 2010-12-22 We present a comprehensive survey of Voice over IP security academic research, using a set of 245 publications forming a closed cross-citation set. We classify these papers according to an extended version of the VoIP Security Alliance (VoIPSA) Threat Taxonomy. Our goal is to provide a roadmap for researchers seeking to understand existing capabilities and to identify gaps in addressing the numerous threats and vulner- abilities present in VoIP systems. We discuss the implications of our findings with respect to vulnerabilities reported in a variety of VoIP products.\r\n\r\nWe identify two specific problem areas (denial of service, and service abuse) as requiring significant more attention from the research community. We also find that the overwhelming majority of the surveyed work takes a black box view of VoIP systems that avoids examining their internal structure and implementation. Such an approach may miss the mark in terms of addressing the main sources of vulnerabilities, i.e., implementation bugs and misconfigurations. Finally, we argue for further work on understanding cross-protocol and cross-mechanism vulnerabilities (emergent properties), which are the byproduct of a highly complex system-of-systems and an indication of the issues in future large-scale systems. (pdf) Modeling User Search Behavior for Masquerade Detection Malek Ben Salem, Salvatore J. Stolfo 2010-12-13 Masquerade attacks are a common security problem\r\nthat is a consequence of identity theft. Masquerade detection may\r\nserve as a means of building more secure and dependable systems\r\nthat authenticate legitimate users by their behavior. Prior work\r\nhas focused on user command modeling to identify abnormal\r\nbehavior indicative of impersonation. This paper extends prior\r\nwork by modeling user search behavior to detect deviations indicating\r\na masquerade attack. We hypothesize that each individual\r\nuser knows their own file system well enough to search in a\r\nlimited, targeted and unique fashion in order to find information\r\ngermane to their current task. Masqueraders, on the other\r\nhand, will likely not know the file system and layout of another\r\nuserï¿½s desktop, and would likely search more extensively and\r\nbroadly in a manner that is different than the victim user being\r\nimpersonated. We devise a taxonomy of Windows applications\r\nand user commands that are used to abstract sequences of\r\nuser actions and identify actions linked to search activities. The\r\nexperimental results show that modeling search behavior reliably\r\ndetects all masqueraders with a very low false positive rate of\r\n1.1%, far better than prior published results. The limited set of\r\nfeatures used for search behavior modeling also results in large\r\nperformance gains over the same modeling techniques that use\r\nlarger sets of features. (pdf) The Tradeoffs of Societal Computing Swapneel Sheth, Gail Kaiser 2010-12-10 As Social Computing has increasingly captivated the general public, it\r\nhas become a popular research area for computer scientists. Social\r\nComputing research focuses on online social behavior and using\r\nartifacts derived from it for providing recommendations and other\r\nuseful community knowledge. Unfortunately, some of that behavior and\r\nknowledge incur societal costs, particularly with regards to Privacy,\r\nwhich is viewed quite differently by different populations as well as\r\nregulated differently in different locales. But clever technical\r\nsolutions to those challenges may impose additional societal costs,\r\ne.g., by consuming substantial resources at odds with Green Computing,\r\nanother major area of societal concern. We propose a new crosscutting\r\nresearch area, \\emph{Societal Computing}, that focuses on the\r\ntechnical tradeoffs among computational models and application domains\r\nthat raise significant societal issues.  We highlight some of the\r\nrelevant research topics and open problems that we foresee in Societal\r\nComputing.\r\nWe feel that\r\nthese topics, and Societal Computing in general, need to gain prominence\r\nas they will provide useful avenues of research leading to\r\nincreasing benefits for society as a whole. (pdf) NetServ: Early Prototype Experiences Michael S. Kester, Eric Liu, Jae Woo Lee, Henning Schulzrinne 2010-12-03 This paper describes a work-in-progress to demonstrate the feasibility of integrating services in the Internet core. The project aims to reduce or eliminate so called ossification of the Internet. Here we discuss the recent contributions of two of the team members at Columbia University. We will describe experiences setting up a Juniper router, running packet forwarding tests, preparing for the GENI demo, and starting prototype 2 of NetServ. (pdf) Towards using Cached Data Mining for Large Scale Recommender Systems Swapneel Sheth, Gail Kaiser 2010-11-01 Recommender systems are becoming increasingly popular.\r\nAs these systems become commonplace and the number of users increases, it will become important for these systems to be able to cope with a large and diverse set of users whose recommendation needs may be very different from each other.\r\nIn particular, large scale recommender systems will need to ensure that users' requests for recommendations can be answered with low response times and high throughput. \r\nIn this paper, we explore how to use caches and cached data mining to improve the performance of recommender systems by improving throughput and reducing response time for providing recommendations. \r\nWe describe the structure of our cache, which can be viewed as a prefetch cache that prefetches all types of supported recommendations, and how it is used in our recommender system.\r\nWe also describe the results of our simulation experiments to measure the efficacy of our cache. (pdf) Automatic Detection of Defects in Applications without Test Oracles Christian Murphy, Gail Kaiser 2010-10-29 In application domains that do not have a test oracle, such as machine learning and scientific computing, quality assurance is a challenge because it is difficult or impossible to know in advance what the correct output should be for general input. Previously, metamorphic testing has been shown to be a simple yet effective technique in detecting defects, even without an oracle. In metamorphic testing, the application's ``metamorphic properties'' are used to modify existing test case input to produce new test cases in such a manner that, when given the new input, the new output can easily be computed based on the original output. If the new output is not as expected, then a defect must exist. In practice, however, metamorphic testing can be a manually intensive technique for all but the simplest cases. The transformation of input data can be laborious for large data sets, and errors can occur in comparing the outputs when they are very complex. In this paper, we present a tool called Amsterdam that automates metamorphic testing by allowing the tester to easily set up and conduct metamorphic tests with little manual intervention, merely by specifying the properties to check, configuring the framework, and running the software. Additionally, we describe an approach called Heuristic Metamorphic Testing, which addresses issues related to false positives and non-determinism, and we present the results of new empirical studies that demonstrate the effectiveness of metamorphic testing techniques at detecting defects in real-world programs without test oracles. (pdf) Bypassing Races in Live Applications with Execution Filters Jingyue Wu, Heming Cui, Junfeng Yang 2010-09-30 Deployed multithreaded applications contain many races\r\nbecause these applications are difficult to write, test, and\r\ndebug. Worse, the number of races in deployed applications\r\nmay drastically increase due to the rise of multicore\r\nhardware and the immaturity of current race detectors.\r\n\r\nLOOM is a “live-workaround” system designed to\r\nquickly and safely bypass application races at runtime.\r\nLOOM provides a flexible and safe language for developers\r\nto write execution filters that explicitly synchronize\r\ncode. It then uses an evacuation algorithm to safely install\r\nthe filters to live applications to avoid races. It reduces\r\nits performance overhead using hybrid instrumen-\r\ntation that combines static and dynamic instrumentation.\r\n\r\nWe evaluated LOOM on nine real races from a diverse\r\nset of six applications, including MySQL and Apache.\r\nOur results show that (1) LOOM can safely fix all evaluated\r\nraces in a timely manner, thereby increasing application\r\navailability; (2) LOOM incurs little performance\r\noverhead; (3) LOOM scales well with the number of application\r\nthreads; and (4) LOOM is easy to use. (pdf) Baseline: Metrics for setting a baseline for web vulnerability scanners Huning Dai, Michael Glass, Gail Kaiser 2010-09-22 As web scanners are becoming more popular because they are faster and cheaper than security consultants, the trend of relying on these scanners also brings a great hazard: users can choose a weak or outdated scanner and trust incomplete results. Therefore, benchmarks are created to both evaluate and compare the scanners. Unfortunately, most existing benchmarks suffer from various drawbacks, often by testing against inappropriate criteria that does not reflect the user's needs. To deal with this problem, we present an approach called Baseline that coaches the user in picking the minimal set of weaknesses (i.e., a baseline) that a qualified scanner should be able to detect and also helps the user evaluate the effectiveness and efficiency of the scanner in detecting those chosen weaknesses. Baseline's goal is not to serve as a generic ranking system for web vulnerability scanners, but instead to help users choose the most appropriate scanner for their specific needs. (pdf) Tractability of the Fredholm problem of the second kind Arthur G. Werschulz, Henryk Wozniakowski 2010-09-21 We study the tractability of computing $\\varepsilon$-approximations of the\r\nFredholm problem of the second kind: given $f\\in F_d$ and $q\\in Q_{2d}$,\r\nfind $u\\in L_2(I^d)$ satisfying\r\n\\[                                                                              \r\n  u(x) - \\int_{I^d} q(x,y)u(y)\\,dy = f(x)                                       \r\n  \\qquad\\forall\\,x\\in I^d=[0,1]^d.                                              \r\n\\]\r\nHere, $F_d$ and $Q_{2d}$ are spaces of $d$-variate right hand functions and\r\n$2d$-variate kernels that are continuously embedded in~$L_2(I^d)$\r\nand~$L_2(I^{2d})$, respectively. We consider the worst case setting, measuring\r\nthe approximation error for the solution $u$ in the $L_2(I^d)$-sense.  We say\r\nthat a problem is tractable if the minimal number of information operations\r\nof $f$ and $q$ needed to obtain an $\\varepsilon$-approximation is\r\nsub-exponential in $\\varepsilon^{-1}$ and~$d$. One information operation\r\ncorresponds to the evaluation of one linear functional or one function\r\nvalue. The lack of sub-exponential behavior may be defined in various ways,\r\nand so we have various kinds of tractability.  In particular, the problem\r\nis strongly polynomially tractable if the minimal number of information\r\noperations is bounded by a polynomial in $\\varepsilon^{-1}$ for all~$d$.\r\n\r\nWe show that tractability (of any kind whatsoever) for the Fredholm problem\r\nis equivalent to tractability of the $L_2$-approximation problems over the\r\nspaces of right-hand sides and kernel functions.  So (for example) if both\r\nthese approximation problems are strongly polynomially tractable, so is the\r\nFredholm problem.  In general, the upper bound provided by this proof is\r\nessentially non-constructive, since it involves an interpolatory algorithm\r\nthat exactly solves the Fredholm problem (albeit for finite-rank\r\napproximations of~$f$ and~$q$).  However, if linear functionals are\r\npermissible and that $F_d$ and~$Q_{2d}$ are tensor product spaces, we are\r\nable to surmount this obstacle; that is, we provide a fully-constructive\r\nalgorithm that provides an approximation with nearly-optimal cost, i.e.,\r\none whose cost is within a factor $\\ln\\,\\varepsilon^{-1}$ of being optimal. (pdf) Trade-offs in Private Search Vasilis Pappas, Mariana Raykova, Binh Vo, Steven M. Bellovin, Tal Malkin 2010-09-17 Encrypted search --- performing queries on protected data --- is a well\r\nresearched problem.  However, existing solutions have inherent\r\ninefficiency that raises questions of practicality.\r\nHere, we step back from the goal of achieving maximal privacy\r\nguarantees in an encrypted search scenario to consider efficiency as\r\na priority. We propose a privacy\r\nframework for search that allows tuning and optimization of the\r\ntrade-offs between privacy and efficiency.\r\nAs an instantiation of\r\nthe privacy framework we introduce a tunable search system based on\r\nthe SADS scheme and provide detailed measurements demonstrating the\r\ntrade-offs of the constructed system. We also analyze other existing\r\nencrypted search schemes with respect to this framework. We further\r\npropose a protocol that addresses the challenge of document content \r\nretrieval in a search setting with relaxed privacy requirements. (pdf) Simple-VPN: Simple IPsec Configuration Shreyas Srivatsan, Maritza Johnson, Steven M. Bellovin 2010-07-12 The IPsec protocol promised easy, ubiquitous encryption.  That has never happened.  For the most part, IPsec usage is confined to VPNs for road warriors, largely due to needless configuration complexity and incompatible implementations. We have designed a simple VPN configuration language that hides the unwanted complexities.  Virtually no options are necessary or possible.  The administrator specifies the absolute minimum of information: the authorized hosts, their operating systems, and a little about the network topology; everything else, including certificate generation, is automatic. Our implementation includes a multitarget compiler, which generates implementation-specific configuration files for three different platforms; others are easy to add. (pdf) Infinite-Dimensional Integration on Weighted Hilbert Spaces Michael Gnewuch 2010-05-21 We study the numerical integration problem for functions with\r\ninfinitely many variables. The functions we want to integrate\r\nare from a reproducing kernel Hilbert space which is endowed with \r\na weighted norm. \r\nWe study the worst case $\\epsilon$-complexity which\r\nis defined as the minimal cost among all algorithms whose worst\r\ncase error over the Hilbert space unit ball is at most $\\epsilon$.\r\nHere we assume that the\r\ncost of evaluating a function depends polynomially on the number\r\nof active variables.\r\n\r\nThe infinite-dimensional integration problem is (polynomially) \r\ntractable if the\r\n$\\epsilon$-complexity is bounded by a constant times a power of \r\n$1/\\epsilon$. The smallest such power is called the exponent of \r\ntractability. \r\n\r\nFirst we study finite-order weights. We provide improved lower\r\nbounds for the exponent of tractability for general finite-order weights\r\nand improved upper bounds for three newly defined classes\r\nof finite-order weights. \r\nThe constructive upper bounds are obtained by multilevel algorithms\r\nthat use for each level quasi-Monte Carlo integration points whose\r\nprojections onto specific sets of coordinates exhibit a small \r\ndiscrepancy. \r\n\r\nThe newly defined finite-intersection weights model the situation where\r\neach group of variables interacts with at most $\\rho$ other groups \r\nof variables, where $\\rho$ is some fixed number.  \r\nFor these weights we obtain a sharp upper bound. This is the\r\nfirst class of weights for which the exact exponent of tractability\r\nis known for any possible decay of the weights and for any polynomial\r\ndegree of the cost function. For  the other two classes of finite-order\r\nweights our upper bounds are sharp if, e.g., \r\nthe decay of the \r\nweights\r\nis fast or slow enough.\r\n\r\nWe extend our analysis to the case of arbitrary weights.\r\nIn particular, from our results for finite-order\r\nweights, we conclude a lower bound on the exponent \r\nof tractability for arbitrary weights and a constructive upper bound for \r\nproduct weights. \r\n\r\nAlthough we confine ourselves for simplicity to \r\nexplicit upper bounds for four classes of \r\nweights, we stress that our multilevel algorithm together with our\r\ndefault choice of quasi-Monte Carlo points \r\nis applicable to any class of weights. (pdf) (ps) Huning Dai's Master's Thesis Huning Dai 2010-05-13 Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations and inputs together with a certain runtime environment. One approach to detecting these vulnerabilities is fuzz testing that feeds randomly generated inputs to the software and witnesses its failures. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, we present a new testing methodology called Configuration Fuzzing. Configuration Fuzzing is a technique whereby the configuration of the running application is mutated at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks \"security invariants'' that, if violated, indicate a vulnerability. We discuss the approach and introduce a prototype framework called ConFu (CONfiguration FUzzing testing framework) implementation. We also present the results of case studies that demonstrate the approach's feasibility and evaluate its performance. (pdf) Modeling User Search-Behavior for Masquerade Detection Malek Ben Salem, Shlomo Hershkop, Salvatore J Stolfo 2010-05-12 Masquerade attacks are a common security problem that is\r\na consequence of identity theft. Prior work has focused on user command\r\nmodeling to identify abnormal behavior indicative of impersonation. This\r\npaper extends prior work by modeling user search behavior to detect\r\ndeviations indicating a masquerade attack. We hypothesize that each\r\nindividual user knows their own \fle system well enough to search in a\r\nlimited, targeted and unique fashion in order to \fnd information germane\r\nto their current task. Masqueraders, on the other hand, will likely\r\nnot know the \fle system and layout of another user's desktop, and would\r\nlikely search more extensively and broadly in a manner that is di\u000berent\r\nthan the victim user being impersonated. We extend prior research by\r\ndevising taxonomies of UNIX commands and Windows applications that\r\nare used to abstract sequences of user commands and actions. The experimental\r\nresults show that modeling search behavior reliably detects\r\nall masqueraders with a very low false positive rate of 0.13%, far better\r\nthan prior published results. The limited set of features used for search\r\nbehavior modeling also results in large performance gains over the same\r\nmodeling techniques that use larger sets of features. (pdf) The weHelp Reference Architecture for Community-Driven Recommender Systems Swapneel Sheth, Nipun Arora, Christian Murphy, Gail Kaiser 2010-05-11 Recommender systems have become increasingly popular. Most research on recommender systems has focused on recommendation algorithms. There has been relatively little research, however, in the area of generalized system architectures for recommendation systems. In this paper, we introduce weHelp - a reference architecture for social recommender systems. Our architecture is designed to be application and domain agnostic, but we briefly discuss here how it applies to recommender systems for software engineering. (pdf) Comparing Speed of Provider Data Entry: Electronic Versus Paper Methods Kevin M. Jackson, Gail Kaiser, Lyndon Wong, Daniel Rabinowitz, Michael F. Chiang 2010-05-11 Electronic health record (EHR) systems have significant potential advantages over traditional paper-based systems, but they require that providers assume responsibility for data entry. One significant barrier to adoption of EHRs is the perception of slowed data-entry by providers. This study compares the speed of data-entry using computer-based templates vs. paper for a large eye clinic, using 10 subjects and 10 simulated clinical scenarios. Dataentry into the EHR was significantly slower (p<0.01) than traditional paper forms. (pdf) Empirical Study of Concurrency Mutation Operators for Java Leon Wu, Gail Kaiser 2010-04-29 Mutation testing is a white-box fault-based software\r\ntesting technique that applies mutation operators to\r\nmodify program source code or byte code in small ways and\r\nthen runs these modified programs (i.e., mutants) against a\r\ntest suite in order to measure its effectiveness and locate the\r\nweaknesses either in the test data or in the program that are\r\nseldom or never exposed during normal execution. In this paper, we describe our implementation of a generic\r\nmutation testing framework and the results of applying three\r\nsets of concurrency mutation operators on four example Java\r\nprograms through empirical study and analysis. (pdf) Metamorphic Testing Techniques to Detect Defects in Applications without Test Oracles Christian Murphy 2010-04-27 Applications in the fields of scientific computing, simulation, optimization, machine learning, etc. are sometimes said to be \"non-testable programs\" because there is no reliable test oracle to indicate what the correct output should be for arbitrary input. In some cases, it may be impossible to know the program's correct output a priori; in other cases, the creation of an oracle may simply be too hard. These applications typically fall into a category of software that Weyuker describes as \"Programs which were written in order to determine the answer in the first place. There would be no need to write such programs, if the correct answer were known.\" The absence of a test oracle clearly presents a challenge when it comes to detecting subtle errors, faults, defects or anomalies in software in these domains.  \r\n\r\nWithout a test oracle, it is impossible to know in general what the expected output should be for a given input, but it may be possible to predict how changes to the input should effect changes in the output, and thus identify expected relations among a set of inputs and among the set of their respective outputs. This approach, introduced by Chen et al., is known as \"metamorphic testing\". In metamorphic testing, if test case input x produces an output f(x), the function's so-called \"metamorphic properties\" can then be used to guide the creation of a transformation function t, which can then be applied to the input to produce t(x); this transformation then allows us to predict the expected output f(t(x)), based on the (already known) value of f(x). If the new output is as expected, it is not necessarily right, but any violation of the property indicates a defect. That is, though it may not be possible to know whether an output is correct, we can at least tell whether an output is incorrect.\r\n\r\nThis thesis investigates three hypotheses. First, I claim that an automated approach to metamorphic testing will advance the state of the art in detecting defects in programs without test oracles, particularly in the domains of machine learning, simulation, and optimization. To demonstrate this, I describe a tool for test automation, and present the results of new empirical studies comparing the effectiveness of metamorphic testing to that of other techniques for testing applications that do not have an oracle. Second, I suggest that conducting function-level metamorphic testing in the context of a running application will reveal defects not found by metamorphic testing using system-level properties alone, and introduce and evaluate a new testing technique called Metamorphic Runtime Checking. Third, I hypothesize that it is feasible to continue this type of testing in the deployment environment (i.e., after the software is released), with minimal impact on the user, and describe a generalized approach called In Vivo Testing. \r\n\r\nAdditionally, this thesis presents guidelines for identifying metamorphic properties, explains how metamorphic testing fits into the software development process, and discusses suggestions for both practitioners and researchers who need to test software without the help of a test oracle. (pdf) Robust, Efficient, and Accurate Contact Algorithms David Harmon 2010-04-26 Robust, efficient, and accurate contact response remains a challenging problem in the simulation of deformable materials. Contact models should robustly handle contact between geometry by preventing interpenetrations. This should be accomplished while respecting natural laws in order to maintain physical correctness. We simultaneously desire to achieve these criteria as efficiently as possible to minimize simulation runtimes. Many methods exist that partially achieve these properties, but none yet fully attain all three. This thesis investigates existing methodologies with respect to these attributes, and proposes a novel algorithm for the simulation of deformable materials that demonstrate them all. This new method is analyzed and optimized, paving the way for future work in this simplified but powerful manner of simulation. (pdf) A Real-World Identity Management System with Master Secret Revocation Elli Androulaki, Binh Vo, Steven Bellovin 2010-04-21 Cybersecurity mechanisms have become increasingly important as online and offline worlds converge. Strong authentication and accountability are key tools for dealing with online attacks, and we would like to realize them through a token-based, centralized identity management system. In this report, we present aprivacy-preserving group of protocols comprising a unique per user digital identity card, with which its owner is able to authenticate himself, prove possession of attributes, register himself to multiple online organizations  (anonymously or not) and provide proof of membership.  Unlike\r\nexisting credential-based identity management systems, this card is revocable, i.e., its legal owner may invalidate it if physically lost, and still recover its content and registrations into a new credential. \r\nThis card will protect an honest individual's anonymity when applicable as well as ensure his activity is known only to appropriate users. (pdf) Quasi-Polynomial Tractability Michael Gnewuch, Henryk Wozniakowski 2010-04-09 Tractability of multivariate problems has become  nowadays\r\na popular research subject. Polynomial tractability means that the solution \r\nof a d-variate problem can be solved to within $\\varepsilon$ with \r\npolynomial cost in $\\varepsilon^{-1}$ and d. Unfortunately, many\r\nmultivariate problems are not polynomially tractable. \r\nThis holds for all non-trivial unweighted linear tensor product problems.\r\nBy an unweighted problem we mean the case when all variables and\r\ngroups of variables play the same role. \r\n\r\nIt seems natural to ask what is the\r\n``smallest'' non-exponential function $T:[1,\\infty)\\times\r\n[1,\\infty)\\to[1,\\infty)$ for which we have\r\nT-tractability of unweighted linear tensor product problems. That is, when\r\nthe cost of a multivariate problem can be bounded \r\nby a multiple of a power of $T(\\varepsilon^{-1},d)$. \r\nUnder natural assumptions, it turns out that this function is\r\n$T^{qpol}(x,y):=\\exp((1+\\ln\\,x)(1+\\ln y))$\r\nfor all $x,y\\in[1,\\infty)$.\r\nThe function $T^{qpol}$ goes to infinity faster than any\r\npolynomial although not ``much'' faster, and that is why we refer to \r\n$T^{qpol}$-tractability as quasi-polynomial tractability.\r\n\r\nThe main purpose of this paper is to promote quasi-polynomial\r\ntractability especially for the study of unweighted multivariate problems.\r\nWe do this for the worst case and randomized settings and for\r\nalgorithms using arbitrary linear functionals or only function values. \r\nWe prove relations between quasi-polynomial tractability \r\nin these two settings and for the two classes of algorithms. (pdf) (ps) BotSwindler: Tamper Resistant Injection of Believable Decoys in VM-Based Hosts for Crimeware Detection Brian M. Bowen, Pratap Prabhu, Vasileios P. Kemerlis, Stelios Sidiroglou-Douskos, Angelos D. Keromytis, Salvatore J. Stolfo 2010-04-09 We introduce BotSwindler, a bait injection system designed to delude and detect crimeware by forcing it to reveal itself during the exploitation of monitored information. Our implementation of BotSwindler relies upon an out-of-host software agent to drive user-like interactions in a virtual machine, seeking to convince malware residing within the guest OS that it has captured legitimate credentials. To aid in the accuracy and realism of the simulations, we introduce a low overhead approach, called virtual machine verification, for verifying whether the guest OS is in one of a predefined set of states. We provide empirical evidence to show that BotSwindler can be used to induce malware into performing observable actions and demonstrate how this approach is superior to that used in other tools. We present results from a user study to illustrate the believability of the simulations and show that financial bait information can be used to effectively detect compromises through experimentation with real credential-collecting malware. (pdf) Privacy-Preserving, Taxable Bank Accounts Elli Androulaki, Binh Vo, Steven Bellovin 2010-04-07 Current banking systems do not aim to protect user privacy.  Purchases made from a single bank account can be linked to each other by many parties. This could be addressed in a straight-forward way by generating unlinkable credentials from a single master credential using\r\nCamenisch and Lysyanskaya's algorithm; however, if bank accounts are\r\ntaxable, some report must be made to the tax authority about each account. Using unlinkable credentials, digital cash, and zero knowledge proofs of knowledge, we present a solution that prevents anyone, even the tax authority, from knowing which accounts belong to which users, or from being able to link any account to another or to purchases or deposits. (pdf) CONFU: Configuration Fuzzing Testing Framework for Software Vulnerability Detection Huning Dai, Christian Murphy, Gail Kaiser 2010-02-19 Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations and inputs together with a certain runtime environment. One approach to detecting these vulnerabilities is fuzz testing. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, we present a new testing methodology called Configuration Fuzzing. Configuration Fuzzing is a technique whereby the configuration of the running application is mutated at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks \"security invariants'' that, if violated, indicate a vulnerability. We discuss the approach and introduce a prototype framework called ConFu (CONfiguration FUzzing testing framework) for implementation. We also present the results of case studies that demonstrate the approach's feasibility and evaluate its performance. (pdf) Empirical Evaluation of Approaches to Testing Applications without Test Oracles Christian Murphy, Gail Kaiser 2010-02-05 Software testing of applications in fields like scientific  omputation, simulation, machine learning, etc. is particularly challenging because many applications in these domains have no reliable \"test oracle\" to indicate whether the program's output is correct when given arbitrary input. A common approach to testing such applications has been to use a \"pseudo-oracle\", in which multiple independently-developed implementations of an algorithm process an input and the results are compared. Other approaches include the use of\r\nprogram invariants, formal specification languages, trace and log file analysis, and metamorphic testing.\r\n\r\nIn this paper, we present the results of two empirical studies in which we compare the effectiveness of some of these approaches, including metamorphic testing, pseudo-oracles, and runtime assertion checking. We also analyze the results in terms of the software development process, and discuss suggestions for practitioners and researchers who need to test software without a test oracle. (pdf) Automatic Detection of Previously-Unseen Application States for Deployment Environment Testing and Analysis Christian Murphy, Moses Vaughan, Waseem Ilahi, Gail Kaiser 2010-01-19 For large, complex software systems, it is typically impossible in terms of time and cost to reliably test the application in all possible execution states and configurations before releasing it into production. One proposed way of addressing this problem has been to continue testing and analysis of the application in the field, after it has been deployed. The theory behind this \"perpetual testing\" approach is that over time, defects will reveal themselves given that multiple instances of the same application may be run globally with different configurations, in different environments, under different patterns of usage, and in different system states.\r\n\r\nA practical limitation of many automated approaches to deployment environment testing and analysis is the potentially high performance overhead incurred by the necessary instrumentation. However, it may be possible to reduce this overhead by selecting test cases and performing analysis only in previously-unseen application states, thus reducing the number of redundant tests and analyses that are run. Solutions for fault detection, model checking, security testing, and fault localization in deployed software may all benefit from a technique that ignores application states that have already been tested or explored.\r\n\r\nIn this paper, we apply such a technique to a testing methodology called \"In Vivo Testing\", which conducts tests in deployed applications, and present a solution that ensures that tests are only executed in states that the application has not previously encountered. In addition to discussing our implementation, we present the results of an empirical study that demonstrates its effectiveness, and explain how the new approach can be generalized to assist other automated testing and analysis techniques. (pdf) Testing and Validating Machine Learning Classifiers by Metamorphic Testing Xiaoyuan Xie, Joshua W. K. Ho, Christian Murphy, Gail Kaiser, Baowen Xu, Tsong Yueh Chen 2010-01-11 Machine Learning algorithms have provided important core functionality to support solutions in many scientific computing applications - such as computational biology, computational linguistics, and others. However, it is difficult to test such applications because often there is no \"test oracle\" to indicate what the correct output should be for arbitrary input. To help address the quality of scientific computing software, in this paper we present a technique for testing the implementations of machine learning classification algorithms on which such scientific computing software depends. Our technique is based on an approach called \"metamorphic testing\", which has been shown to be effective in such cases. Also presented is a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has very high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficient to test for the correctness of a supervised classification program. Metamorphic testing is strongly recommended as a complementary approach. Finally we discuss how our findings can be used in other areas of computational science and engineering. (pdf) ONEChat: Enabling Group Chat and Messaging in Opportunistic Networks Heming Cui, Suman Srinivasan, Henning Schulzrinne 2010-01-04 Opportunistic networks, which are wireless network \"islands\" formed when transient and highly mobile nodes meet for a short period of time, are becoming commonplace as wireless devices become more and more popular. It is thus imperative to develop communication tools and applications that work well in opportunistic networks. In particular, group chat and instant messaging applications are particularly lacking for such opportunistic networks today.\r\n\r\nIn this paper, we present ONEChat, a group chat and instant messaging program that works in such opportunistic networks. ONEChat uses message multicasting on top of service discovery protocols in order to support group chat and reduce bandwidth consumption in opportunistic networks. ONEChat does not require any pre-configuration, a fixed network infrastructure or a client-server architecture in order to operate. In addition, it supports features such as group chat, private rooms, line-by-line or character-by-character messaging, file transfer, etc.\r\n\r\nWe also present our quantitative analysis of ONEChat, which we believe indicates that the ONEChat architecture is an efficient group collaboration platform for opportunistic networks. (pdf) Exploiting Local Logic Structures to Optimize Multi-Core SoC Floorplanning Cheng-Hong Li, Sampada Sonalkar, Luca P. Carloni 2009-12-10 We present a throughput-driven partitioning and a\r\nthroughput-preserving merging algorithm for the high-level physical\r\nsynthesis of latency-insensitive (LI) systems.  These two algorithms\r\nare integrated along with a published floorplanner in a\r\nnew iterative physical synthesis flow to optimize system throughput\r\nand reduce area occupation.  The synthesis flow iterates a\r\nfloorplanning-partitioning-floorplanning-merging sequence of\r\noperations to improve the system topology and the physical locations\r\nof cores.  The partitioning algorithm performs bottom-up clustering of\r\nthe internal logic of a given IP core to divide it into smaller ones,\r\neach of which has no combinational path from input to output and thus\r\nis legal for LI-interface encapsulation.  Applying this algorithm to\r\ncores on critical feedback loops optimizes their length and in turn\r\nenables throughput optimization via the subsequent floorplanning.\r\nThe merging algorithm reduces the number of cores on non-critical\r\nloops, lowering the overall area taken by LI interfaces without\r\nhurting the system throughput.  Experimental results on a large\r\nsystem-on-chip design show a 16.7% speedup in system throughput and\r\na 2.1% reduction in area occupation. (pdf) ConFu: Configuration Fuzzing Framework for Software Vulnerability Detection Huning Dai, Gail E. Kaiser 2009-12-08 Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations of the software and certain inputs together with its particular runtime environment. One approach to detecting these vulnerabilities is fuzz testing, which feeds a range of randomly modified inputs to a software application while monitoring it for failures. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, in this proposal we present a new testing methodology called Configuration Fuzzing. Configuration Fuzzing is a technique whereby the configuration of the running application is mutated at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks \"security invariants\" that, if violated, indicate a vulnerability; however, the fuzzing is performed in a duplicated copy of the original process, so that it does not affect the state of the running application. Configuration Fuzzing uses a covering array algorithm when fuzzing the configuration which guarantees a certain degree of coverage of the configuration space in the lifetime of the program-under-test. In addition, Configuration Fuzzing tests that are run after the software is released ensure representative real-world user inputs to test with. In addition to discussing the approach and describing a prototype framework for implementation, we also present the results of case studies to prove the approach's feasibility and evaluate its performance.\r\n\r\nIn this thesis, we will continue developing the framework called ConFu (CONfiguration FUzzi ng framework) that supports the generation of test functions, parallel sandboxed execution and vulnerability detection. Given the initial ConFu, we will optimize the way that configurations get mutated, define more security invariants and conduct additional empirical studies of ConFu's effectiveness in detecting vulnerabilities.\r\n\r\nAt the conclusion of this work, we want to prove that ConFu is efficient and effective in detecting common vulnerabilities and tests executed by ConFu can ensure reasonable degree of coverage of both the configuration and user input space in the lifetime of the software. (pdf) Record and Transplay: Partial Checkpointing for Replay Debugging Dinesh Subhraveti, Jason Nieh 2009-11-21 Software bugs that occur in production are often difficult to reproduce in the lab due to subtle differences in the application\r\n environment and nondeterminism. Toward addressing this problem, we present Transplay, a system that captures application software bugs as they occur in production and deterministically reproduces them in a completely different environment, potentially running a different operating system, where the application, its binaries and other support data do not exist. Transplay introduces partial checkpointing, a new mechanism that provides two key properties. It efficiently captures the minimal state necessary to reexecute just the last few moments of the application before it encountered a failure. The recorded state, which typically consists of a few megabytes of data, is used to replay the application without requiring the specific application binaries or the original execution environment. Transplay integrates with existing debuggers to provide facilities such as breakpoints and single-stepping to allow the user to examine the contents of variables and other program state at each source line of the application’s replayed execution. We have implemented a Transplay prototype that can record unmodified Linux applications and replay them on different versions of Linux as well as Windows. Experiments with server applications such as the Apache web server show that Transplay can be used in production with modest recording overhead. (pdf) On TCP-based SIP Server Overload Control Charles Shen, Henning Schulzrinne 2009-11-10 SIP server overload management has attracted interest recently as SIP becomes the core signaling protocol for Next Generation Networks. Yet virtually all existing SIP overload control work is focused on SIP-over-UDP, despite the fact that TCP is increasingly seen as the more viable choice of SIP transport. This paper answers the following questions: is the existing TCP flow control capable of handling the SIP overload problem? If not, why and how can we make it work? We provide a comprehensive explanation of the default SIP-over-TCP overload behavior through server instrumentation. We also propose and implement novel but\r\nsimple overload control algorithms without any kernel or protocol level modification. Experimental evaluation shows that with our mechanism the overload performance improves from its original zero throughput to nearly full capacity. Our work also leads to the important high level insight that the traditional notion of TCP flow control alone is incapable of managing overload for time-critical session based applications, which would be applicable not only to SIP, but also to\r\na wide range of other common applications such as database servers. (pdf) PBS: Signaling architecture for network traffic authorization Se Gi Hong, Henning Schulzrinne, Swen Weiland 2009-10-27 We present a signaling architecture for network traffic authorization, Permission-Based Sending (PBS). This architecture aims to prevent Denial-of-Service (DoS) attacks and other forms of unauthorized traffic. Towards this goal, PBS takes a hybrid approach: a proactive approach of explicit permissions and a reactive approach of monitoring and countering attacks. On-path signaling is used to configure the permission state stored in routers for a data flow. The signaling approach enables easy installation and management of the permission state, and its use of soft-state improves robustness of the system. For secure permission state setup, PBS provides security for signaling in two ways: signaling messages are encrypted end-to-end using public key encryption and TLS provides hop-by-hop encryption of signaling paths.\r\nIn addition, PBS uses IPsec for data packet authentication. Our analysis and performance evaluation show that PBS is an effective and scalable solution for preventing various kinds of attack scenarios, including Byzantine attacks. (pdf) A Secure and Privacy-Preserving Targeted Ad System Elli Androulaki, Steven Bellovin 2009-10-22 Thanks to its low product-promotion cost and its efficiency, targeted online advertising has become very popular. Unfortunately, being profile-based, online advertising methods violate consumers' privacy, which has engendered resistance to the ads. However, protecting privacy\r\nthrough anonymity seems to encourage click-fraud. In this paper, we define consumer's privacy and present a privacy-preserving, targeted ad system (PPOAd) which is resistant towards click fraud. Our scheme is structured to provide financial incentives to to all entities involved. (pdf) Rank-Aware Subspace Clutering for Structured Datasets Julia Stoyanovich, Sihem Amer-Yahia 2009-10-21 In online applications such as Yahoo! Personals and Trulia.com users define structured profiles in order to find potentially interesting matches.  Typically, profiles are evaluated against large datasets and\r\nproduce thousands of matches. In addition to filtering, users also specify ranking in their profile, and matches are returned in the form\r\nof a ranked list. Top results in ranked lists are typically homogeneous, which hinders data exploration. For example, a user\r\nlooking for 1- or 2-bedroom apartments sorted by price will see a\r\nlarge number of cheap 1-bedrooms in undesirable neighborhoods before\r\nseeing any apartment with different characteristics. An alternative to\r\nranking is to group matches on common attribute values (e.g., cheap\r\n1-bedrooms in good neighborhoods, 2-bedrooms with 2 baths). However,\r\nnot all groups will be of interest to the user given the ranking\r\ncriteria.\r\n\r\nWe argue here that neither single-list ranking nor attribute-based\r\ngrouping is adequate for effective exploration of ranked datasets. We\r\nformalize rank-aware clustering and develop a novel rank-aware\r\nbottom-up subspace clustering algorithm. We evaluate the performance\r\nof our algorithm over large datasets from a leading online dating\r\nsite, and present an experimental evaluation of its effectiveness. (pdf) Metamorphic Runtime Checking of Non-Testable Programs Christian Murphy, Gail Kaiser 2009-10-20 Challenges arise in assuring the quality of applications that do not have test oracles, i.e., for which it is impossible to know what the correct output should be for arbitrary input. Metamorphic testing has been shown to be a simple yet effective technique in addressing the quality assurance of these \"non-testable programs\". In metamorphic testing, if test input x produces output f(x), specified \"metamorphic properties\" are used to create a transformation function t, which can be applied to the input to produce t(x); this transformation then allows the output f(t(x)) to be predicted based on the already-known value of f(x). If the output is not as expected, then a defect must exist.\r\n\r\nPreviously we investigated the effectiveness of testing based on metamorphic properties of the entire application. Here, we improve upon that work by presenting a new technique called Metamorphic Runtime Checking, a testing approach that automatically conducts metamorphic testing of individual functions during the program's execution. We also describe an implementation framework called Columbus, and discuss the results of empirical studies that demonstrate that checking the metamorphic properties of individual functions increases the effectiveness of the approach in detecting defects, with minimal performance impact. (pdf) Configuration Fuzzing for Software Vulnerability Detection Huning Dai, Christian Murphy, Gail Kaiser 2009-10-07 Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations of the software together with its particular runtime environment. One approach to detecting these vulnerabilities is fuzz testing, which feeds a range of randomly modified inputs to a software application while monitoring it for failures. However, fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, in this paper we present a new testing methodology called configuration fuzzing. Configuration fuzzing is a technique whereby the configuration of the running application is randomly modified at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks \"security invariants\" that, if violated, indicate a vulnerability; however, the fuzzing is performed in a duplicated copy of the original process, so that it does not affect the state of the running application. In addition to discussing the approach and describing a prototype framework for implementation, we also present the results of a case study to demonstrate the approach’s efficiency. (pdf) Curtailed Online Boosting Raphael Pelossof, Michael Jones 2009-09-28 The purpose of this work is to lower the average number of features that are evaluated by an online algorithm. This is achieved by merging Sequential Analysis and Online Learning. \r\nMany online algorithms use the example's margin to decide whether the model should be updated. Usually, the algorithm's model is updated when the margin is smaller than a certain threshold. The evaluation of the margin for each example requires the algorithm to evaluate all the model's features. Sequential Analysis allows us to early stop the computation of the margin when uninformative examples are encountered. It is desirable to save computation on uninformative examples since they will have very little impact on the final model.\r\nWe show the successful speedup of Online Boosting while maintaining accuracy on a synthetic and the MNIST data sets. (pdf) Using a Model Checker to Determine Worst-case Execution Time Sungjun Kim, Hiren D. Patel, Stephen A. Edwards 2009-09-03 Hard real-time systems use worst-case execution\r\ntime (WCET) estimates to ensure that timing requirements\r\nare met. The typical approach for obtaining WCET estimates\r\nis to employ static program analysis methods. While these\r\napproaches provide WCET bounds, they struggle to analyze\r\nprograms with loops whose iteration counts depend on input data.\r\nSuch programs mandate user-guided annotations. We propose a\r\nhybrid approach by augmenting static program analysis with\r\nmodel-checking to analyze such programs and derive the loop\r\nbounds automatically. In addition, we use model-checking to\r\nguarantee repeatable timing behaviors from segments of program\r\ncode. Our target platform is a precision timed architecture: a\r\nSPARC-based architecture promising predictable and repeatable\r\ntiming behaviors. We use CBMC and illustrate our approach\r\non Euclidean greatest common divisor algorithm (for WCET\r\nanalysis) and a VGA controller (for repeatable timing validation). (pdf) Smashing the Stack with Hydra: The Many Heads of Advanced  Polymorphic Shellcode Pratap V. Prabhu, Yingbo Song, Salvatore J. Stolfo 2009-08-31 Recent work on the analysis of polymorphic shellcode engines suggests that modern obfuscation methods would soon eliminate the usefulness of signature-based network intrusion detection methods and supports growing views that the new generation of shellcode cannot be accurately and efficiently represented by the string signatures which current IDS and AV scanners rely upon. In this paper, we expand on this area of study by demonstrating never before seen concepts in advanced shellcode polymorphism with a proof-of-concept engine which we call Hydra. Hydra distinguishes itself by integrating an array of obfuscation techniques, such as recursive NOP sleds and multi-layer ciphering into one system while offering multiple improvements upon existing strategies. We also introduce never before seen attack methods such as byte-splicing statistical mimicry, safe-returns with forking shellcode and syscall-time-locking. In total, Hydra simultaneously attacks signature, statistical, disassembly, behavioral and emulation-based sensors, as well as frustrates ofﬂine forensics. This engine was developed to present an updated view of the frontier of modern polymorphic shellcode and provide an effective tool for evaluation of IDS systems, Cyber test ranges and other related security technologies. (pdf) On the Learnability of Monotone Functions Homin K. Lee 2009-08-13 A longstanding lacuna in the field of computational learning theory is the learnability of succinctly representable monotone Boolean functions, i.e., functions that preserve the given order of the input. This thesis makes significant progress towards understanding both the\r\npossibilities and the limitations of learning various classes of monotone functions by carefully considering the complexity measures used to evaluate them.\r\n\r\nWe show that Boolean functions computed by polynomial-size monotone circuits are hard to learn assuming the existence of one-way functions. Having shown the hardness of learning general polynomial-size monotone circuits, we show that the class of Boolean functions computed by polynomial-size depth-3 monotone circuits are hard to learn using statistical queries. As a counterpoint, we give a statistical query learning algorithm that can learn random polynomial-size depth-2 monotone circuits (i.e., monotone DNF formulas).\r\n\r\nAs a preliminary step towards a fully polynomial-time, proper learning algorithm for learning polynomial-size monotone decision trees, we also show the relationship between the average depth of a monotone decision tree, its average sensitivity, and its variance.\r\n\r\nFinally, we return to monotone DNF formulas, and we show that they are teachable (a different model of learning) in the average case. We also show that non-monotone DNF formulas, juntas, and sparse GF2 formulas are teachable in the average case. (pdf) Mouth-To-Ear Latency in Popular VoIP Clients Chitra Agastya, Dan Mechanic, Neha Kothari 2009-08-06 Most popular instant messaging clients are now offering Voiceover-\r\nIP (VoIP) technology. The many options running on similar\r\nplatforms, implementing common audio codecs and encryption\r\nalgorithms offers the opportunity to identify what factors affect\r\ncall quality. We measure call quality objectively based on mouthto-\r\near latency. Based on our analysis we determine that the\r\nmouth-to-ear latency can be influenced by operating system\r\n(process priority and interrupt handling), the VoIP client\r\nimplementation and network quality. (pdf) Apiary: Easy-to-use Desktop Application Fault Containment on Commodity Operating Systems Shaya Potter, Jason Nieh 2009-08-05 Desktop computers are often compromised by the interaction of untrusted data and buggy software. To address this problem, we present Apiary, a system that provides transparent application fault containment while retaining the ease of use of a traditional integrated desktop environment. Apiary accomplishes this with three key mechanisms. It isolates applications in containers that integrate in a controlled manner at the display and file system. It introduces ephemeral containers that are quickly instantiated for single application execution and then removed, to prevent any exploit that occurs from persisting and to protect user privacy. It introduces the virtual layered file system to make instantiating containers fast and space efficient, and to make managing many containers no more complex than having a single traditional desktop. We have implemented Apiary on Linux without any application or operating system kernel changes. Our results from running real applications, known exploits, and a 24-person user study show that Apiary has modest performance overhead, is effective in limiting the damage from real vulnerabilities to enable quick recovery, and is as easy to use as a traditional desktop while improving desktop computer security and privacy. (pdf) Source Prefix Filtering in ROFL Hang Zhao, Maritza Johnson, Chi-Kin Chau, Steven M. Bellovin 2009-07-26 Traditional firewalls have the ability to allow or block traffic based on source address as well as destination address and port number. Our original ROFL scheme implements firewalling by layering it on top of routing; however, the original proposal focused just on destination address and port number.  Doing route selection based in part on source addresses is a form of policy routing, which has started to receive increased amounts of attention.  In this paper, we extend the original ROFL (ROuting as the Firewall Layer) scheme by including source prefix constraints in route announcement.  We present algorithms for route propagation and packet forwarding, and demonstrate the correctness of these algorithms using rigorous proofs.  The new scheme not only accomplishes the complete set of filtering functionality provided by traditional firewalls, but also introduces a new direction for policy routing. (pdf) Semantic Ranking and Result Visualization for Life Sciences Publications Julia Stoyanovich, William Mee, Kenneth A. Ross 2009-06-23 An ever-increasing amount of data and semantic knowledge in the domain\r\nof life sciences is bringing about new data management challenges.  In\r\nthis paper we focus on adding the semantic dimension to literature\r\nsearch, a central task in scientific research. We focus our attention\r\non PubMed, the most significant bibliographic source in life sciences,\r\nand explore ways to use high-quality semantic annotations from the\r\nMeSH vocabulary to rank search results.  We start by developing\r\nseveral families of ranking functions that relate a search query to a\r\ndocument's annotations.  We then propose an efficient adaptive ranking\r\nmechanism for each of the families.  We also describe a\r\ntwo-dimensional Skyline-based visualization that can be used in\r\nconjunction with the ranking to further improve the user's interaction\r\nwith the system, and demonstrate how such Skylines can be computed\r\nadaptively and efficiently.  Finally, we present a user study that\r\ndemonstrates the effectiveness of our ranking. We use the full PubMed\r\ndataset and the complete MeSH ontology in our experimental evaluation. (pdf) A Software Checking Framework Using Distributed Model Checking and   Checkpoint/Resume of Virtualized PrOcess Domains Nageswar Keetha, Leon Wu, Gail Kaiser, Junfeng Yang 2009-06-18 Complexity and heterogeneity of the deployed software\r\napplications often result in a wide range of dynamic states\r\nat runtime. The corner cases of software failure during execution\r\noften slip through the traditional software checking.\r\nIf the software checking infrastructure supports the\r\ntransparent checkpoint and resume of the live application\r\nstates, the checking system can preserve and replay the live\r\nstates in which the software failures occur. We introduce\r\na novel software checking framework that enables application\r\nstates including program behaviors and execution\r\ncontexts to be cloned and resumed on a computing cloud.\r\nIt employs (1) EXPLODE’s model checking engine for a\r\nlightweight and general purpose software checking (2) ZAP\r\nsystem for faster, low overhead and transparent checkpoint\r\nand resume mechanism through virtualized PODs (PrOcess\r\nDomains), which is a collection of host-independent processes,\r\nand (3) scalable and distributed checking infrastructure\r\nbased on Distributed EXPLODE. Efficient and portable\r\ncheckpoint/resume and replay mechanism employed in this\r\nframework enables scalable software checking in order to\r\nimprove the reliability of software products. The evaluation\r\nwe conducted showed its feasibility, efficiency and applicability. (pdf) Serving Niche Video-on-Demand Content in a Managed P2P Environment Eli Brosh, Chitra Agastya, John Morales, Vishal Misra, Dan Rubenstein 2009-06-17 A limitation of existing P2P VoD services is their inability to support\r\nefficient streamed access to niche content that has relatively small demand. This limitation stems\r\nfrom the poor performance of P2P when the number of peers sharing the content is small. In this paper, we propose a new provider-managed P2P VoD framework for efficient delivery of niche content based on two principles: reserving small portions of peers' storage and upload resources, as well as using novel, weighed caching techniques. We demonstrate through analytical analysis, simulations, and experiments on planetlab that our architecture can provide high streaming quality for niche content. In particular, we show that our architecture increases the catalog size by up to $40\\%$ compared to standard P2P VoD systems, and that a weighted cache policy can reduce the startup delay for niche content by a factor of more than three. (pdf) Flexible Filters: Load Balancing through Backpressure for Stream Programs Rebecca Collins, Luca Carloni 2009-06-16 Stream processing is a promising paradigm for programming multi-core systems for high-performance embedded applications. We propose flexible filters as a technique that combines static mapping of the stream program tasks with dynamic load balancing of their execution. The goal is to improve the system-level processing throughput of the program when it is executed on a distributed-memory multi-core system as well as the local (core-level) memory utilization. Our technique is distributed and scalable because it is based on point-to-point handshake signals exchanged between neighboring cores. Load balancing with flexible filters can be applied to stream applications that present large dynamic variations in the computational load of their tasks and the dimension of the stream data tokens. In order to demonstrate the practicality of our technique, we present the performance improvements for the case study of a JPEG encoder running\r\non the IBM Cell multi-core processor. (pdf) Adaptive Anomaly Detection via Self-Calibration and Dynamic Updating Gabriela Cretu, Angelos Stavrou, Michael E. Locasto, Salvatore J. Stolfo 2009-06-11 The deployment and use of Anomaly Detection (AD) sensors often requires the intervention of a human expert to manually calibrate and optimize their performance. Depending on the site and the type of traffic it receives, the operators might have to provide recent and sanitized training data sets, the characteristics of expected traffic (i.e. outlier ratio), and exceptions or even expected future modifications of system’s behavior. In this paper, we study the potential performance issues that stem from fully automating the AD sensors’ day-to-day maintenance and calibration. Our goal is to remove the dependence on human operator using an unlabeled, and thus potentially dirty, sample of incoming traffic. To that end, we propose to enhance the training phase of AD sensors with a self-calibration phase, leading to the automatic determination of the optimal AD parameters. We show how this novel calibration phase can be employed in conjunction with previously proposed methods for training data sanitization resulting in a fully automated AD maintenance cycle. Our approach is completely agnostic to the underlying AD sensor algorithm. Furthermore, the self-calibration can be applied in an online fashion to ensure that the resulting AD models reflect changes in the system’s behavior which would otherwise render the sensor’s internal state inconsistent. We verify the validity of our approach through a series of experiments where we compare the manually obtained optimal parameters with the ones computed from the self-calibration phase. Modeling traffic from two different sources, the fully automated calibration shows a 7.08% reduction in detection\r\nrate and a 0.06% increase in false positives, in the worst case, when compared to the optimal selection of parameters. Finally, our adaptive models outperform the statically generated ones retaining the gains in performance from the sanitization process over time. (pdf) Masquerade Attack Detection Using a Search-Behavior Modeling Approach Malek Ben Salem, Salvatore J. Stolfo 2009-06-10 Masquerade attacks are unfortunately a familiar security problem that is a consequence of\r\nidentity theft. Detecting masqueraders is very hard. Prior work has focused on user command\r\nmodeling to identify abnormal behavior indicative of impersonation. This paper extends prior\r\nwork by presenting one-class Hellinger distance-based and one-class SVM modeling techniques\r\nthat use a set of novel features to reveal user intent. The speci\fc objective is to model user\r\nsearch pro\fles and detect deviations indicating a masquerade attack. We hypothesize that\r\neach individual user knows their own \fle system well enough to search in a limited, targeted\r\nand unique fashion in order to \fnd information germane to their current task. Masqueraders,\r\non the other hand, will likely not know the \fle system and layout of another user's desktop,\r\nand would likely search more extensively and broadly in a manner that is di\u000berent than the\r\nvictim user being impersonated. We extend prior research that uses UNIX command sequences\r\nissued by users as the audit source by relying upon an abstraction of commands. We devise\r\ntaxonomies of UNIX commands and Windows applications that are used to abstract sequences\r\nof user commands and actions. We also gathered our own normal and masquerader data sets\r\ncaptured in a Windows environment for evaluation. The datasets are publicly available for\r\nother researchers who wish to study masquerade attack rather than author identi\fcation as in\r\nmuch of the prior reported work. The experimental results show that modeling search behavior\r\nreliably detects all masqueraders with a very low false positive rate of 0.1%, far better than prior\r\npublished results. The limited set of features used for search behavior modeling also results in\r\nhuge performance gains over the same modeling techniques that use larger sets of features. (pdf) Self-monitoring Monitors Salvatore Stolfo, Isaac Greenbaum, Simha Sethumadhavan 2009-06-03 Many different monitoring systems have been created to identify system state conditions to detect or prevent a myriad of deliberate attacks, or arbitrary faults inherent in any complex system. Monitoring systems are also vulnerable to attack. A stealthy attacker can simply turn off or disable these monitoring systems without being detected; he would thus be able to perpetrate the very attacks that these systems were designed to stop. For example, many examples of virus attacks against antivirus scanners have appeared in the wild. In this paper, we present a novel technique to “monitor the monitors” in such a way that (a) unauthorized shutdowns of critical monitors are detected with high probability, (b) authorized shutdowns raise no alarm, and (c) the proper shutdown sequence for authorized shutdowns cannot be inferred from reading memory.  The techniques proposed to prevent unauthorized shut down (turning off) of monitoring systems was inspired by the duality of safety technology devised to prevent unauthorized discharge (turning on) of nuclear weapons. (pdf) Thwarting Attacks in Malcode-Bearing Documents by Altering Data Sector Values Wei-Jen Li, Salvatore J. Stolfo 2009-06-01 Embedding malcode within documents provides a convenient means of attacking systems. Such attacks can be very targeted and difficult to detect to stop due to the multitude of document-exchange vectors and\r\nthe vulnerabilities in modern document processing applications. Detecting malcode embedded in a document is difficult owing to the complexity of modern document formats that provide ample opportunity to embed code in a myriad of ways. We focus on Microsoft Word documents as malcode carriers as a case study in this paper. To detect stealthy embedded malcode in documents, we develop an arbitrary data transformation technique that changes the value of data segments in documents in such a way as to purposely damage any hidden malcode that may be embedded in those sections. Consequently, the embedded malcode will not only fail but also introduce a system exception that would be easily detected. The method is intended to be applied in a safe sandbox, the transformation is reversible after testing a document, and does not require any learning phase. The method depends upon knowledge of the structure of the document binary format to parse a document and identify the specific sectors to which the method can be safely applied for malcode detection. The method can be implemented in MS Word as a security feature to enhance the safety of Word documents. (pdf) weHelp: A Reference Architecture for Social Recommender Systems Swapneel Sheth, Nipun Arora, Christian Murphy, Gail Kaiser 2009-05-15 Recommender systems have become increasingly popular.\r\nMost of the research on recommender systems has focused on recommendation algorithms.\r\nThere has been relatively little research, however, in the area of generalized system architectures for recommendation systems.\r\nIn this paper, we introduce \\textit{weHelp}: a reference architecture for social recommender systems - systems where recommendations are derived automatically from the aggregate of logged activities conducted by the system's users.\r\nOur architecture is designed to be application and domain agnostic.\r\nWe feel that a good reference architecture will make designing a recommendation system easier; in particular, weHelp aims to provide a practical design template to help developers design their own well-modularized systems. (pdf) The Zodiac Policy Subsystem: a Policy-Based Management System for a High-Security MANET Yuu-Heng Cheng, Scott Alexander, Alex Poylisher, Mariana Raykova, Steven M. Bellovin 2009-05-07 Zodiac (Zero Outage Dynamic Intrinsically Assurable\r\nCommunities) is an implementation of a high-security\r\nMANET, resistant to multiple types of attacks, including\r\nByzantine faults. The Zodiac architecture poses a set of unique\r\nsystem security, performance, and usability requirements to\r\nits policy-based management system (PBMS). In this paper,\r\nwe identify theses requirements, and present the design and\r\nimplementation of the Zodiac Policy Subsystem (ZPS), which\r\nallows administrators to securely specify, distribute and evaluate\r\nnetwork control and system security policies to customize\r\nZodiac behaviors. ZPS uses Keynote language for specifying\r\nall authorization policies with simple extension to support\r\nobligation policies. (pdf) The Impact of TLS on SIP Server Performance Charles Shen, Erich Nahum, Henning Schulzrinne, Charles Wright 2009-05-05 This report studies the performance impact of using TLS as a transport protocol for SIP servers. We evaluate the cost of TLS\r\nexperimentally using a testbed with OpenSIPS, OpenSSL, and Linux running on an Intel-based server. We analyze TLS costs\r\nusing application, library, and kernel profiling, and use the profiles to illustrate when and how different costs are incurred, such\r\nas bulk data encryption, public key encryption, private key decryption, and MAC-based verification. We show that using TLS can reduce performance by up to a factor of 20 compared to the typical case of SIP over UDP. The primary factor in determining performance is whether and how TLS connection establishment is performed, due to the heavy\r\ncosts of RSA operations used for session negotiation. This depends both on how the SIP proxy is deployed (e.g., as an inbound or\r\noutbound proxy) and what TLS options are used (e.g., mutual authentication, session reuse). The cost of symmetric key operations\r\nsuch as AES or 3DES, in contrast, tends to be small.\r\nNetwork operators deploying SIP over TLS should attempt to maximize the persistence of secure connections, and will need\r\nto assess the server resources required. To aid them, we provide a measurement-driven cost model for use in provisioning SIP\r\nservers using TLS. Our cost model predicts performance within 15 percent on average. (pdf) COMPASS: A Community-driven Parallelization Advisor for Sequential Software Simha Sethumadhavan, Gail E. Kaiser 2009-04-22 The widespread adoption of multicores has renewed the\r\nemphasis on the use of parallelism to improve performance.\r\nThe present and growing diversity in hardware architectures\r\nand software environments, however, continues to\r\npose difficulties in the effective use of parallelism thus delaying\r\na quick and smooth transition to the concurrency era.\r\nIn this paper, we describe the research being conducted at\r\nColumbia University on a system called COMPASS that aims\r\nto simplify this transition by providing advice to programmers\r\nwhile they reengineer their code for parallelism. The\r\nadvice proffered to the programmer is based on the wisdom\r\ncollected from programmers who have already parallelized\r\nsome similar code. The utility of COMPASS rests, not only\r\non its ability to collect the wisdom unintrusively but also on\r\nits ability to automatically seek, find and synthesize this wisdom\r\ninto advice that is tailored to the task at hand, i.e., the\r\ncode the user is considering parallelizing and the environment\r\nin which the optimized program is planned to execute.\r\nCOMPASS provides a platform and an extensible framework\r\nfor sharing human expertise about code parallelization –\r\nwidely, and on diverse hardware and software. By leveraging\r\nthe “wisdom of crowds” model [26], which has been\r\nconjectured to scale exponentially and which has successfully\r\nworked for wikis, COMPASS aims to enable rapid propagation\r\nof knowledge about code parallelization in the context\r\nof the actual parallelization reengineering, and thus\r\ncontinue to extend the benefits of Moore’s law scaling to\r\nscience and society. (pdf) Have I Met You Before? Using Cross-Media Relations to Reduce SPIT Kumiko Ono, Henning Schulzrinne 2009-04-14 Most legitimate calls are from persons or organizations with\r\nstrong social ties such as friends. Some legitimate calls, however, are from those with weak social ties such as a restaurant the callee booked a table on-line. Since a callee's contact list usually contains only the addresses of persons or organizations with strong social ties, filtering out unsolicited calls using the contact list is prone to false positives. To reduce these false positives, we first analyzed call logs and identified that legitimate calls are initiated from persons or organizations with weak social ties through transactions over\r\nthe web or email exchanges. This paper proposes two approaches to label incoming calls by using cross-media relations to previous contact mechanisms which initiate the calls. One approach is that potential callers offer the callee their contact addresses which might be used in future correspondence. Another is that a callee provides potential callers with weakly-secret information that the callers should use in\r\nfuture correspondence in order to identify them as someone the callee has contacted before through other means. Depending on previous contact mechanisms, the callers use either customized contact addresses or message identifiers. The latter approach enables a callee to label incoming calls even without caller identifiers. Reducing false positives during filtering using our proposed approaches will contribute\r\nto the reduction in SPIT (SPam over Internet Telephony). (pdf) F3ildCrypt: End-to-End Protection of Sensitive Information in Web   Services Matthew Burnside, Angelos D. Keromytis 2009-03-30 The frequency and severity of recent intrusions involving data theft\r\nand leakages has shown that online users' trust, voluntary or not, in\r\nthe ability of third parties to protect their sensitive data is often\r\nunfounded.  Data may be exposed anywhere along a corporation's web\r\npipeline, from the outward-facing web servers to the back-end\r\ndatabases.  Additionally, in service-oriented architectures (SOAs),\r\ndata may also be exposed as they transit between SOAs.  For example,\r\ncredit card numbers may be leaked during transmission to or handling\r\nby transaction-clearing intermediaries.\r\n\r\nWe present F3ildCrypt, a system that provides end-to-end protection of\r\ndata across a web pipeline and between SOAs. Sensitive data are\r\nprotected from their origin (the user's browser) to their legitimate\r\nfinal destination.  To that end, F3ildCrypt exploits browser scripting\r\nto enable application- and merchant-aware handling of sensitive data.\r\nSuch techniques have traditionally been considered a security risk; to\r\nour knowledge, this is one of the first uses of web scripting that\r\nenhances overall security.  F3ildCrypt uses proxy re-encryption\r\nto re-target messages as they enter and cross SOA boundaries, and uses\r\nXACML, the XML-based access control language, to define protection\r\npolicies.  Our approach scales well in the number of public key\r\noperations required for web clients and does not reveal proprietary\r\ndetails of the logical enterprise network (because of the application\r\nof proxy re-encryption).  We evaluate F3ildCrypt and show an\r\nadditional cost of 40 to 150 ms when making sensitive transactions\r\nfrom the web browser, and a processing rate of 100 to 140 XML\r\nfields/second on the server.  We believe such costs to be a reasonable\r\ntradeoff for increased sensitive-data confidentiality. (pdf) Baiting Inside Attackers using Decoy Documents Brian M. Bowen, Shlomo Hershkop, Angelos D. Keromytis, Salvatore J. Stolfo 2009-03-30 The insider threat remains one of the most vexing problems in computer security. A number of approaches have been proposed to detect nefarious insider actions including user modeling and profiling techniques, policy and access enforcement techniques, and misuse detection. In this work we propose trap-based defense mechanisms for the case where insiders attempt to exfiltrate and use sensitive information. Our goal is to confuse and confound the attacker requiring far more effort to identify real information from bogus information and to provide a means of detecting when an inside attacker attempts to exploit sensitive information. ``Decoy Documents\" are automatically generated and stored on a file system with the aim of enticing a malicious insider to open and review the contents of the documents. The decoy documents contain several different types of bogus credentials that when used, trigger an alert.  We also embed ``stealthy beacons\" inside the documents that cause a signal to be emitted to a server indicating when and where the particular decoy was opened. We evaluate decoy documents on honeypots penetrated by attackers demonstrating the feasibility of the method. (pdf) Metamorphic Runtime Checking of Non-Testable Programs Christian Murphy, Gail Kaiser 2009-03-16 Challenges arise in assuring the quality of applications that do not  have test oracles, i.e., for which it is difficult or impossible to know that the correct output should be for arbitrary input. Recently, metamorphic testing has been shown to be a simple yet effective technique in addressing the quality assurance of these so-called \"non-testable programs\". In metamorphic testing, existing test case input is modified to produce new test cases in such a manner that, when given the new input, the function should produce an output that can easily be computed based on the original output. That is, if input x produces output f(x), then we create input x' such that we can predict f(x') based on f(x); if the application does not produce the expected output, then a defect must exist, and either f(x) or f(x') (or both) is wrong.\r\n\r\nPreviously we have presented an approach called \"Automated Metamorphic System Testing\", in which metamorphic testing is conducted automatically as the program executes. In the approach, metamorphic properties of the entire application are specified, and then checked after execution is complete. Here, we improve upon that work by presenting a technique in which the metamorphic properties of individual functions are used, allowing for the specification of more complex properties and enabling finer-grained runtime checking. Our goal is to demonstrate that such an approach will be more effective than one based on specifying metamorphic properties at the system level, and is also feasible for use in the deployment environment.\r\n\r\nThis technique, called Metamorphic Runtime Checking, is a system testing approach in which the metamorphic properties of individual functions are automatically checked during the program's execution. The tester is able to easily specify the functions' properties so that metamorphic testing can be conducted in a running application, allowing the tests to execute using real input data and in the context of real system states, without affecting those states. We also describe an implementation framework called Columbus, and present the results of empirical studies that demonstrate that checking the metamorphic properties of individual functions increases the effectiveness of the approach in detecting defects, with minimal performance impact. (pdf) An Anonymous Credit Card System Elli Androulaki, Steven Bellovin 2009-02-27 Credit cards have many important bene\fts; however, these same benefits often carry with them many privacy concerns. In particular, the need for users to be able to monitor their own transactions, as well as bank's need to justify its payment requests from cardholders, entitle the latter to maintain a detailed log of all transactions its credit card customers were involved in. A bank can thus build a profile of each cardholder even without the latter's consent. In this technical report, we present a practical and accountable anonymous credit system based on ecash , with a privacy preserving mechanism for error correction and expense-reporting. (pdf) Turn-Taking and Affirmative Cue Words in Task-Oriented Dialogue Agustin Gravano 2009-02-23 As interactive voice response systems spread at a rapid pace, providing\r\nan increasingly more complex functionality, it is becoming clear that\r\nthe challenges of such systems are not solely associated to their\r\nsynthesis and recognition capabilities. Rather, issues such as the\r\ncoordination of turn exchanges between system and user, or the correct\r\ngeneration and understanding of words that may convey multiple meanings,\r\nappear to play an important role in system usability. This thesis\r\nexplores those two issues in the Columbia Games Corpus, a collection\r\nof spontaneous task-oriented dialogues in Standard American English.\r\n\r\nWe provide evidence of the existence of seven turn-yielding cues --\r\nprosodic, acoustic and syntactic events strongly associated with\r\nconversational turn endings -- and show that the likelihood of a\r\nturn-taking attempt from the interlocutor increases linearly with the\r\nnumber of cues conjointly displayed by the speaker. We present similar\r\nresults related to six backchannel-inviting cues -- events that invite\r\nthe interlocutor to produce a short utterance conveying continued\r\nattention.\r\n\r\nAdditionally, we describe a series of studies of affirmative cue words\r\n-- a family of cue words such as 'okay' or 'alright' that speakers use\r\nfrequently in conversation for several purposes: for acknowledging what\r\nthe interlocutor has said, or for cueing the start of a new topic,\r\namong others. We find differences in the acoustic/prosodic realization\r\nof such functions, but observe that contextual information figures\r\nprominently in human disambiguation of these words. We also conduct\r\nmachine learning experiments to explore the automatic classification of\r\naffirmative cue words. Finally, we examine a novel measure of speaker\r\nentrainment related to the usage of these words, showing its association\r\nwith task success and dialogue coordination. (pdf) Automatic System Testing of Programs without Test Oracles Christian Murphy, Kuang Shen, Gail Kaiser 2009-01-30 Metamorphic testing has been shown to be a simple yet effective technique in addressing the quality assurance of applications that do not have test oracles, i.e., for which it is difficult or impossible to know what the correct output should be for arbitrary input. In metamorphic testing, existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can be easily be computed based on the original output. That is, if input x produces output f (x), then we create input x' such that we can predict f (x') based on f(x); if the application does not produce the expected output, then a defect must exist, and either f (x) or f (x') (or both) is wrong. \r\n\r\nIn practice, however, metamorphic testing can be a manually intensive technique for all but the simplest cases. The transformation of input data can be laborious for large data sets, or practically impossible for input that is not in human-readable format. Similarly, comparing the outputs can be error-prone for large result sets, especially when slight variations in the results are not actually indicative of errors (i.e., are false positives), for instance when there is non-determinism in the application and multiple outputs can be considered correct.\r\n\r\nIn this paper, we present an approach called Automated Metamorphic System Testing. This involves the automation of metamorphic testing at the system level by checking that the metamorphic properties of the entire application hold after its execution. The tester is able to easily set up and conduct metamorphic tests with little manual intervention, and testing can continue in the field with minimal impact\r\non the user. Additionally, we present an approach called Heuristic Metamorphic Testing which seeks to reduce false positives and address some cases of non-determinism. We also describe an implementation framework called Amsterdam, and present the results of empirical studies in which we demonstrate the effectiveness of the technique on real-world programs without test oracles. (pdf) Example application under PRET environment -- Programming a MultiMediaCard Devesh Dedhia 2009-01-22 PRET philosophy proposes the temporal\r\ncharacteristics to be made predictable. However for\r\nvarious applications the PRET processor will have to\r\ninteract with a non predictable environment. In this\r\npaper an example of one such environment, an\r\nMultiMediaCard (MMC) is considered. This paper\r\nillustrates a method to make the response of the\r\nMMC predictable. (pdf) Improving the Quality of Computational Science Software by Using Metamorphic Relations to Test Machine Learning Applications Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen Xu, T.Y. Chen 2009-01-19 Many applications in the field of scientific computing - such as computational biology, computational linguistics, and others - depend on Machine Learning algorithms to provide important core functionality to support solutions in the particular problem domains. However, it is difficult to test such applications because often there is no \"test oracle\" to indicate what the correct output should be for arbitrary input. To help address the quality of scientific computing software, in this paper we present a technique for testing the implementations of machine learning classification algorithms on which such scientific computing software depends. Our technique is based on an approach called\r\n\"metamorphic testing\", which has been shown to be effective in such cases. In addition to presenting our technique, we describe a case study we performed on a real-world machine learning application framework, and discuss how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also discuss how our findings can be of use to other areas of computational science and engineering. (pdf) Multi-perspective Evaluation of Self-Healing Systems Using Simple Probabilistic Models Rean Griffith, Gail Kaiser, Javier Alonso Lopez 2009-01-19 Quantifying the efficacy of self-healing systems is a challenging\r\nbut important task, which has implications for increasing\r\ndesigner, operator and end-user confidence in\r\nthese systems. During design system architects benefit from\r\ntools and techniques that enhance their understanding of\r\nthe system, allowing them to reason about the tradeoffs\r\nof proposed or existing self-healing mechanisms and the\r\noverall effectiveness of the system as a result of different\r\nmechanism-compositions. At deployment time, system integrators\r\nand operators need to understand how the selfhealing\r\nmechanisms work and how their operation impacts\r\nthe system’s reliability, availability and serviceability (RAS)\r\nin order to cope with any limitations of these mechanisms\r\nwhen the system is placed into production.\r\n\r\nIn this paper we construct an evaluation framework for selfhealing\r\nsystems around simple, yet powerful, probabilistic\r\nmodels that capture the behavior of the system’s selfhealing\r\nmechanisms from multiple perspectives (designer,\r\noperator, and end-user). We combine these analytical models\r\nwith runtime fault-injection to study the operation of\r\nVM-Rejuv – a virtual machine based rejuvenation scheme\r\nfor web-application servers. We use the results from the\r\nfault-injection experiments and model-analysis to reason\r\nabout the efficacy of VM-Rejuv, its limitations and strategies\r\nfor managing/mitigating these limitations in systemdeployments.\r\nWhereas we use VM-Rejuv as the subject of\r\nour evaluation in this paper, our main contribution is a\r\npractical evaluation approach that can be generalized to\r\nother self-healing systems. (pdf) A Case Study in Distributed Deployment of Embedded Software for Camera NetworksA Case Study in Distributed Deployment of Embedded Software for Camera Networks Francesco Leonardi, Alessandro Pinto, Luca P. Carloni 2009-01-15 We present an embedded software application for the real-time estimation of building occupancy using a network of video cameras. We analyze a series of alternative decompositions of the main application tasks and profile each of them by running the corresponding embedded software on three different processors. Based on the profiling measures, we build various alternative embedded platforms by combining different embedded processors, memory modules and network interfaces.\r\nIn particular, we consider the choice of two possible network technologies: ARCnet and Ethernet. After deriving an analytical model of the network costs, we use it to complete an exploration of the design space as we scale the number of video cameras in an hypothetical building. We compare our results with those obtained for two real buildings of different characteristics. We conclude discussing the results of our case study in the broader context of other camera-network applications. (pdf) Improving Virtual Appliance Management through Virtual Layered File Systems Shaya Potter, Jason Nieh 2009-01-15 Managing many computers is difficult. Recent virtualization trends exacerbate this problem by making it easy to create and deploy multiple virtual appliances per physical machine, each of which can be configured with different applications and utilities. This results in a huge scaling problem for large organizations as management overhead grows linearly with the number of appliances.\r\n\r\nTo address this problem, we present Strata, a system that introduces the Virtual Layered File System (VLFS) and integrates it with virtual appliances to simplify system management. Unlike a traditional file system, which is a monolithic entity, a VLFS is a collection of individual software layers composed together to provide the traditional file system view. Individual layers are maintained in a central repository and shared across all VLFSs that use them. Layer changes and upgrades only need to be done once in the repository and are then automatically propagated to all VLFSs, resulting in management overhead independent of the number of virtual appliances. We have implemented a Strata Linux prototype without any application or operating system kernel changes. Using this prototype, we demonstrate how Strata enables fast system provisioning, simplifies system maintenanc and upgrades, speeds system recovery from security exploits, and incurs only modest performance overhead. (pdf) Retrocomputing on an FPGA: Reconstructing an 80's-Era Home Computer with Programmable Logic Stephen A. Edwards 2009-01-12 The author reconstructs a computer of his childhood, an Apple II+. (pdf) (ps) A MPEG Decoder in SHIM Keerti Joshi, Delvin Kellebrew 2008-12-23 The emergence of world-wide standards for video compression has created a demand for design tools and simulation resources to support algorithm research and new product development. Because of the need for subjective study in the design of video compression algorithms it is essential that flexible yet computationally efficient tools be developed.\r\n\r\nFor this project, we plan to implement a MPEG standard using the SHIM programming language. The SHIM is a software/hardware integration language whose aim is to provide communication between hardware and software while providing deterministic concurrency.\r\n\r\nThe focus of this project will be to emphasize the efficiency of the SHIM language in embedded applications as compared to other existing implementations. (pdf) Using Metamorphic Testing at Runtime to Detect Defects in Applications without Test Oracles Christian Murphy 2008-12-22 First, we will present an approach called Automated Metamorphic System Testing. This will involve automating system-level metamorphic testing by treating the application as a black box and checking that the metamorphic properties of the entire application hold after execution. This will allow for metamorphic testing to be conducted in the production environment without affecting the user, and will not require the tester to have access to the source code. The tests do not require an oracle upon their creation; rather, the metamorphic properties act as built-in test oracles. We will also introduce an implementation framework called Amsterdam.\r\n\r\nSecond, we will present a new type of testing called Metamorphic Runtime Checking. This involves the execution of metamorphic tests from within the application, i.e., the application launches its own tests, within its current context. The tests execute within the application’s current state, and in particular check a function’s metamorphic properties. We will also present a system called Columbus that supports the execution of the Metamorphic Runtime Checking from within the context of the running application. Like Amsterdam, it will conduct the tests with acceptable performance overhead, and will ensure that the execution of the tests does not affect the state of the original application process from the users’ perspective; however, the  implementation of Columbus will be more challenging in that it will require more sophisticated mechanisms for conducting the tests without pre-empting the rest of the application, and for comparing the results\r\nwhich may conceivably be in different processes or environments.\r\n\r\nThird, we will describe a set of metamorphic testing guidelines that can be followed to assist in the formulation and specification of metamorphic properties that can be used with the above approaches. These will categorize the different types of properties exhibited by many applications in the domain of machine learning and data mining in particular (as a result of the types of applications we will investigate), but we will demonstrate that they are also generalizable to other domains as well. This set of guidelines will also correlate to the different types of defects that we expect the approaches will be able to find. (pdf) Static Deadlock Detection in SHIM with an Automata Type Checking System Dave Aaron Smith, Nalini Vasudevan, Stephen Edwards 2008-12-21 With the advent of multicores, concurrent programming languages are become more prevelant. Data Races and Deadlocks are two major problems with concurrent programs. SHIM is a concurrent programming language that guarantees absence of data races through its semantics. However, a program written in SHIM can deadlock if not carefully written. \r\n\r\n In this paper, we present a divide-and-merge technique to statically detect deadlocks in SHIM. SHIM is asynchronous, but we can greatly reduce its state space without loosing precision because of its semantics. (pdf) (ps) SHIM Optimization: Elimination Of Unstructured Loops Ravindra Babu Ganapathi, Stephen A. Edwards 2008-12-21 The SHIM compiler for the IBM CELL processor generates distinct code for the two processing units, PPE (Power Processor Element) and SPE (Synergistic Processor Elements). The SPE is specialized to give high throughput with computation intensive application operating on dense data. We propose mechanism to tune the code generated by the SHIM compiler to enable optimizing compilers to generate structured code.\r\n\r\nAlthough, the discussion here is related to optimizing SHIM IR (Intermediate Representation) code, the techniques discussed here can be incorporated into compilers to convert unstructured loops consisting of goto statements to structured loops such as while and do-while statements to ease back end compiler optimizations.\r\n\r\nOur research based SHIM compiler takes the code written in SHIM language and performs various static analysis and finally transforms it into C code. This generated code is compiled to binary using standard compilers available for IBM cell processor such as GCC and IBM XL compiler. (pdf) (ps) uClinux on the Altera DE2 David Lariviere, Stephen A. Edwards 2008-12-21 This technical report provides an introduction on how to compile and run uClinux and third-party programs to be run on a Nios II CPU core instantiated within the FPGA on the Altera DE2. It is based on experiences working with the OS and development board while teaching the Embedded Systems course during the springs of 2007 and 2008. (pdf) Memory Issues in PRET Machines Nishant R. Shah 2008-12-21 In a processor design the premier issues with\r\nmemory are (1) main memory allocation and (2)\r\ninterprocess communication. These two mainly\r\naffect the performance of the memory system. The\r\ngoal of this paper is to formulate a deterministic\r\nmodel for memory systems of PRET, taking into\r\naccount all the intertwined parallelism of modern\r\nmemory chips.\r\n\r\nStudying existing memory models is necessary\r\nto understand the implications of these factors to\r\nrealize a perfectly time predictable memory\r\nsystem. (pdf) Analysis of Clocks in X10 Programs (Extended) Nalini Vasudevan, Olivier Tardieu, Julian Dolby, Stephen A. Edwards 2008-12-19 Clocks are a mechanism for providing synchronization barriers in\r\nconcurrent programming languages. They are usually implemented using\r\nprimitive communication mechanisms and thus spare the programmer from\r\nreasoning about low-level implementation details such as remote\r\nprocedure calls and error conditions.\r\n\r\nClocks provide flexibility, but programs often use them in specific\r\nways that do not require their full implementation. In this paper, we\r\ndescribe a tool that mitigates the overhead of general-purpose clocks\r\nby statically analyzing how programs use them and choosing optimized\r\nimplementations when available.\r\n\r\nWe tackle the clock implementation in the standard library of the X10\r\nprogramming language---a parallel, distributed object-oriented\r\nlanguage. We report our findings for a small set of analyses and\r\nbenchmarks. Our tool only adds a few seconds to analysis time, making\r\nit practical to use as part of a compilation chain. (pdf) Classifying High-Dimensional Text and Web Data using Very Short Patterns Hassan Malik, John Kender 2008-12-17 In this paper, we propose the \"Democratic Classifier\", a simple, democracy-inspired pattern-based classification algorithm that uses very short patterns for classification, and does not rely on the minimum support threshold. Borrowing ideas from democracy, our training phase allows each training instance to vote for an equal number of candidate size-2 patterns. Similar to the usual democratic election process, where voters select candidates by considering their qualifications, prior contributions at the constituency and territory levels, as well as their own perception about candidates, the training instances select patterns by effectively balancing between local, class, and global significance of patterns. In addition, we respect \"each voter's opinion\" by simultaneously adding shared patterns to all applicable classes, and then apply a novel power law based weighing scheme, instead of making binary decisions on these patterns.\r\n\r\nResults of experiments performed on 121 common text and web datasets show that our algorithm almost always outperforms state of the art classification algorithms, without requiring any dataset-specific parameter tuning. On 100 real-life, noisy, web datasets, the average absolute classification accuracy improvement was as great as 10% over SVM, Harmony, C4.5 and KNN. Also, our algorithm ran about 3.5 times faster than the fastest existing pattern-based classification algorithm. (pdf) Distributed eXplode: A High-Performance Model Checking Engine to Scale Up State-Space Coverage Nageswar Keetha, Leon Wu, Gail Kaiser, Junfeng Yang 2008-12-10 Model checking the state space (all possible behaviors) of software systems is a promising technique for verification and validation. Bugs such as security vulnerabilities, file storage issues, deadlocks and data races can occur anywhere in the state space and are often triggered by corner cases; therefore, it becomes important to explore and model check all runtime choices. However, large and complex software systems generate huge numbers of behaviors leading to ‘state explosion’. eXplode is a lightweight, deterministic and depth-bound model checker that explores all dynamic choices at runtime. Given an application-specific test-harness, eXplode performs state search in a serialized fashion - which limits its scalability and performance. This paper proposes a distributed eXplode engine that uses multiple host machines concurrently in order to achieve more state space coverage in less time, and is very helpful to scale up the software verification and validation effort. Test results show that Distributed eXplode runs several times faster and covers more state space than the standalone eXplode. (pdf) Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum Fumihito Yasuma, Tomoo Mitsunaga, Daisuke Iso, Shree K. Nayar 2008-11-24 We propose the concept of a generalized assorted pixel (GAP) camera, which enables the user to\r\ncapture a single image of a scene and, after the fact, control the trade-off between spatial resolution,\r\ndynamic range and spectral detail. The GAP camera uses a complex array (or mosaic) of color filters.\r\nA major problem with using such an array is that the captured image is severely under-sampled for at\r\nleast some of the filter types. This leads to reconstructed images with strong aliasing. We make three\r\ncontributions in this paper: (a) We present a comprehensive optimization method to arrive at the spatial\r\nand spectral layout of the color filter array of a GAP camera. (b) We develop a novel anti-aliasing\r\nalgorithm for reconstructing the under-sampled channels of the image with minimal aliasing. (c) We\r\ndemonstrate how the user can capture a single image and then control the trade-off of spatial resolution\r\nto generate a variety of images, including monochrome, high dynamic range (HDR) monochrome, RGB,\r\nHDR RGB, and multispectral images. Finally, the performance of our GAP camera has been verified using extensive simulations that use multispectral images of real world scenes. A large database of these multispectral images is being made publicly available for use by the research community. (pdf) Measurements of Multicast Service Discovery in a Campus Wireless Network Se Gi Hong, Suman Srinivasan, Henning Schulzrinne 2008-11-14 Applications utilizing multicast service discovery protocols, such as iTunes, have become increasingly popular. However, multicast service discovery protocols are considered to generate network traffic overhead, especially in a wireless network. Therefore, it becomes important to evaluate the traffic and overhead caused by multicast service discovery packets in real-world networks. We measure and analyze the traffic of one of the mostly deployed multicast service discovery protocols, multicast DNS (mDNS) service discovery, in a campus wireless network that forms a single multicast domain of large users. We also analyze different service discovery models in terms of packet overhead and service discovery delay under different network sizes and churn rates. Our measurement shows that mDNS traffic consumes about 13 percent of the total bandwidth. (pdf) Improving the Dependability of Machine Learning Applications Christian Murphy, Gail Kaiser 2008-10-10 As machine learning (ML) applications become prevalent in various aspects of everyday life, their dependability takes on\r\nincreasing importance. It is challenging to test such applications, however, because they are intended to learn properties of data sets\r\nwhere the correct answers are not already known. Our work is not concerned with testing how well an ML algorithm learns, but rather\r\nseeks to ensure that an application using the algorithm implements the specification correctly and fulfills the users' expectations. These\r\nare critical to ensuring the application's dependability. This paper presents three approaches to testing these types of applications.\r\nIn the first, we create a set of limited test cases for which it is, in fact, possible to predict what the correct output should be. In\r\nthe second approach, we use random testing to generate large data sets according to parameterization based on the application’s\r\nequivalence classes. Our third approach is based on metamorphic testing, in which properties of the application are exploited to define\r\ntransformation functions on the input, such that the new output can easily be predicted based on the original output. Here we discuss\r\nthese approaches, and our findings from testing the dependability of three real-world ML applications. (pdf) Opportunistic Use of Client Repeaters to Improve Performance of WLANs Victor Bahl, Ranveer Chandra, Patrick Pak-Ching Lee, Vishal Misra, Jitendra Padhye, Dan Rubenstein 2008-10-09 Currently deployed IEEE 802.11WLANs (Wi-Fi networks) share access point (AP) bandwidth on a per-packet basis. However, the various stations communicating with the AP often have different signal qualities, resulting in different transmission rates. This induces a phenomenon known as the rate anomaly problem, in which stations with lower signal\r\nquality transmit at lower rates and consume a significant majority of airtime, thereby dramatically reducing the throughput of stations transmitting at high rates. We propose a practical, deployable system, called SoftRepeater, in which stations cooperatively address the rate\r\nanomaly problem. Specifically, higher-rate Wi-Fi stations opportunistically transformthemselves into repeaters for stations\r\nwith low data-rates when transmitting to/from the AP. The key challenge is to determine when it is beneficial to enable the repeater functionality. In this paper, we propose an initiation protocol that ensures that repeater functionality is enabled only when appropriate. Also, our system can run directly on top of today’s 802.11 infrastructure networks. We also describe a novel, zero-overhead network coding scheme that further alleviates undesirable symptoms of the rate anomaly problem. We evaluate our system using simulation and testbed implementation, and find that SoftRepeater can improve cumulative throughput by up to 200%. (pdf) The 7U Evaluation Method: Evaluating Software Systems via Runtime Fault-Injection and Reliability, Availability and Serviceability (RAS) Metrics and Models Rean Griffith 2008-10-06 Renewed interest in developing computing systems that meet additional non-functional requirements such as reliability, high availability and ease-of-management/self-management (serviceability) has fueled research into developing systems that exhibit enhanced reliability,\r\navailability and serviceability (RAS) capabilities. This research focus on enhancing the RAS capabilities of computing systems impacts not only the legacy/existing systems we have today, but also has implications for the design and development of next generation (self-\r\nmanaging/self-*) systems, which are expected to meet these non-functional requirements with minimal human intervention.\r\n\r\nTo reason about the RAS capabilities of the systems of today or the self-* systems of tomorrow, there are three evaluation-related challenges to address. First, developing (or identifying) practical fault-injection tools that can be used to study the failure behavior of\r\ncomputing systems and exercise any (remediation) mechanisms the system has available for mitigating or resolving problems. Second, identifying techniques that can be used to quantify RAS deficiencies in computing systems and reason about the eﬃcacy of individual or combined RAS-enhancing mechanisms (at design-time or after system deployment).\r\nThird, developing an evaluation methodology that can be used to objectively compare systems based on the (expected or actual) beneﬁts of RAS-enhancing mechanisms.\r\n\r\nThis thesis addresses these three challenges by introducing the 7U Evaluation Methodology, a complementary approach to traditional performance-centric evaluations that identifies criteria for comparing and analyzing existing (or yet-to-be-added) RAS-enhancing mechanisms,\r\nis able to evaluate and reason about combinations of mechanisms, exposes under-performing mechanisms and highlights the lack of mechanisms in a rigorous, objective and quantitative\r\nmanner.\r\n\r\nThe development of the 7U Evaluation Methodology is based on the following three hypotheses. First, that runtime adaptation provides a platform for implementing eﬃcient and ﬂexible fault-injection tools capable of in-situ and in-vivo interactions with computing systems. Second, that mathematical models such as Markov chains, Markov reward networks and Control theory models can successfully be used to create simple, reusable templates for describing speciﬁc failure scenarios and scoring the system’s responses, i.e., studying the failure-behavior of systems, and the various facets of its remediation mechanisms and\r\ntheir impact on system operation. Third, that combining practical fault-injection tools with mathematical modeling techniques based on Markov Chains, Markov Reward Networks and Control Theory can be used to develop a benchmarking methodology for evaluating and comparing the reliability, availability and serviceability (RAS) characteristics of computing systems.\r\n\r\nThis thesis demonstrates how the 7U Evaluation Method can be used to evaluate the RAS capabilities of real-world computing systems and in so doing makes three contributions. First, a suite of runtime fault-injection tools (Kheiron tools) able to work in a variety\r\nof execution environments is developed. Second, analytical tools that can be used to construct mathematical models (RAS models) to evaluate and quantify RAS capabilities using appropriate metrics are discussed. Finally, the results and insights gained from conducting fault-injection experiments on real-world systems and modeling the system\r\nresponses (or lack thereof) using RAS models are presented. In conducting 7U Evaluations of real-world systems, this thesis highlights the similarities and differences between traditional performance-oriented evaluations and RAS-oriented evaluations and outlines a general\r\nframework for conducting RAS evaluations. (pdf) Quality Assurance of Software Applications using the In Vivo Testing Approach Christian Murphy, Gail Kaiser, Ian Vo, Matt Chu 2008-10-02 Software products released into the field typically have\r\nsome number of residual defects that either were not detected\r\nor could not have been detected during testing. This\r\nmay be the result of flaws in the test cases themselves, incorrect\r\nassumptions made during the creation of test cases,\r\nor the infeasibility of testing the sheer number of possible\r\nconfigurations for a complex system; these defects may also\r\nbe due to application states that were not considered during\r\nlab testing, or corrupted states that could arise due to\r\na security violation. One approach to this problem is to\r\ncontinue to test these applications even after deployment,\r\nin hopes of finding any remaining flaws. In this paper, we\r\npresent a testing methodology we call in vivo testing, in\r\nwhich tests are continuously executed in the deployment\r\nenvironment. We also describe a type of test we call in\r\nvivo tests that are specifically designed for use with such\r\nan approach: these tests execute within the current state of\r\nthe program (rather than by creating a clean slate) without\r\naffecting or altering that state from the perspective of the\r\nend-user. We discuss the approach and the prototype testing\r\nframework for Java applications called Invite. We also\r\nprovide the results of case studies that demonstrate Invite’s\r\neffectiveness and efficiency. (pdf) Using JML Runtime Assertion Checking to Automate Metamorphic Testing in Applications without Test Oracles Christian Murphy, Kuang Shen, Gail Kaiser 2008-10-02 It is challenging to test applications and functions for which the correct output for arbitrary input cannot be known in advance, e.g. some computational science or machine learning applications. In the absence of a test oracle, one approach to testing these applications is to use metamorphic testing: existing test case input is modified to produce new test cases in such a manner that, when given the new input, the application should produce an output that can be easily be computed based on the original output. That is, if input x produces output f(x), then we create input x' such that we can predict f(x') based on f(x); if the application or function does not produce the expected output, then a defect must exist, and either f(x) or f(x') (or both) is wrong. By using metamorphic testing, we are able to provide built-in \"pseudo-oracles\" for these so-called \"nontestable programs\" that have no test oracles. In this paper, we describe an approach in which a function's\r\nmetamorphic properties are specified using an extension to the Java Modeling Language (JML), a behavioral interface specification language that is used to support the \"design by contract\" paradigm in Java applications. Our implementation, called Corduroy, pre-processes these specifications and generates test code that can be executed using JML runtime assertion checking, for ensuring that the specifications hold during program execution. In addition to presenting our approach and implementation, we also describe our findings from case studies in which we apply our technique to applications without test oracles. (pdf) VoIP-based Air Traffic Controller Training Supreeth Subramanya, Xiaotao Wu, Henning Schulzrinne 2008-09-26 Extending VoIP beyond the Internet telephony, we propose a case study of applying the technology outside of its intended domain, to solve a real-world problem.  This work is an attempt to understand an analog hardwired communication system of the U.S. Federal Aviation Administration (FAA), and effectively translate it into a generic, standards-based VoIP system that runs on their existing data network. We develop insights into the air traffic training and weigh on the design choices for building a soft real-time data communication system. We also share our real-world deployment and maintenance experiences, as the FAA Academy has been successfully using this VoIP system in five training rooms since 2006 to train the future air traffic controllers of the U.S. and the world. (pdf) A Better Approach than Carrier-Grade-NAT Olaf Maennel, Randy Bush, Luca Cittadini, Steven M. Bellovin 2008-09-24 We are facing the exhaustion of newly assignable IPv4 addresses. Unfortunately, IPv6 is not yet deployed widely enough to fully replace IPv4, and it is unrealistic to expect that this is going to change before we run out of IPv4 addresses.  Letting hosts seamlessly communicate in an IPv4-world without assigning a unique globally routable IPv4 address to each of them is a challenging problem, for which many solutions have been proposed.  Some prominent ones target towards carrier-grade-NATs (CGN), which we feel is a bad idea.  Instead, we propose using specialized NATs at the edge that treat some of the port number bits as part of the address. (pdf) Spectrogram: A Mixture-of-Markov-Chains Model for Anomaly Detection in Web Traffic Yingbo Song, Angelos D. Keromytis, Salvatore J. Stolfo 2008-09-15 We present Spectrogram, a mixture of Markov-chains sensor for anomaly detection (AD) against web-layer (port 80) code-injection attacks such as PHP file inclusion, SQL-injection, cross-site-scripting, as well as memory layer buffer overflows. Port 80 is the gateway to many application level services and a large array of attacks are channeled through this vector, servers cannot easily firewall this port. Signature-based sensors are effective in filtering known exploits but can’t detect 0-day vulnerabilities or deal with polymorphism and statistical AD approaches have mostly been limited to network layer, protocol-agnostic modeling, weakening their effectiveness. N -gram based modeling approaches have recently demonstrated success but the ill-posed nature of modeling large grams have thus far prevented exploration of higher order statistical models. In this paper, we provide a solution to this problem based on a factorization into Markov-chains and aim to model higher order structure as well as content for web requests. Spectrogram is implemented in a protocol-aware, passive, network-situated, but CGI-layered, AD architecture and we show in our evaluation that this model demonstrates significant detection results on an array of real world web-layer attacks, achieving at least 97% detection rates on all but one dataset and comparing favorably against other AD sensors. (pdf) Retina: Helping Students and Instructors Based on Observed Programming Activities Christian Murphy, Gail Kaiser, Kristin Loveland, Sahar Hasan 2008-08-28 It is difficult for instructors of CS1 and CS2 courses to get accurate answers to such critical questions as \"how long are students spending on programming assignments?\", or \"what sorts of errors are they making?\" At the same time, students often have no idea of where they stand with respect to the rest of the class in terms of time spent on an assignment or the number or types of errors that they encounter. In this paper, we present a tool called Retina, which collects information about students' programming activities, and then provides useful and informative reports to both students and instructors based on the aggregation of that data. Retina can also make real-time recommendations to students, in order to help them quickly address some of the errors they make. In addition to describing Retina and its features, we also present some of our initial \fndings during two trials of the tool in a real classroom setting. (pdf) Approximating a Global Passive Adversary Against Tor Sambuddho Chakravarty, Angelos Stavrou, Angelos D. Keromytis 2008-08-18 We present a novel, practical, and effective mechanism for\r\nidentifying the IP address of Tor clients. We approximate\r\nan almost-global passive adversary (GPA) capable of eavesdropping\r\nanywhere in the network by using LinkWidth, a novel bandwidth-estimation technique. LinkWidth allows network edge-attached entities to estimate the available bandwidth in an arbitrary Internet link without a cooperating peer host, router, or ISP. By modulating the bandwidth of an anonymous connection (e.g., when the destination server\r\nor its router is under our control), we can observe these fluctuations\r\nas they propagate through the Tor network and the Internet to the end-user’s IP address. Our technique exploits one of the design criteria for Tor (trading off GPA-resistance for improved latency/bandwidth over MIXes) by allowing well-provisioned (in terms of bandwidth) adversaries to effectively become GPAs.\r\n\r\nAlthough timing-based attacks have been demonstrated\r\nagainst non-timing-preserving anonymity networks, they have\r\ndepended either on a global passive adversary or on the compromise\r\nof a substantial number of Tor nodes. Our technique\r\ndoes not require compromise of any Tor nodes or collaboration\r\nof the end-server (for some scenarios). We demonstrate\r\nthe effectiveness of our approach in tracking the IP address\r\nof Tor users in a series of experiments. Even for an underprovisioned\r\nadversary with only two network vantage points, we can identify the end user (IP address) in many cases. (pdf) Deux: Autonomic Testing System for Operating System Upgrades Leon Wu, Gail Kaiser, Jason Nieh, Christian Murphy 2008-08-15 Operating system upgrades and patches sometimes\r\nbreak applications that worked fine on the older version.\r\nWe present an autonomic approach to testing of OS updates\r\nwhile minimizing downtime, usable without local regression\r\nsuites or IT expertise. Deux utilizes a dual-layer virtual\r\nmachine architecture, with lightweight application process\r\ncheckpoint and resume across OS versions, enabling simultaneous\r\nexecution of the same applications on both OS versions\r\nin different VMs. Inputs provided by ordinary users\r\nto the production old version are also fed to the new version.\r\nThe old OS acts as a pseudo-oracle for the update,\r\nand application state is automatically re-cloned to continue\r\ntesting after any output discrepancies (intercepted at system\r\ncall level) - all transparently to users. If all differences\r\nare deemed inconsequential, then the VM roles are\r\nswitched with the application state already in place. Our\r\nempirical evaluation with both LAMP and standalone applications\r\ndemonstrates Deux’s efficiency and effectiveness. (pdf) Predictive Models of Gene Regulation Anshul Kundaje 2008-08-15 The regulation of gene expression plays a central role in the development and function of a living cell. A complex network of interacting regulatory proteins bind specific sequence elements in the genome to control the amount and timing of gene expression. The abundance of genome-scale datasets from different organisms provides an opportunity to accelerate our understanding of the mechanisms of gene regulation. Developing computational tools to infer gene regulation programs from high-throughput genomic data is one of the central problems in computational biology.\r\n\r\nIn this thesis, we present a new predictive modeling framework for studying gene regulation. We formulate the problem of learning regulatory programs as a binary classification task: to accurately predict the the condition-specific activation (up-regulation) and repression (down-regulation) of gene expression. The gene expression response is measured by microarray expression data. Genes are represented by various genomic regulatory sequence features. Experimental conditions are represented by the gene expression levels of various regulatory proteins. We use this combination of features to learn a prediction function for the regulatory response of genes under different experimental conditions. The core computational approach is based on boosting. Boosting algorithms allow us to learn high-accuracy, large-margin classifiers and avoid overfitting. We describe three applications of our framework to study gene regulation:\r\n\r\n- In the GeneClass algorithm, we use a compendium of known transcription factor binding sites and gene expression data to learn a global context-specific regulation program that accurately predicts differential expression. GeneClass learns a prediction function in the form of an alternating decision tree, a margin-based generalization of a decision tree. We introduce a novel robust variant of boosting that improves stability and biological interpretability in the presence of correlated features. We also show how to incorporate genome-wide protein-DNA binding data from ChIP-chip experiments into the framework.\r\n- In several organisms, the DNA binding sites of many transcription factors are unknown. Hence, automatic discovery of regulatory sequence motifs is required. In the MEDUSA algorithm, we integrate raw promoter sequence data and gene expression data to simultaneously discover cis regulatory motifs ab initio and learn predictive regulatory programs. MEDUSA automatically learns probabilistic representations of motifs and their corresponding target genes. We show that we are able to accurately learn the binding sites of most known transcription factors in yeast.\r\n- We also design new techniques for extracting biologically and statistically significant information from the learned regulatory models. We use a margin-based score to extract global condition-specific regulomes as well as cluster-specific and gene-specific regulation programs. We develop a post-processing framework for interpreting and visualizing biological information encapsulated in our models.\r\n\r\nWe show the utility of our framework in analyzing several interesting biological contexts (environmental stress responses, DNA-damage response and hypoxia-response) in the budding yeast Saccharomyces cerevisiae. We also show that our methods can learn regulatory programs and cis regulatory motifs in higher eukaryotes such as worms and humans. Several hypotheses generated by our methods are validated by our collaborators using biochemical experiments. Experimental results demonstrate that our framework is quantitatively and qualitatively predictive. We are able to achieve high prediction accuracy on test data and also generate specific, testable hypotheses. (pdf) Using Runtime Testing to Detect Defects in Applications without Test Oracles Christian Murphy, Gail Kaiser 2008-08-07 It is typically infeasible to test a large, complex software system in all its possible configurations and system states prior to deployment. Moreover, some such applications have no test oracles to indicate their correctness. In my thesis, we will address these problems in two ways. First, we suggest that executing tests within the context of an application running in the field can reveal defects that would not ordinarily otherwise be found. Second, we believe that this approach can further be extended to applications for which there is no test oracle by using a variant of metamorphic testing at runtime. (pdf) Towards the Quality of Service for VoIP traffic in IEEE 802.11 Wireless Networks Sangho Shin, Henning Schulzrinne 2008-07-09 The usage of voice over IP (VoIP) traffic in IEEE 802.11 wireless networks is expected to increase in the near future due to widely deployed 802.11 wireless networks and VoIP services on fixed lines. However, the quality of service (QoS) of VoIP traffic in wireless networks is still unsatisfactory. In this thesis, I identify several sources for the QoS problems of VoIP traffic in IEEE 802.11 wireless networks and propose solutions for these problems.\r\n\r\nThe QoS problems discussed can be divided into three categories, namely, user mobility, VoIP capacity, and call admission control. User mobility causes network disruptions during handoffs. In order to reduce the handoff time between Access Points (APs), I propose a new handoff algorithm, Selective Scanning and Caching, which finds available APs by scanning a minimum number of channels and furthermore allows clients to perform handoffs without scanning, by caching AP information. I also describe a new architecture for the client and server side for seamless IP layer handoffs, which are caused when mobile clients change the subnet due to layer 2 handoffs. \r\n\r\nI also present two methods to improve VoIP capacity for 802.11 networks, Adaptive Priority Control (APC) and Dynamic Point Coordination Function (DPCF). APC is a new packet scheduling algorithm at the AP and improves the capacity by balancing the uplink and downlink delay of VoIP traffic, and DPCF uses a polling based protocol and minimizes the bandwidth wasted from unnecessary polling, using a dynamic polling list. Additionally, I estimated the capacity for VoIP traffic in IEEE 802.11 wireless networks via theoretical analysis, simulations, and experiments in a wireless test-bed and show how to avoid mistakes in the measurements and comparisons.\r\n\r\nFinally, to protect the QoS for existing VoIP calls while maximizing the channel utilization, I propose a novel admission control algorithm called QP-CAT (Queue size Prediction using Computation of Additional Transmission), which accurately predicts the impact of new voice calls by virtually transmitting virtual new VoIP traffic. (pdf) genSpace: Exploring Social Networking Metaphors for Knowledge Sharing and Scientific Collaborative Work Christian Murphy, Swapneel Sheth, Gail Kaiser, Lauren Wilcox 2008-06-13 Many collaborative applications, especially in scientific\r\nresearch, focus only on the sharing of tools or the sharing\r\nof data. We seek to introduce an approach to scientific collaboration\r\nthat is based on knowledge sharing. We do this\r\nby automatically building organizational memory and enabling\r\nknowledge sharing by observing what users do with\r\na particular tool or set of tools in the domain, through the\r\naddition of activity and usage monitoring facilities to standalone\r\napplications. Once this knowledge has been gathered,\r\nwe apply social networking models to provide collaborative\r\nfeatures to users, such as suggestions on tools to use,\r\nand automatically-generated sequences of actions based on\r\npast usage amongst the members of a social network or\r\nthe entire community. In this work, we investigate social\r\nnetworking models as an approach to scientific knowledge\r\nsharing, and present an implementation called genSpace,\r\nwhich is built as an extension to the geWorkbench platform\r\nfor computational biologists. Last, we discuss the approach\r\nfrom the viewpoint of social software engineering. (pdf) Application Layer Feedback-based SIP Server Overload Control Charles Shen, Henning Schulzrinne, Erich Nahum 2008-06-06 A SIP server may be overloaded by emergency-induced call volume,\r\n\"American Idol\" style flash crowd effects or denial of service\r\nattacks. The SIP server overload problem is interesting especially\r\nbecause the costs of serving or rejecting a SIP session can be\r\nsimilar. For this reason, the built-in SIP\r\noverload control mechanism based on generating rejection messages\r\ncannot prevent the server from entering congestion collapse under\r\nheavy load. The SIP overload problem calls for a pushback control\r\nsolution in which the potentially overloaded receiving server may\r\nnotify its upstream sending servers to have them send only the\r\namount of load within the receiving server's processing capacity.\r\nThe pushback framework can be achieved by SIP application layer rate-based\r\nfeedback or window-based feedback. The centerpiece of the\r\nfeedback mechanism is the algorithm used to generate load\r\nregulation information. We propose three new window-based feedback\r\nalgorithms and evaluate them together with two existing rate-based\r\nfeedback algorithms. We compare the different algorithms in terms\r\nof the number of tuning parameters and performance under both steady\r\nand variable load. Furthermore, we identify two categories of\r\nfairness requirements for SIP overload control, namely,\r\nuser-centric and provider-centric fairness. With the introduction\r\nof a new double-feed SIP overload control architecture, we show\r\nhow the algorithms meet those fairness criteria. (pdf) CPU Torrent -- CPU Cycle Offloading to Reduce User Wait Time and Provider Resource Requirements Swapneel Sheth, Gail Kaiser 2008-06-04 Developers of novel scientific computing systems are often eager to\r\nmake their algorithms and databases available for community use, but\r\ntheir own computational resources may be inadequate to fulfill\r\nexternal user demand -- yet the system's footprint is far too large\r\nfor prospective user organizations to download and run locally.  Some\r\nheavyweight systems have become part of designated ``centers''\r\nproviding remote access to supercomputers and/or clusters supported by\r\nsubstantial government funding; others use virtual supercomputers\r\ndispersed across grids formed by massive numbers of volunteer\r\nInternet-connected computers. But public funds are limited and not all\r\nsystems are amenable to huge-scale divisibility into independent\r\ncomputation units. We have identified a class of scientific computing\r\nsystems where ``utility'' sub-jobs can be offloaded to any of several\r\nalternative providers thereby freeing up local cycles for the main\r\nproprietary jobs, implemented a proof-of-concept framework enabling\r\nsuch deployments, and analyzed its expected throughput and\r\nresponse-time impact on a real-world bioinformatics system (Columbia's\r\nPredictProtein) whose present users endure long wait queues. (pdf) FairTorrent: Bringing Fairness to Peer-to-Peer Systems Alex Sherman, Jason Nieh, Clifford Stein 2008-05-27 The lack of fair bandwidth allocation in Peer-to-Peer systems causes\r\n many performance problems, including users being disincentivized from\r\ncontributing upload bandwidth, free riders taking as much from the\r\nsystem as possible while contributing as little as possible, and a\r\nlack of quality-of-service guarantees to support streaming\r\napplications.  We present FairTorrent, a simple distributed scheduling\r\nalgorithm for Peer-to-Peer systems that fosters fair bandwidth\r\nallocation among peers.  For each peer, FairTorrent maintains a deficit\r\ncounter which represents the number of bytes uploaded to a peer minus\r\nthe number of bytes downloaded from it.  It then uploads to the peer\r\nwith the lowest deficit counter.  FairTorrent automatically adjusts to\r\nvariations in bandwidth among peers and is resilient to exploitation\r\nby free-riding peers.  We have implemented FairTorrent inside a BitTorrent client without modifications to the BitTorrent protocol, and compared its performance on PlanetLab against other widely-used BitTorrent clients.  Our results show that FairTorrent can provide up to two orders of magnitude better fairness and up to five times better download performance for high contributing peers.  It thereby gives users an incentive to contribute more bandwidth, and improve overall system performance. (pdf) IEEE 802.11 in the Large: Observations at an IETF Meeting Andrea G. Forte, Sangho Shin, Henning Schulzrinne 2008-05-05 We observed wireless network traffic at the 65th IETF Meeting in Dallas, Texas in March of 2006, attended by approximately 1200 engineers. The event was supported by a very large number of 802.11a and 802.11b access points, often seeing hundreds of simultaneous users. We were particularly interested in the stability of wireless connectivity, load balancing and loss behavior, rather than just traffic.We observed distinct differences among client implementations and saw a number of factors that made the overall system less than optimal, pointing to the need for better design tools and automated adaptation mechanisms. (pdf) ROFL: Routing as the Firewall Layer Hang Zhao, Chi-Kin Chau, Steven M. Bellovin 2008-05-03 We propose a firewall architecture that treats port numbers as part of the IP address.  Hosts permit connectivity to a service by advertising the IPaddr:port/48 address; they block connectivity by ensuring that there is no route to it.  This design, which is especially well-suited to MANETs, provides greater protection against insider attacks than do conventional firewalls, but drops unwanted traffic far earlier than distributed firewalls do. (pdf) Stored Media Streaming in BitTorrent-like P2P Networks Kyung-Wook Hwang, Vishal Misra, Dan Rubenstein 2008-05-01 Peer-to-peer (P2P) networks exist on the Internet today\r\nas a popular means of data distribution. However, conventional\r\nuses of P2P networking involve distributing stored\r\nfiles for use after the entire file has been downloaded. In\r\nthis work, we investigate whether P2P networking can be\r\nused to provide real-time playback capabilities for stored\r\nmedia. For real-time playback, users should be able to start\r\nplayback immediately, or almost immediately, after requesting\r\nthe media and have uninterrupted playback during the\r\ndownload. To achieve this goal, it is critical to efficiently\r\nschedule the order in which pieces of the desired media\r\nare downloaded. Simply downloading pieces in sequential\r\n(earliest-first) order is prone to bottlenecks. Consequently\r\nwe propose a hybrid of earliest-first and rarest-first scheduling\r\n- ensuring high piece diversity while at the same time\r\nprioritizing pieces needed to maintain uninterrupted playback.\r\nWe consider an approach to peer-assisted streaming\r\nthat is based on BitTorrent. In particular, we show that dynamic\r\nadjustment of the probabilities of earliest-first and\r\nrarest-first strategies along with utilization of coding techniques\r\npromoting higher data diversity, can offer noticeable\r\nimprovements for real-time playback. (pdf) ReoptSMART: A Learning Query Plan Cache Julia Stoyanovich, Kenneth A. Ross, Jun Rao, Wei Fan, Volker Markl, Guy Lohman 2008-04-24 The task of query optimization in modern relational database systems\r\nis important but can be computationally expensive.  Parametric query\r\noptimization(PQO) has as its goal the prediction of optimal query\r\nexecution plans based on historical results, without consulting the\r\nquery optimizer.  We develop machine learning techniques that can\r\naccurately model the output of a query optimizer. Our algorithms\r\nhandle non-linear boundaries in plan space and achieve high prediction\r\naccuracy even when a limited amount of data is available for training.\r\nWe use both predicted and actual query execution times for learning,\r\nand are the first to demonstrate a total net win of a PQO method over\r\na state-of-the-art query optimizer for some workloads.  ReoptSMART\r\nrealizes savings not only in optimization time, but also in query\r\nexecution time, for an over-all improvement by more than an order of\r\nmagnitude in some cases. (pdf) Masquerade Detection Using a Taxonomy-Based Multinomial Modeling Approach in UNIX Systems Malek Ben Salem, Salvatore J. Stolfo 2008-04-14 This paper presents one-class Hellinger distance-based and one-class SVM modeling techniques that use a set of features to reveal user intent. The specific objective is to model user command profiles and detect deviations indicating a masquerade attack. The approach aims to model user intent, rather than only modeling sequences of user issued commands.  We hypothesize that each individual user will search in a targeted and limited fashion in order to find information germane to their current task. Masqueraders, on the other hand, will likely not know the file system and layout of another user's desktop, and would likely search more extensively and broadly. Hence, modeling a user search behavior to detect deviations may more accurately detect masqueraders. To that end, we extend prior research that uses UNIX command sequences issued by users as the audit source by relying upon an abstraction of commands. We devised a taxonomy of UNIX commands that is used to abstract command sequences.  The experimental results show that the approach does not lose information and performs comparably to or slightly better than the modeling approach based on simple UNIX command frequencies. (pdf) Approximating the Permanent with Belief Propagation Bert Huang, Tony Jebara 2008-04-05 This work describes a method of approximating matrix permanents efficiently using belief propagation. We formulate a probability distribution whose partition function is exactly the permanent, then use Bethe free energy to approximate this partition function. After deriving some speedups to standard belief propagation, the resulting algorithm requires $(n^2)$ time per iteration. Finally, we demonstrate the advantages of using this approximation. (pdf) Behavior-Based Network Access Control: A Proof-of-Concept Vanessa Frias-Martinez 2008-03-27 Current NAC technologies implement a pre-connect phase where the status of a device is checked against a set of policies before being granted access to a network, and a post-connect phase that examines whether the device complies with the policies that correspond to its role in the network. In order to enhance current NAC technologies, we propose a new architecture based on behaviors rather than roles or identity, where the policies are automatically learned and updated over time by the members of the network in order to adapt to behavioral\r\nchanges of the devices. Behavior proﬁles may be presented as identity cards that can change over time. By incorporating an Anomaly Detector (AD) to the NAC server or to each of the hosts, their behavior proﬁle is\r\nmodeled and used to determine the type of behaviors that should be accepted within the network. These models constitute behavior-based policies. In our enhanced NAC architecture, global decisions are made using a group voting process. Each host’s behavior proﬁle is used to compute a partial decision for or against the acceptance of a new proﬁle or trafﬁc. The aggregation of these partial votes amounts to the model-group decision. This voting process makes the architecture more resilient to attacks. Even after accepting a certain percentage of malicious devices, the enhanced NAC is able to compute an adequate decision. We provide proof-of-concept experiments of our architecture using web trafﬁc from our department network. Our results show that the model-group decision approach based on behavior proﬁles has a 99% detection rate of anomalous trafﬁc with a false positive rate of\r\nonly 0.005%. Furthermore, the architecture achieves short latencies for both the pre- and post-connect phases. (pdf) Path-based Access Control for Enterprise Networks Matthew Burnside, Angelos D. Keromytis 2008-03-27 Enterprise networks are ubiquitious and increasingly complex.  The\r\nmechanisms for defining security policies in these networks have not\r\nkept up with the advancements in networking technology.  In most\r\ncases, system administrators must define policies on a per-application\r\nbasis, and subsequently, these policies do not interact.  For example,\r\nthere is no mechanism that allows a firewall to communicate decisions\r\nbased on its ruleset to a web server behind it, even though decisions\r\nbeing made at the firewall may be relevant to decisions made at the\r\nweb server.  In this paper, we describe a path-based access control\r\nsystem which allows applications in a network to pass\r\naccess-control-related information to neighboring applications, as the\r\napplications process requests from outsiders and from each other.\r\nThis system defends networks against a class of attacks wherein\r\nindividual applications may make correct access control decisions but\r\nthe resulting network behavior is incorrect.  We demonstrate the\r\nsystem on service-oriented architecture (SOA)-style networks, in two\r\nforms, using graph-based policies, and leveraging the KeyNote trust\r\nmanagement system. (pdf) Tractability of multivariate approximation over a weighted unanchored Sobolev space: Smoothness sometimes hurts Arthur G. Werschulz, Henryk Wozniakowski 2008-03-25 We study $d$-variate approximation for a weighted unanchored Sobolev\r\n  space having smoothness $m\\ge1$.  Folk wisdom would lead us to believe\r\n  that this problem should become easier as its smoothness increases.  This\r\n  is true if we are only concerned with asymptotic analysis: the $n$th\r\n  minimal error is of order~$n^{-(m-\\delta)}$ for any $\\delta>0$.  However,\r\n  it is unclear how long we need to wait before this asymptotic behavior\r\n  kicks in.  How does this waiting period depend on $d$ and~$m$?  We prove\r\n  that no matter how the weights are chosen, the waiting period is at\r\n  least~$m^d$, even if the error demand~$\\varepsilon$ is arbitrarily close\r\n  to~$1$.  Hence, for $m\\ge2$, this waiting period is exponential in~$d$,\r\n  so that the problem suffers from the curse of dimensionality and is\r\n  intractable.  In other words, the fact that the asymptotic behavior\r\n  improves with~$m$ is irrelevant when $d$~is large.  So, we will be unable\r\n  to vanquish the curse of dimensionality unless $m=1$, i.e., unless the\r\n  smoothness is minimal.  We then show that our problem \\emph{can} be\r\n  tractable if $m=1$.  That is, we can find an $\\varepsilon$-approximation\r\n  using polynomially-many (in $d$ and~$\\varepsilon^{-1}$) information\r\n  operations, even if only function values are permitted.  When $m=1$, it\r\n  is even possible for the problem to be \\emph{strongly} tractable, i.e.,\r\n  we can find an $\\varepsilon$-approximation using polynomially-many\r\n  (in~$\\varepsilon^{-1}$) information operations, independent of~$d$.\r\n  These positive results hold when the weights of the Sobolev space decay\r\n  sufficiently quickly or are bounded finite-order weights, i.e., the\r\n  $d$-variate functions we wish to approximate can be decomposed as sums of\r\n  functions depending on at most~$\\omega$ variables, where $\\omega$ is\r\n  independent of~$d$. (pdf) Spreadable Connected Autonomic Networks (SCAN) Joshua Reich, Vishal Misra, Dan Rubestein, Gil Zussman 2008-03-24 A Spreadable Connected Autonomic Network (SCAN) is a mobile network that automatically maintains its own connectivity as nodes move.  We envision SCANs to enable a diverse set of applications such as self-spreading mesh networks and robotic search and rescue systems.  This paper describes our experiences developing a prototype robotic SCAN built from commercial, off-the-shelf hardware, to support such applications.  A major contribution of our work is the development of a protocol, called SCAN1, which maintains network connectivity by enabling individual nodes to determine when they must constrain their mobility in order to avoid disconnecting the network.  SCAN1 achieves its goal through an entirely distributed process in which individual nodes utilize only local (2-hop) knowledge of the network's topology to periodically make a simple decision: move, or freeze in place.   Along with experimental results from our hardware testbed, we model SCAN1's performance, providing both supporting analysis and simulation for the efficacy of SCAN1 as a solution to enable SCANs.  While our evaluation of SCAN1 in this paper is limited to systems whose capabilities match those of our testbed, SCAN1 can be utilized in conjunction with a wide-range of potential applications and environments, as either a primary or backup connectivity maintenance mechanism. (pdf) Leveraging Local Intra-Core Information to Increase Global Performance in Block-Based Design of Systems-on-Chip Cheng-Hong Li, Luca P. Carloni 2008-03-18 Latency-insensitive design is a methodology for system-on-chip (SoC) design that simplifies the reuse of intellectual property cores and the implementation of the communication among them. This simplification is based on a system-level protocol that decouples the intra-core logic design from the design of the inter-core communication channels. Each core is encapsulated within a shell, a synthesized logic block that dynamically controls its operation to interface it with the rest of the SoC and to absorb any latency variations on its I/O signals. In particular, a shell stalls a core whenever new valid data are not available on the input channels or a down-link core has requested a delay in the data production on the output channels. \r\n\r\nWe study how knowledge about the internal logic structure of a core can be applied to the design of its shell to improve the overall system-level performance by avoiding unnecessary local stalling. We introduce the notion of functional independence conditions (FIC) and present a novel circuit design of a generic shell template that can leverage FIC. We propose a procedure for the logic synthesis of a FIC-shell instance that is only based on the analysis of the intra-core logic and does not require any input from the designers. Finally, we present a comprehensive experimental analysis that shows the performance benefits and limited design overhead of the proposed technique. This includes the semi-custom design of an SoC, an ultra-wideband baseband transmitter, using a 90nm industrial standard cell library. (pdf) The Delay-Friendliness of TCP Eli Brosh, Salman Baset, Vishal Misra, Dan Rubenstein, Henning Schulzrinne 2008-03-10 TCP has been traditionally considered unfriendly for real-time\r\napplications. Nonetheless, popular applications such as Skype use\r\nTCP due to the deployment of NATs and firewalls that prevent UDP\r\ntraffic. Motivated by this observation we study the delay\r\nperformance of TCP for real-time media flows. We develop an\r\nanalytical performance model for the delay of TCP. We use\r\nextensive experiments to validate the model and to evaluate the\r\nimpact of various TCP mechanisms on its delay performance. Based\r\non our results, we derive the working region for VoIP and live\r\nvideo streaming applications and provide guidelines for\r\ndelay-friendly TCP settings. Our research indicates that simple\r\napplication-level schemes, such as packet splitting and parallel\r\nconnections, can reduce the delay of real-time TCP flows by as\r\nmuch as 30\\% and 90\\%, respectively. (pdf) (ps) Properties of Machine Learning Applications for Use in Metamorphic Testing Christian Murphy, Gail Kaiser, Lifeng Hu 2008-02-28 It is challenging to test machine learning (ML) applications, which are intended to learn properties of data sets where the correct answers are not already known. In the absence of a test oracle, one approach to testing these applications is to use metamorphic testing, in which properties of the application are exploited to define transformation\r\nfunctions on the input, such that the new output will be unchanged or can easily be predicted based on the original output; if the output is not as expected, then a defect must exist in the application. Here, we seek to enumerate and classify the metamorphic properties of some machine learning algorithms, and demonstrate how these can be applied\r\nto reveal defects in the applications of interest. In addition to the results of our testing, we present a set of properties that can be used to define these metamorphic relationships so that metamorphic testing can be used as a general approach to testing machine learning applications. (pdf) The Impact of SCTP on Server Scalability and Performance Kumiko Ono, Henning Schulzrinne 2008-02-28 The Stream Control Transmission Protocol (SCTP) is a newer transport\r\nprotocol, having additional features to TCP.  Although SCTP is an\r\nalternative transport protocol for the Session Initiation Protocol\r\n(SIP), we do not know how SCTP features influence SIP server\r\nscalability and performance. To estimate this, we measured the\r\nscalability and performance of two servers, an echo server and a\r\nsimplified SIP server on Linux, comparing to TCP.\r\nOur measurements found that using SCTP does not significantly\r\naffect on data latency: approximately 0.3 ms longer for the handshake than that for TCP. However, server scalability in terms of the number\r\nof sustainable associations drops to 17-21%, or to 43% of TCP if we \r\nadjust the acceptable gap size of unordered data delivery. (pdf) Optimal Splitters for Database Partitioning with Size Bounds Kenneth A. Ross, John Cieslewicz 2008-02-27 Partitioning is an important step in several database algorithms, including sorting, aggregation, and joins. Partitioning is also fundamental for dividing work into equal-sized (or balanced) parallel subtasks. In this paper, we aim to find, materialize and maintain a set of partitioning elements (splitters) for a data set. Unlike traditional partitioning elements, our splitters define both inequality and equality partitions, which allows us to bound the size of the inequality partitions. We provide an algorithm for determining an optimal set of splitters from a sorted data set and show that it has time complexity O(k lg_2 N), where k is the number of splitters requested and N is the size of the data set. We show how the algorithm can be extended to pairs of tables, so that joins can be partitioned into work units that have balanced cost. We demonstrate experimentally (a) that finding the optimal set of splitters can be done efficiently, and (b) that using the precomputed splitters can improve the time to sort a data set by up to 76%, with particular benefits in the presence of a few heavy hitters. (pdf) One Server Per City: Using TCP for Very Large SIP Servers Kumiko Ono, Henning Schulzrinne 2008-02-26 The transport protocol for SIP can be chosen based on the requirements\r\nof services and network conditions.  How does the choice of TCP affect the scalability and performance compared to UDP? We experimentally analyze the impact of using TCP as a transport protocol for a SIP server. We first investigate scalability of a TCP echo server, then compare performance of a SIP server for three TCP connection lifetimes: transaction, dialog, and persistent. Our results show that a Linux machine can establish 450,000+ TCP connections and maintaining connections does not affect the transaction response time. Additionally, the transaction response times using the three TCP connection lifetimes and UDP show no significant difference at 2,500 registration requests/second and at 500 call requests/second. However, sustainable request rate is lower for TCP than for UDP, since using TCP requires more message processing. More message processing causes longer delays at the thread queue for the server implementing a thread-pool model. Finally, we suggest how to reduce the impact of TCP for a scalable SIP server especially under overload control. This is applicable to other servers with very large connection counts. (pdf) Newspeak: A Secure Approach for Designing Web Applications Kyle Dent, Steven M. Bellovin 2008-02-16 Internet applications are being used for more and more important business and personal purposes. Despite efforts to lock down web servers and isolate databases, there is an inherent problem in the web application architecture that leaves databases necessarily exposed to possible attack from the Internet. We propose a new design that removes the web server as a trusted component of the architecture and provides an extra layer of protection against database attacks. We have created a prototype system that demonstrates the feasibility of the new design. (pdf) Summary-Based Pointer Analysis Framework for Modular Bug Finding Marcio O. Buss 2008-02-07 Modern society is irreversibly dependent on computers and,\r\nconsequently, on software. However, as the complexity of programs\r\nincrease, so does the number of defects within them. To\r\nalleviate the problem, automated techniques are constantly used to\r\nimprove software quality. Static analysis is one such approach in\r\nwhich violations of correctness properties are searched and\r\nreported. Static analysis has many advantages, but it is necessarily\r\nconservative because it symbolically executes the program instead of\r\nusing real inputs, and it considers all possible executions\r\nsimultaneously. Being conservative often means issuing false alarms,\r\nor missing real program errors.\r\n\r\nPointer variables are a challenging aspect of many languages that can\r\nforce static analysis tools to be overly conservative. It is often\r\nunclear what variables are affected by pointer-manipulating\r\nexpressions, and aliasing between variables is one of the banes of\r\nprogram analysis. To alleviate that, a common solution is to allow\r\nthe programmer to provide annotations such as declaring a variable\r\nas unaliased in a given scope, or providing special constructs\r\nsuch as the ``never-null'' pointer of Cyclone. However,\r\nprogrammers rarely keep these annotations up-to-date.\r\n\r\nThe solution is to provide some form of pointer analysis, which\r\nderives useful information about pointer variables in the program.  An\r\nappropriate pointer analysis equips the static tool so that it is\r\ncapable of reporting more errors without risking too many false alarms.\r\n\r\nThis dissertation proposes a methodology for pointer analysis that is\r\nspecially tailored for ``modular bug finding.'' It presents a new\r\nanalysis space for pointer analysis, defined by finer-grain ``dimensions\r\nof precision,'' which allows us to explore and evaluate a variety of\r\ndifferent algorithms to achieve better trade-offs between analysis\r\nprecision and efficiency. This framework is developed around a new\r\nabstraction for computing points-to sets, the Assign-Fetch Graph, that\r\nhas many interesting features. Empirical evaluation shows promising\r\nresults, as some unknown errors in well-known applications were\r\ndiscovered. (pdf) SPARSE: A Hybrid System to Detect Malcode-Bearing Documents Wei-Jen Li, Salvatore J. Stolfo 2008-01-31 Embedding malcode within documents provides a convenient means of penetrating systems which may be unreachable by network-level service attacks. Such attacks can be very targeted and difficult to detect compared to the typical network worm threat due to the multitude of document-exchange vectors. Detecting malcode embedded in a document is difficult owing to the complexity of modern document formats that provide ample opportunity to embed code in a myriad of ways. We focus on Microsoft Word documents as malcode carriers as a case study in this paper. We introduce a hybrid system that integrates static and dynamic techniques to detect the presence and location of malware embedded in documents. The system is designed to automatically update its detection models to improve accuracy over time. The overall hybrid detection system with a learning feedback loop is demonstrated to achieve a 99.27% detection rate and 3.16% false positive rate on a corpus of 6228 Word documents. (pdf) The In Vivo Approach to Testing Software Applications Christian Murphy, Gail Kaiser, Matt Chu 2008-01-31 Software products released into the field typically have some number of residual bugs that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system. Testing approaches such as perpetual testing or continuous testing seek to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present our initial work towards a testing methodology we call in vivo testing, in which unit tests are continuously executed inside a running application in the deployment environment. These tests execute within the current state of the program (rather than by creating a clean slate) without affecting or altering that state. Our approach can reveal defects both in the applications of interest and in the unit tests themselves. It can also be used for detecting concurrency or robustness issues that may not have appeared in a testing lab. Here we describe the approach and the testing framework called Invite that we have developed for Java applications. We also enumerate the classes of bugs our approach can discover, and provide the results of a case study on a publicly-available application, as well as the results of experiments to measure the added overhead. (pdf) Mitigating the Effect of Free-Riders in BitTorrent using Trusted Agents Alex Sherman, Angelos Stavrou, Jason Nieh, Cliff Stein 2008-01-25 Even though Peer-to-Peer (P2P) systems present a cost-effective and\r\nscalable solution to content distribution, most entertainment, media\r\nand software, content providers continue to rely on expensive,\r\ncentralized solutions such as Content Delivery Networks. One of the\r\nmain reasons is that the current P2P systems cannot guarantee\r\nreasonable performance as they depend on the willingness of users to\r\ncontribute bandwidth. Moreover, even systems like BitTorrent, which\r\nemploy a tit-for-tat protocol to encourage fair bandwidth exchange\r\nbetween users, are prone to free-riding (i.e. peers that do not\r\nupload). Our experiments on PlanetLab extend previous research\r\n(e.g. LargeViewExploit, BitTyrant) demonstrating that\r\nsuch selfish behavior can seriously degrade the performance of regular\r\nusers in many more scenarios beyond simple free-riding: we observed an\r\noverhead of upto 430\\% for 80\\% of free-riding identities easily\r\ngenerated by a small set of selfish users.\r\n\r\nTo mitigate the effects of selfish users, we propose a new P2P\r\narchitecture that classifies peers with the help of a small number of\r\n{\\em trusted nodes} that we call Trusted Auditors (TAs). TAs\r\nparticipate in P2P download like regular clients and detect\r\nfree-riding identities by observing their neighbors' behavior. Using\r\nTAs, we can separate compliant users into a separate service pool\r\nresulting in better performance. Furthermore, we show that TAs are\r\nmore effective ensuring the performance of the system than a mere\r\nincrease in bandwidth capacity: for 80\\% of free-riding identities a\r\nsingle-TA system has a 6\\% download time overhead while without the\r\nTA and three times the bandwidth capacity we measure a 100\\%\r\noverhead. (pdf) A Distance Learning Approach to Teaching eXtreme Programming Christian Murphy, Dan Phung, Gail Kaiser 2008-01-23 As university-level distance learning programs become more and more popular, and software engineering courses incorporate eXtreme Programming (XP) into their curricula, certain challenges arise when teaching XP to students who are not physically co-located. In this paper, we present the results of a three-year study of such an online software engineering course targeted to graduate students, and describe some of the specific challenges faced, such as students’ aversion to aspects of XP and difficulties in scheduling. We discuss our findings in terms of the course’s educational objectives, and present suggestions to other educators who may face similar situations. (pdf) Topology-Based Performance Analysis and Optimization of Latency-Insensitive Systems Rebecca Collins, Luca Carloni 2008-01-15 Latency-insensitive protocols allow system-on-chip engineers to decouple the design of the computing cores from the design of the inter-core communication channels while following the synchronous design paradigm.\r\nIn a latency-insensitive system (LIS) each core is encapsulated within a shell, a synthesized interface module that dynamically controls its operation.  At each clock period, if new data has not arrived on an input channel or a stalling request has arrived on an output channel, the shell stalls the core and buffers other incoming valid data for future processing.  The combination of finite buffers and backpressure from stalling can cause throughput degradation.  Previous works addressed this problem by increasing buffer space to reduce the backpressure requests or inserting extra buffering to balance the channel latency around a LIS.  We explore the theoretical complexity of these approaches and propose a heuristic algorithm for efficient queue sizing.  We also practically characterize several LIS topologies and how the topology of a LIS can impact not only how much throughput degradation will occur, but also the difficulty of finding optimal queue sizing solutions. (pdf) LinkWidth: A Method to Measure Link Capacity and Available Bandwidth using Single-End Probes Sambuddho Chakravarty, Angelos Stavrou, Angelos D. Keromytis 2008-01-05 We introduce LinkWidth, a method for estimating capacity and available\r\nbandwidth using single-end controlled TCP packet probes. To estimate\r\ncapacity, we generate a train of TCP RST packets ``sandwiched''\r\nbetween trains of TCP SYN packets. Capacity is computed from the\r\nend-to-end packet dispersion of the received TCP RST/ACK packets\r\ncorresponding to the TCP SYN packets going to closed ports. Our\r\ntechnique is significantly different from the rest of the packet-pair\r\nbased measurement techniques, such as {\\em CapProbe,} {\\em pathchar}\r\nand {\\em pathrate,} because the long packet trains minimize errors due\r\nto bursty cross-traffic. Additionally, TCP RST packets do not generate\r\nadditional ICMP replies, thus avoiding cross-traffic due to such\r\npackets from interfering with our probes. In addition, we use TCP\r\npackets for all our probes to prevent QoS-related traffic shaping\r\n(based on packet types) from affecting our measurements (eg. CISCO\r\nrouters by default are known have to very high latency while\r\ngenerating to ICMP TTL expired replies).\r\n\r\nWe extend the {\\it Train of Packet Pairs} technique to approximate the\r\navailable link capacity. We use a train of TCP packet pairs with\r\nvariable intra-pair delays and sizes. This is the first attempt to\r\nimplement this technique using single-end TCP probes, tested on a\r\nrange of networks with different bottleneck capacities and cross\r\ntraffic rates. The method we use for measuring from a single point of\r\ncontrol uses TCP RST packets between a train of TCP SYN packets. The\r\nidea is quite similar to the technique for measuring the bottleneck\r\ncapacity. We compare our prototype with {\\em pathchirp,} {\\em\r\npathload,} {\\em IPERF,} which require control of both ends as well as\r\nanother single end controlled technique {\\em abget}, and demonstrate\r\nthat in most cases our method gives approximately the same results if\r\nnot better. (pdf) Autotagging to Improve Text Search for 3D Models Corey Goldfeder, Peter Allen 2008-01-02 Text search on 3D models has traditionally worked poorly, as text annotations on 3D models are often unreliable or incomplete. In this paper we attempt to improve the recall of text search by automatically assigning appropriate tags to models. Our algorithm finds relevant tags by appealing to a large corpus of partially labeled example models, which does not have to be preclassified or otherwise prepared. For this purpose we use a copy of Google 3DWarehouse, a database of user contributed models which is publicly available on the Internet. Given a model to tag, we find geometrically similar models in the corpus, based on distances in a reduced dimensional space derived from Zernike descriptors. The labels of these neighbors are used as tag candidates for the model with probabilities proportional to the degree of geometric similarity. We show experimentally that text based search for 3D models using our computed tags can work as well as geometry based search. Finally, we demonstrate our 3D model search engine that uses this algorithm and discuss some implementation issues. (pdf) Schema Polynomials and Applications Kenneth A. Ross, Julia Stoyanovich 2007-12-17 Conceptual complexity is emerging as a new bottleneck as database\r\ndevelopers, application developers, and database administrators\r\nstruggle to design and comprehend large, complex schemas.  The\r\nsimplicity and conciseness of a schema depends critically on the\r\nidioms available to express the schema.  We propose a formal\r\nconceptual schema representation language that combines different\r\ndesign formalisms, and allows schema manipulation that exposes the\r\nstrengths of each of these formalisms. We demonstrate how the schema\r\nfactorization framework can be used to generate relational,\r\nobject-oriented, and faceted physical schemas, allowing a wider\r\nexploration of physical schema alternatives than traditional\r\nmethodologies.  We illustrate the potential practical benefits of\r\nschema factorization by showing that simple heuristics can\r\nsignificantly reduce the size of a real-world schema description.  We\r\nalso propose the use of schema polynomials to model and derive\r\nalternative representations for complex relationships with\r\nconstraints. (pdf) A Recursive Data-Driven Approach to Programming Multicore Systems Rebecca Collins, Luca Carloni 2007-12-05 In this paper, we propose a method to program divide-and-conquer problems on multicore systems that is based on a data-driven recursive programming model.  Data intensive programs are difficult to program on multicore architectures because they require efficient utilization of inter-core communication.  Models for programming multicore systems available today generally lack the ability to automatically extract concurrency from a sequential style program and map concurrent tasks to efficiently leverage data and temporal locality.  For divide-and-conquer algorithms, a recursive programming model can address both of these problems.  Furthermore, since a recursive function has the same behavior patterns at all granularities of a problem, the same recursive model can be used to implement a multicore program at all of its levels: 1. the operations of a single core, 2. how to distribute tasks among several cores, and 3. in what order  to schedule tasks on a multicore system when it is not possible to schedule all of the tasks at the same time.  We present a novel selective execution technique that can enable automatic parallelization and task mapping of a recursive program onto a multicore system.  To verify the practicality of this approach, we perform a case-study of bitonic sort on the Cell BE processor. (pdf) Speech Enabled Avatar from a Single Photograph Dmitri Bitouk, Shree K.  Nayar 2007-11-25 This paper presents a complete framework for creating speech-enabled\r\n2D and 3D avatars from a single image of a person. Our approach uses a\r\ngeneric facial motion model which represents deformations of the prototype face during speech. \r\nWe have developed an HMM-based facial\r\nanimation algorithm which takes into account both lexical stress and\r\ncoarticulation. This algorithm produces realistic animations of the\r\nprototype facial surface from either text or speech. The generic facial motion model is transformed to\r\na novel face geometry using a set of corresponding points between the generic mesh and the novel face.\r\nIn the case of a 2D avatar, a single photograph of the person is used as input. We manually select a small number of features on the photograph and these are used to deform the prototype surface. The deformed surface is then used to animate the photograph. In the case of a 3D avatar, we use a single stereo image of the person as input. The sparse geometry of the face is computed from this image and used to warp the prototype surface to obtain the complete 3D surface of the person's face. This surface is etched into a glass cube using sub-surface laser engraving (SSLE) technology. Synthesized facial animation videos are then projected onto the etched glass cube. Even though the etched surface is static, the projection of facial animation onto it results in a compelling experience for the viewer. We show several examples of 2D and 3D avatars that are driven by text and speech inputs. (pdf) Partial Evaluation for Code Generation from Domain-Specific Languages Jia Zeng 2007-11-20 Partial evaluation has been applied to compiler optimization and\r\ngeneration for decades. Most of the successful partial evaluators have\r\nbeen designed for general-purpose languages. Our observation is that\r\ndomain-specific languages are also suitable targets for partial\r\nevaluation. The unusual computational models in many DSLs bring\r\nchallenges as well as optimization opportunities to the compiler.\r\n\r\nTo enable aggressive optimization, partial evaluation\r\nhas to be specialized to fit the specific paradigm of a DSL. In this\r\ndissertation, we present three such specialized partial evaluation\r\ntechniques designed for specific languages that address a variety of\r\ncompilation concerns. The first algorithm provides a low-cost solution\r\nfor simulating concurrency on a single-threaded processor. The second\r\nenables a compiler to compile modest-sized synchronous programs in\r\npieces that involve communication cycles. The third statically\r\nelaborates recursive function calls that enable programmers to\r\ndynamically create a system's concurrent components in a convenient\r\nand algorithmic way. Our goal is to demonstrate the potential of\r\npartial evaluation to solve challenging issues in code generation for\r\ndomain-specific languages.\r\n\r\nNaturally, we do not cover all DSL compilation issues.  We hope our\r\nwork will enlighten and encourage future research on the application\r\nof partial evaluation to this area. (pdf) Distributed In Vivo Testing of Software Applications Matt Chu, Christian Murphy, Gail Kaiser 2007-11-16 The in vivo software testing approach focuses on testing\r\nlive applications by executing unit tests throughout the\r\nlifecycle, including after deployment. The motivation is that\r\nthe “known state” approach of traditional unit testing is unrealistic;\r\ndeployed applications rarely operate under such\r\nconditions, and it may be more informative to perform the\r\ntesting in live environments. One of the limitations of this\r\napproach is the high performance cost it incurs, as the unit\r\ntests are executed in parallel with the application. Here we\r\npresent distributed in vivo testing, which focuses on easing\r\nthe burden by sharing the load across multiple instances of\r\nthe application of interest. That is, we elevate the scope\r\nof in vivo testing from a single instance to a community of\r\ninstances, all participating in the testing process. Our approach\r\nis different from prior work in that we are actively\r\ntesting during execution, as opposed to passively monitoring\r\nthe application or conducting tests in the user environment\r\nprior to execution. We discuss new extensions to\r\nthe existing in vivo testing framework (called Invite) and\r\npresent empirical results that show the performance overhead\r\nimproves linearly with the number of clients. (pdf) Tractability of the Helmholtz equation with non-homogeneous Neumann boundary conditions: Relation to $L_2$-approximation Arthur G. Werschulz 2007-11-08 We want to compute a worst case $\\varepsilon$-approximation to the solution\r\nof the Helmholtz equation $-\\Delta u+qu=f$ over the unit $d$-cube~$I^d$,\r\nsubject to Neumann boundary conditions $\\partial_\\nu u=g$ on~$\\partial\r\nI^d$.  Let $\\mathop{\\rm card}(\\varepsilon,d)$ denote the minimal number of\r\nevaluations of $f$, $g$, and~$q$ needed to compute an absolute or\r\nnormalized $\\varepsilon$-approximation, assuming that $f$, $g$, and~$q$\r\nvary over balls of weighted reproducing kernel Hilbert spaces.  This\r\nproblem is said to be weakly tractable if $\\mathop{\\rm\r\n  card}(\\varepsilon,d)$ grows subexponentially in~$\\varepsilon^{-1}$ and\r\n$d$.  It is said to be polynomially tractable if $\\mathop{\\rm\r\n  card}(\\varepsilon,d)$ is polynomial in~$\\varepsilon^{-1}$ and~$d$, and\r\nstrongly polynomially tractable if this polynomial is independent of~$d$.\r\nWe have previously studied tractability for the homogeneous version $g=0$\r\nof this problem.  In this paper, we investigate the tractability of the\r\nnon-homogeneous problem, with general~$g$.  First, suppose that we use\r\nproduct weights, in which the role of any variable is moderated by its\r\nparticular weight.  We then find that if the weight sum is sublinearly\r\nbounded, then the problem is weakly tractable; moreover, this condition is\r\nmore or less necessary.  We then show that the problem is polynomially\r\ntractable if the weight sum is logarithmically or uniformly bounded, and we\r\nestimate the exponents of tractability for these two cases.  Next, we turn\r\nto finite-order weights of fixed order~$\\omega$, in which a $d$-variate\r\nfunction can be decomposed as sum, each term depending on at most\r\n$\\omega$~variables.  We show that the problem is always polynomially\r\ntractable for finite-order weights, and we give estimates for the exponents\r\nof tractability.  Since our results so far have established nothing\r\nstronger than polynomial tractability, we look more closely at whether\r\nstrong polynomial tractability is possible.  We show that our problem is\r\nnever strongly polynomially tractable for the absolute error criterion.\r\nMoreover, we believe that the same is true for the normalized error\r\ncriterion, but we have been able to prove this lack of strong tractability\r\nonly when certain conditions hold on the weights.  Finally, we use the\r\nKorobov- and min-kernels, along with product weights, to illustrate our\r\nresults. (pdf) High Level Synthesis for Packet Processing Pipelines Cristian Soviani 2007-10-30 Packet processing is an essential function of state-of-the-art network\r\nrouters and switches.  Implementing packet processors in pipelined\r\narchitectures is a well-known, established technique, albeit different\r\napproaches have been proposed.\r\n\r\nThe design of packet processing pipelines is a delicate trade-off\r\nbetween the desire for abstract specifications, short development time,\r\nand design maintainability on one hand and very aggressive performance\r\nrequirements on the other.\r\n\r\nThis thesis proposes a coherent design flow for packet processing\r\npipelines.  Like the design process itself, I start by introducing a\r\nnovel domain-specific language that provides a high-level\r\nspecification of the pipeline.  Next, I address synthesizing this\r\nmodel and calculating its worst-case throughput.  Finally, I address\r\nsome specific circuit optimization issues.\r\n\r\nI claim, based on experimental results, that my proposed technique can\r\ndramatically improve the design process of these pipelines, while the\r\nresulting performance matches the expectations of hand-crafted design.\r\n\r\nThe considered pipelines exhibit a pseudo-linear topology, which can\r\nbe too restrictive in the general case.  However, especially due to\r\nits high performance, such an architecture may be suitable for\r\napplications outside packet processing, in which case some of my\r\nproposed techniques could be easily adapted.\r\n\r\nSince I ran my experiments on FPGAs, this work has an inherent bias\r\ntowards that technology; however, most results are\r\ntechnology-independent. (pdf) (ps) Generalized Tractability for Multivariate Problems Michael Gnewuch, Henryk Wozniakowski 2007-10-30 \\usepackage{amssymb}\r\n\r\n\\begin{document}\r\nWe continue the study of generalized tractability initiated in our\r\nprevious paper ``Generalized tractability for multivariate problems,\r\nPart I: Linear tensor product problems and linear information'', J.\r\nComplexity, 23, 262-295 (2007).  We study linear tensor product\r\nproblems for which we can compute linear information which is given by\r\narbitrary continuous linear functionals. We want to approximate an\r\noperator $S_d$ given as the $d$-fold tensor product of a compact\r\nlinear operator $S_1$ for $d=1,2,\\dots\\,$, with $\\|S_1\\|=1$ and $S_1$\r\nhas at least two positive singular values.\r\n\r\nLet $n(\\varepsilon,S_d)$ be the minimal number of information\r\nevaluations needed to approximate $S_d$ to within\r\n$\\varepsilon\\in[0,1]$. We study \\emph{generalized tractability} by\r\nverifying when $n(\\varepsilon,S_d)$ can be bounded by a multiple of a\r\npower of $T(\\varepsilon^{-1},d)$ for all\r\n$(\\varepsilon^{-1},d)\\in\\Omega \\subseteq[1,\\infty)\\times \\mathbb{N}$.  Here,\r\n$T$ is a \\emph{tractability} function which is non-decreasing in both\r\nvariables and grows slower than exponentially to infinity.  We study\r\nthe \\emph{exponent of tractability} which is the smallest power of\r\n$T(\\varepsilon^{-1},d)$ whose multiple bounds $n(\\varepsilon,S_d)$.\r\nWe also study \\emph{weak tractability}, i.e., when\r\n$\\lim_{\\varepsilon^{-1}+d\\to\\infty,(\\varepsilon^{-1},d)\\in\\Omega}\r\n\\ln\\,n(\\varepsilon,S_d)/(\\varepsilon^{-1}+d)=0$.\r\n \r\nIn our previous paper, we studied generalized tractability for proper\r\nsubsets $\\Omega$ of $[1,\\infty)\\times\\mathbb{N}$, whereas in this paper we\r\ntake the unrestricted domain $\\Omega^{\\rm unr}=[1,\\infty)\\times\\mathbb{N}$.\r\n\r\nWe consider the three cases for which we have only finitely many\r\npositive singular values of $S_1$, or they decay exponentially or\r\npolynomially fast. Weak tractability holds for these three cases, and\r\nfor all linear tensor product problems for which the singular values\r\nof $S_1$ decay slightly faster that logarithmically.  We provide\r\nnecessary and sufficient conditions on the function~$T$ such that\r\ngeneralized tractability holds. These conditions are obtained in terms\r\nof the singular values of $S_1$ and mostly limiting properties of $T$.\r\nThe tractability conditions tell us how fast $T$ must go to infinity.\r\nIt is known that $T$ must go to infinity faster than polynomially. We\r\nshow that generalized tractability is obtained for\r\n$T(x,y)=x^{1+\\ln\\,y}$. We also study tractability functions $T$ of\r\nproduct form, $T(x,y) =f_1(x)f_2(x)$. Assume that\r\n$a_i=\\liminf_{x\\to\\infty}(\\ln\\,\\ln f_i(x))/(\\ln\\,\\ln\\,x)$ is finite\r\nfor $i=1,2$.  Then generalized tractability takes place iff\r\n$$a_i>1 \\ \\ \\mbox{and}\\ \\  (a_1-1)(a_2-1)\\ge1,$$\r\nand if $(a_1-1)(a_2-1)=1$ then we need to assume one more condition\r\ngiven in the paper. If $(a_1-1)(a_2-1)>1$ then the exponent of\r\ntractability is zero, and if $(a_1-1)(a_2-1)=1$ then the exponent of\r\ntractability is finite. It is interesting to add that for $T$ being of\r\nthe product form, the tractability conditions as well as the exponent\r\nof tractability depend only on the second singular eigenvalue of $S_1$\r\nand they do \\emph{not} depend on the rate of their decay.\r\n\r\nFinally, we compare the results obtained in this paper for the\r\nunrestricted domain $\\Omega^{\\rm unr}$ with the results from our\r\nprevious paper obtained for the restricted domain \r\n$\\Omega^{\\rm res}=\r\n[1,\\infty)\\times\\{1,2,\\dots,d^*\\}\\,\\cup\\,[1,\\varepsilon_0^{-1})\\times\\mathbb{N}$\r\nwith $d^*\\ge1$ and $\\varepsilon_0\\in(0,1)$. In general, the tractability \r\nresults are quite different. We may have generalized tractability\r\nfor the restricted domain and no generalized tractability for the\r\nunrestricted domain which is the case, for instance, \r\nfor polynomial tractability $T(x,y)=xy$. We may also have generalized\r\ntractability for both domains with different or with the same\r\nexponents of tractability. \r\n\\end{document} (pdf) Optimizing Frequency Queries for Data Mining Applications Hassan Malik, John Kender 2007-10-27 Data mining algorithms use various Trie and bitmap-based representations to optimize the support (i.e., frequency) counting performance. In this paper, we compare the memory requirements and support counting performance of FP Tree, and Compressed Patricia Trie against several novel variants of vertical bit vectors. First, borrowing ideas from the VLDB domain, we compress vertical bit vectors using WAH encoding. Second, we evaluate the Gray code rank-based transaction reordering scheme, and show that in practice, simple lexicographic ordering, obtained by applying LSB Radix sort, outperforms this scheme.  \r\n\r\nLed by these results, we propose HDO, a novel Hamming-distance-based greedy transaction reordering scheme, and aHDO, a linear-time approximation to HDO. We present results of experiments performed on 15 common datasets with varying degrees of sparseness, and show that HDO- reordered, WAH encoded bit vectors can take as little as 5% of the uncompressed space, while aHDO achieves similar compression on sparse datasets. Finally, with results from over a billion database and data mining style frequency query executions, we show that bitmap-based approaches result in up to hundreds of times faster support counting, and HDO-WAH encoded bitmaps offer the best space-time tradeoff. (pdf) Automated Social Hierarchy Detection through Email Network Analysis Ryan Rowe, German Creamer, Shlomo Heshkop, Sal Stolfo 2007-10-17 We present our work on automatically extracting social hierarchies\r\nfrom electronic communication data. Data mining based on user behavior\r\ncan be leveraged to analyze and catalog patterns of communications\r\nbetween entities to rank relationships. The advantage is that the\r\nanalysis can be done in an automatic fashion and can adopt itself to\r\norganizational changes over time. \r\n\r\nWe illustrate the algorithms over real world data using the Enron\r\ncorporation's email archive. The results show great promise when\r\ncompared to the corporations work chart and judicial proceeding\r\nanalyzing the major players. (pdf) A New Framework for Unsupervised Semantic Discovery Barry Schiffman 2007-10-16 This paper presents a new framework for the unsupervised discovery of semantic information, using a divide-and-conquer approach to take advantage of contextual regularities and to avoid problems of polysemy and sublanguages. Multiple sets of documents are formed and analyzed to create multiple sets of frames. The overall procedure is wholly unsupervised and domain independent. The end result will be a collection of sets of semantic frames that will be useful in a wide range of applications, including question-answering, information extraction, summarization and  text generation. (pdf) Towards In Vivo Testing of Software Applications Christian Murphy, Gail Kaiser, Matt Chu 2007-10-15 Software products released into the field typically have some number of residual bugs that either were not detected or could not have been detected during testing. This may be the result of flaws in the test cases themselves, assumptions made during the creation of test cases, or the infeasibility of testing the sheer number of possible configurations for a complex system. Testing approaches such as perpetual testing or continuous testing seek to continue to test these applications even after deployment, in hopes of finding any remaining flaws. In this paper, we present our initial work towards a testing methodology we call “in vivo testing”, in which unit tests are continuously executed inside a running application in the deployment environment. In this novel approach, unit tests execute within the current state of the program (rather than by creating a clean slate) without affecting or altering that state. Our approach has been shown to reveal defects both in the applications of interest and in the unit tests themselves. It can also be used for detecting concurrency or robustness issues that may not have appeared in a testing lab. Here we describe the approach, the testing framework we have developed for Java applications, classes of bugs our approach can discover, and the results of experiments to measure the added overhead. (pdf) Experiences in Teaching eXtreme Programming in a Distance Learning Program Christian Murphy, Dan Phung, Gail Kaiser 2007-10-12 As university-level distance learning programs become more and more popular, and software engineering courses incorporate eXtreme Programming (XP) into their curricula, certain challenges arise when teaching XP to students who are not physically co-located. In this paper, we present our experiences and observations from managing such an online software engineering course, and describe some of the specific challenges we faced, such as students’ aversion to using XP and difficulties in scheduling. We also present some suggestions to other educators who may face similar situations. (pdf) BARTER: Profile Model Exchange for Behavior-Based Access Control and Communication Security in MANETs Vanessa Frias-Martinez, Salvatore J. Stolfo, Angelos D. Keromytis 2007-10-10 There is a considerable body of literature and technology that\r\nprovides access control and security of communication for Mobile Ad-hoc Networks (MANETs) based on cryptographic authentication technologies\r\nand protocols. We introduce a new method of granting access and securing communication in a MANET environment to augment, not replace, existing techniques. Previous approaches grant access to the MANET, or to its services, merely by means of an authenticated identity or a qualified role. We present BARTER, a framework that, in addition, requires nodes to exchange a model of their behavior to grant access to the MANET\r\nand to assess the legitimacy of their subsequent communication. This framework forces the nodes not only to say who\r\nor what they are, but also how they behave. BARTER will continuously\r\nrun membership acceptance and update protocols to give access to and accept traffic only from nodes whose behavior model is considered ``normal'' according to the behavior model of the nodes in the MANET.\r\nWe implement and experimentally evaluate the merger between BARTER and other cryptographic technologies and show that BARTER can implement\r\na fully distributed automatic access control and update with\r\nsmall cryptographic costs. Although the methods proposed involve the use of content-based anomaly detection models, the generic infrastructure\r\nimplementing the methodology may utilize any behavior model.\r\nEven though the experiments are implemented for MANETs, the idea\r\nof model exchange for access control can be applied to any type of network. (pdf) Post-Patch Retraining for Host-Based Anomaly Detection Michael E. Locasto, Gabriela F. Cretu, Shlomo Hershkop, Angelos Stavrou 2007-10-05 Applying patches, although a disruptive activity, remains a vital part\r\nof software maintenance and defense. When host-based anomaly detection\r\n(AD) sensors monitor an application, patching the application requires\r\na corresponding update of the sensor's behavioral model.  Otherwise,\r\nthe sensor may incorrectly classify new behavior as malicious (a false\r\npositive) or assert that old, incorrect behavior is normal (a false\r\nnegative). Although the problem of ``model drift'' is an almost\r\nuniversally acknowledged hazard for AD sensors, relatively little work\r\nhas been done to understand the process of re-training a ``live'' AD\r\nmodel --- especially in response to legal behavioral updates like\r\nvendor patches or repairs produced by a self-healing system.\r\n\r\nWe investigate the feasibility of automatically deriving and applying\r\na ``model patch'' that describes the changes necessary to update a\r\n``reasonable'' host-based AD behavioral model ({\\it i.e.,} a model\r\nwhose structure follows the core design principles of existing\r\nhost--based anomaly models).  We aim to avoid extensive retraining and\r\nregeneration of the entire AD model when only parts may have changed\r\n--- a task that seems especially undesirable after the exhaustive\r\ntesting necessary to deploy a patch. (pdf) (ps) Privacy-Enhanced Searches Using Encrypted Bloom Filters Steven M. Bellovin, William R. Cheswick 2007-09-25 It is often necessary for two or more or more parties that do not\r\nfully trust each other to share data selectively.  For example,\r\none intelligence agency might be willing to turn over certain\r\ndocuments to another such agency, but only if the second agency\r\nrequests the specific documents.  The problem, of course, is finding\r\nout that such documents exist when access to the database is\r\nrestricted.\r\n\r\nWe propose a search scheme based on Bloom filters and group ciphers\r\nsuch as Pohlig-Hellman encryption.  A semi-trusted third party can\r\ntransform one party's search queries to a form suitable for querying\r\nthe other party's database, in such a way that neither the third\r\nparty nor the database owner can see the original query.  Furthermore,\r\nthe encryption keys used to construct the Bloom filters are not\r\nshared with this third party.  Multiple providers and queriers are\r\nsupported; provision can be made for third-party ``warrant servers'',\r\nas well as ``censorship sets'' that limit the data to be shared. (pdf) Service Composition in a Global Service Discovery System Knarig Arabshian, Christian Dickmann, Henning Schulzrinne 2007-09-17 GloServ is a global service discovery system which\r\naggregates information about different types of services in a\r\nglobally distributed network. GloServ classifies services in an\r\nontology and maps knowledge obtained by the ontology onto a\r\nscalable hybrid hierarchical peer-to-peer network. The network\r\nmirrors the semantic relationships of service classes and as a\r\nresult, reduces the number of message hops across the global\r\nnetwork due to the domain-specific way services are distributed.\r\nAlso, since services are described in greater detail, due to\r\nthe ontology representation, greater reasoning is applied when\r\nquerying and registering services. In this paper, we describe an\r\nenhancement to the GloServ querying mechanism which allows\r\nGloServ servers to process and issue subqueries between servers\r\nof different classes. Thus, information about different service\r\nclasses may be queried for in a single query and issued directly\r\nfrom the front end, creating an extensible platform for service\r\ncomposition. The results are then aggregated and presented to the\r\nuser such that services which share an attribute are categorized\r\ntogether. We have built and evaluated a location-based web\r\nservice discovery prototype which demonstrates the flexibility\r\nof service composition in GloServ and discuss the design and\r\nevaluation of this system. Keywords: service discovery, ontologies,\r\nOWL, CAN, peer-to-peer, web service composition (pdf) Using boosting for automated planning and trading systems German Creaer 2007-09-15 The problem: Much of finance theory is based on the efficient market hypothesis. According to this hypothesis, the prices of financial assets, such as stocks, incorporate all information that may affect their future performance. However, the translation of publicly\r\navailable information into predictions of future performance is far from trivial. Making such predictions is the livelihood of stock traders, market analysts, and the like. Clearly, the efficient market hypothesis is only an approximation which ignores the cost of producing accurate predictions. \r\n\r\nMarkets are becoming more efficient and more accessible because of the use of ever faster methods for communicating and analyzing financial data. Algorithms developed in machine learning can be used to automate parts of this translation process. In other words, we can now use machine learning algorithms to analyze vast amounts of information and compile them to predict the performance of companies, stocks, or even market analysts. In financial terms, we would say that such algorithms discover inefficiencies in the current market. These discoveries can be used to make a profit and, in turn, reduce the market inefficiencies or support strategic planning processes.\r\n\r\nRelevance: Currently, the major stock exchanges such as NYSE and NASDAQ are transforming their markets into electronic financial markets. Players in these markets must process large amounts of information and make instantaneous investment decisions. \r\n\r\nMachine learning techniques help investors and corporations recognize new business opportunities or potential corporate problems in these markets. With time, these techniques help the financial market become better regulated and more stable. Also, corporations could save significant amount of resources if they can automate certain corporate finance functions such as planning and trading.\r\n\r\nResults: This dissertation offers a novel approach to using boosting as a predictive and interpretative tool for problems in finance. Even more, we demonstrate how boosting can support the automation of strategic planning and trading functions. \r\n\r\nMany of the recent bankruptcy scandals in publicly held US companies such as Enron and WorldCom are inextricably linked to the conflict of interest between shareholders (principals) and managers (agents). We evaluate this conflict in the case of Latin American and US companies. In the first part of this dissertation, we use Adaboost to analyze the impact of corporate governance variables on performance. In this respect, we present an algorithm that calculates alternating decision trees (ADTs), ranks variables according to their level of importance, and generates representative ADTs. We develop a board Balanced Scorecard (BSC) based on these representative ADTs which is part of the process to automate the planning functions. \r\n\r\nIn the second part of this dissertation we present three main algorithms to improve forecasting and automated trading. First, we introduce a link mining algorithm using a mixture of economic and social network indicators to forecast earnings surprises, and cumulative abnormal return. Second, we propose a trading algorithm for short-term technical trading. The algorithm was tested in the context of the Penn-Lehman Automated Trading Project (PLAT) competition using the Microsoft stock. The algorithm was profitable during the competition. Third, we present a multi-stock automated trading system that includes a machine learning algorithm that makes the prediction, a weighting algorithm that combines the experts, and a risk management layer that selects only the strongest prediction and avoids trading when there is a history of negative performance. This algorithm was tested with 100 randomly selected S&P 500 stocks. We find that even an efficient learning algorithm, such as boosting, still requires powerful control mechanisms in order to reduce unnecessary and unprofitable trades that increase transaction costs. (pdf) Oblivious Image Matching Shai Avidan, Ariel Elbaz, Tal Malkin, Ryan Moriarty 2007-09-13 We present the problem of Oblivious Image Matching, where two parties want to determine whether they have images of the same object or scene, without revealing any additional information.  While image matching has attracted a great deal of attention in the computer vision community, it was never treated in a cryptographic sense.\r\n\r\nIn this paper we study the private version of the problem, oblivious image matching, and provide an efficient protocol for it.  In doing so, we design a novel image matching algorithm, and a few private protocols that may be of independent interest.  Specifically, we first show how to reduce the image matching problem to a two-level version of the fuzzy set matching problem, and then present a novel protocol to privately compute this (and several other) matching problems. (pdf) OpenTor: Anonymity as a Commodity Service Elli Androulaki, Mariana Raykova, Angelos Stavrou, Steven Bellovin 2007-09-13 Despite the growth of the Internet and the increasing concern for\r\nprivacy of online communications, current deployments of\r\nanonymization networks depends on a very small set of nodes that\r\nvolunteer their bandwidth. We believe that the main reason is not\r\ndisbelief in their ability to protect anonymity, but rather the\r\npractical limitations in bandwidth and latency that stem from\r\nlimited participation. This limited participation, in turn, is due\r\nto a lack of incentives. We propose providing economic incentives,\r\nwhich historically have worked very well.\r\n\r\nIn this technical report, we demonstrate a payment scheme that can\r\nbe used to compensate nodes which provide anonymity in Tor, an\r\nexisting onion routing, anonymizing network. We show that current\r\nanonymous payment schemes are not suitable and  introduce a hybrid\r\npayment system based on a combination of the Peppercoin Micropayment\r\nsystem and a new type of ``one use'' electronic cash. Our system\r\nclaims to maintain users' anonymity, although payment techniques\r\nmentioned previously --- when adopted individually --- provably\r\nfail. (pdf) Reputation Systems for Anonymous Networks Elli Androulaki, Seung Geol Choi, Steven M. Bellovin, Tal G. Malkin 2007-09-12 We present a reputation scheme for a pseudonymous peer-to-peer (P2P) system in an anonymous network.\r\nMisbehavior is one of the biggest problems in pseudonymous P2P systems, where there is little incentive for\r\nproper behavior. In our scheme, using ecash for reputation points, the reputation of each user is closely related to\r\nhis real identity rather than to his current pseudonym. Thus, our scheme allows an honest user to switch to a new\r\npseudonym keeping his good reputation, while hindering a malicious user from erasing his trail of evil deeds with\r\na new pseudonym. (pdf) A Study of Malcode-Bearing Documents Wei-Jen Li, Salvatore Stolfo, Angelos Stavrou, Elli Androulaki, Angelos D. Keromytis 2007-09-07 By exploiting the object-oriented dynamic composability of\r\nmodern document applications and formats, malcode hidden in otherwise\r\ninconspicuous documents can reach third-party applications that may\r\nharbor exploitable vulnerabilities otherwise unreachable by network-level service attacks. Such attacks can be very selective and difficult to detect compared to the typical network worm threat, owing to the complexity of these applications and data formats, as well as the multitude of document-exchange vectors. As a case study, this paper focuses on Microsoft Word documents as malcode carriers. We investigate the possibility of detecting embedded malcode in Word documents using two techniques: static content analysis using statistical models of typical document content, and run-time dynamic tests on diverse platforms. The experiments demonstrate these approaches can not only detect known malware, but also most zero-day attacks. We identify several problems with both approaches, representing both challenges in addressing the problem and opportunities for future research. (pdf) Backstop: A Tool for Debugging Runtime Errors Christian Murphy, Eunhee Kim, Gail Kaiser, Adam Cannon 2007-09-06 The errors that Java programmers are likely to encounter can roughly be categorized into three groups: compile-time (semantic and syntactic), logical, and runtime (exceptions). While much work has focused on the first two, there are very few tools that exist for interpreting the sometimes cryptic messages that result from runtime errors. Novice programmers in particular have difficulty dealing with uncaught exceptions in their code and the resulting stack traces, which are by no means easy to understand. We present Backstop, a tool for debugging runtime errors in Java applications. This tool provides more user-friendly error messages when an uncaught exception occurs, but also provides debugging support by allowing users to watch the execution of the program and the changes to the values of variables. We also present the results of two studies conducted on introductory-level programmers using the two different features of the tool. (pdf) RAS-Models: A Building Block for Self-Healing Benchmarks Rean Griffith, Ritika Virmani, Gail Kaiser 2007-09-01 To evaluate the efficacy of self-healing systems a rigorous, objective, quantitative benchmarking methodology is needed. However, developing such a benchmark is a non-trivial task given the many evaluation issues to be resolved, including but not limited to: quantifying the impacts\r\nof faults, analyzing various styles of healing (reactive, preventative, proactive), accounting for partially automated healing and accounting for incomplete/imperfect healing. We posit, however,that it is possible to realize a self-healing benchmark using a collection of analytical techniques and practical tools as building blocks. This paper highlights the flexibility of one analytical tool, the Reliability, Availability and Serviceability (RAS) model, and illustrates its power and relevance\r\nto the problem of evaluating self-healing mechanisms/systems, when combined with practical tools for fault-injection. (pdf) A Precomputed Polynomial Representation for Interactive BRDF Editing with Global Illumination Aner Ben-Artzi, Kevin Egan, Fredo Durand, Ravi Ramamoorthi 2007-07-31 The ability to interactively edit BRDFs in their final placement within a computer graphics scene is vital to making informed choices for material properties. We significantly extend previous work on BRDF editing for static scenes (with fixed lighting and view), by developing a precomputed polynomial representation that enables interactive BRDF editing with global illumination. Unlike previous recomputation based rendering techniques, the image is not linear in the BRDF when considering interreflections. We introduce a framework for precomputing a multi-bounce tensor of polynomial coefficients, that encapsulates the nonlinear nature of the task. Significant reductions in complexity are achieved by leveraging the low-frequency nature of indirect light. We use a high-quality representation for the BRDFs at the first bounce from the eye, and lower-frequency (often diffuse) versions for further bounces. This approximation correctly captures the general global illumination in a scene, including color-bleeding, near-field object reflections, and even caustics. We adapt Monte Carlo path tracing for precomputing the tensor of coefficients for BRDF basis functions. At runtime, the high-dimensional tensors can be reduced to a simple dot product at each pixel for rendering. We present a number of examples of editing BRDFs in complex scenes, with interactive feedback rendered with global illumination. (pdf) Parameterizing Random Test Data According to Equivalence Classes Christian Murphy, Gail Kaiser, Marta Arias 2007-07-12 We are concerned with the problem of detecting bugs in machine learning applications. In the absence of sufficient real-world data, creating suitably large data sets for testing can be a difficult task. Random testing is one solution, but may have limited effectiveness in cases in which a reliable test oracle does not exist, as is the case of the machine learning applications of interest. To address this problem, we have developed an approach to creating data sets called “parameterized random data generation”. Our data generation framework allows us to isolate or combine different equivalence classes as desired, and then randomly generate large data sets using the properties of those equivalence classes as parameters. This allows us to take advantage of randomness but still have control over test case selection at the system testing level. We present our findings from using the approach to test two different machine learning ranking applications. (pdf) The Delay-Friendliness of TCP Salman Abdul Baset, Eli Brosh, Vishal Misra, Dan Rubenstein, Henning Schulzrinne 2007-06-30 Traditionally, TCP has been considered unfriendly for real-time\r\napplications. Nonetheless, popular applications such as Skype use\r\nTCP due to the deployment of NATs and firewalls that prevent UDP\r\ntraffic. This observation motivated us to study the delay\r\nperformance of TCP for real-time media flows using an analytical\r\nmodel and experiments. The results obtained yield the working region\r\nfor VoIP and live video streaming applications and guidelines for\r\ndelay-friendly TCP settings. Further, our research indicates that\r\nsimple application-level schemes, such as packet splitting and\r\nparallel connections, can significantly improve the delay\r\nperformance of real-time TCP flows. (pdf) STAND: Sanitization Tool for ANomaly Detection Gabriela F. Cretu, Angelos Stavrou, Slavatore J. Stolfo, Angelos D. Keromytis 2007-05-30 The efficacy of Anomaly Detection (AD) sensors depends\r\nheavily on the quality of the data used to train them. Arti-\r\nficial or contrived training data may not provide a realistic\r\nview of the deployment environment. Most realistic data\r\nsets are dirty; that is, they contain a number of attacks\r\nor anomalous events. The size of these high-quality training\r\ndata sets makes manual removal or labeling of attack\r\ndata infeasible. As a result, sensors trained on this data can\r\nmiss attacks and their variations. We propose extending the\r\ntraining phase of AD sensors (in a manner agnostic to the\r\nunderlying AD algorithm) to include a sanitization phase.\r\nThis phase generates multiple models conditioned on small\r\nslices of the training data. We use these “micro-models”\r\nto produce provisional labels for each training input, and\r\nwe combine the micro-models in a voting scheme to determine\r\nwhich parts of the training data may represent attacks.\r\nOur results suggest that this phase automatically and significantly\r\nimproves the quality of unlabeled training data\r\nby making it as “attack-free” and “regular” as possible in\r\nthe absence of absolute ground truth. We also show how a\r\ncollaborative approach that combines models from different\r\nnetworks or domains can further refine the sanitization process\r\nto thwart targeted training or mimicry attacks against\r\na single site. (pdf) The Role of Reliability, Availability and Serviceability (RAS) Models in the Design and Evaluation of Self-Healing Systems Rean Griffith, Ritika Virmani, Gail Kaiser 2007-04-10 In an idealized scenario, self-healing systems predict,\r\nprevent or diagnose problems and take the appropriate actions\r\nto mitigate their impact with minimal human intervention.\r\nTo determine how close we are to reaching this goal\r\nwe require analytical techniques and practical approaches\r\nthat allow us to quantify the effectiveness of a system’s remediations\r\nmechanisms. In this paper we apply analytical\r\ntechniques based on Reliability, Availability and Serviceability\r\n(RAS) models to evaluate individual remediation\r\nmechanisms of select system components and their combined\r\neffects on the system. We demonstrate the applicability\r\nof RAS-models to the evaluation of self-healing systems\r\nby using them to analyze various styles of remediations (reactive,\r\npreventative etc.), quantify the impact of imperfect\r\nremediations, identify sub-optimal (less effective) remediations\r\nand quantify the combined effects of all the activated\r\nremediations on the system as a whole. (pdf) Aequitas: A Trusted P2P System for Paid Content Delivery Alex Sherman, Japinder Chawla, Jason Nieh, Cliff Stein, Justin Sarma 2007-03-30 P2P file-sharing has been recognized as a powerful and efficient\r\ndistribution model due to its ability to leverage users' upload\r\nbandwidth. However, companies\r\nthat sell digital content on-line are hesitant to rely on P2P \r\nmodels for paid content distribution due to the free file-sharing\r\ninherent in P2P models. \r\n\r\nIn this paper we present Aequitas, a P2P system in which users \r\nshare paid content anonymously via a layer of intermediate nodes.\r\nWe argue that with the extra anonymity in Aequitas, vendors\r\ncould leverage P2P bandwidth while effectively maintaining\r\nthe same level of trust towards their customers as in traditional \r\nmodels of paid content distribution. As a result, a \r\ncontent provider could reduce its infrastructure costs and \r\nsubsequently lower the costs for the end-users. \r\n\r\nThe intermediate nodes are \r\nincentivized to contribute their bandwidth via electronic micropayments. \r\nWe also introduce techniques that prevent the intermediate nodes\r\nfrom learning the content of the files they help transmit. \r\n\r\nIn this paper we present the design of our system, an analysis of its\r\nproperties and an implementation and experimental evaluation.  We\r\nquantify the value of the intermediate nodes, both in terms of\r\nefficiency and their effect on anonoymity.  We argue in support of the\r\neconomic and technological merits of the system. (pdf) Can P2P Replace Direct Download for Content Distribution Alex Sherman, Angelos Stavrou, Jason Nieh, Cliff Stein, Angelos Keromytis 2007-03-30 While peer-to-peer (P2P) file-sharing is a powerful and cost-effective\r\ncontent distribution model, most paid-for digital-content providers\r\n(CPs) rely on direct download to deliver their content. CPs such as\r\nApple iTunes that command a large base of paying users are hesitant to\r\nuse a P2P model that could easily degrade their user base into yet\r\nanother free file-sharing community.\r\n\r\nWe present TP2, a system that makes P2P file sharing a viable\r\ndelivery mechanism for paid digital content by providing the same\r\nsecurity properties as the currently used direct-download model.}\r\n introduces the novel notion of trusted auditors (TAs) -- P2P\r\npeers that are controlled by the system operator. TAs monitor the\r\nbehavior of other peers and help detect and prevent formation of\r\nillegal file-sharing clusters among the CP's user base.  TAs both\r\ncomplement and exploit the strong authentication and authorization\r\nmechanisms that are used in TP2 to control access to content.  It\r\nis important to note that TP2 does not attempt to solve the\r\nout-of-band file-sharing or DRM problems, which also exist in the\r\ndirect-download systems currently in use.\r\n\r\nWe analyze TP2 by modeling it as a novel game between misbehaving\r\nusers who try to form unauthorized file-sharing clusters and TAs who\r\ncurb the growth of such clusters.  Our analysis shows that a small\r\nfraction of TAs is sufficient to protect the P2P system against\r\nunauthorized file sharing.  In a system with as many as 60\\% of\r\nmisbehaving users, even a small fraction of TAs can detect 99\\% of\r\nunauthorized cluster formation.  We developed a simple economic model\r\nto show that even with such a large fraction of malicious nodes,\r\nTP2 can improve CP's profits (which could translate to user\r\nsavings) by 62 to 122\\%, even while assuming conservative estimates of\r\ncontent and bandwidth costs.  We implemented TP2 as a layer on top\r\nof BitTorrent and demonstrated experimentally using PlanetLab that our\r\nsystem provides trusted P2P file sharing with negligible performance\r\noverhead. (pdf) Policy Algebras for Hybrid Firewalls Hang Zhao, Steven M. Bellovin 2007-03-21 Firewalls are a effective means of protecting a local system or network of systems  from network-based security threats. In this paper, we propose a policy algebra  framework for security policy enforcement in hybrid firewalls, ones that exist both in the network and on end systems. To preserve the security semantics, the policy algebras  provide a formalism to compute addition, conjunction, subtraction, and summation on  rule sets; it also defines the cost and risk functions associated with policy enforcement.  Policy outsourcing triggers global cost minimization. We show that our framework can easily be extended to support packet filter firewall policies. Finally, we discuss special challenges and requirements for applying the policy algebra  framework to MANETs. (pdf) The PBS Policy: Some Properties and Their Proofs Hanhua Feng, Vishal Misra, Dan Rubenstein 2007-03-20 In this report we analyze a configurable blind scheduler \r\ncontaining a continuous, tunable parameter.\r\nAfter the definition of this policy, we prove the\r\nproperty of no surprising interruption, the property of no permanent\r\nstarvation, and two theorems about monotonicity of this policy.\r\n\r\nThis technical report contains supplemental materials for the following publication: Hanhua Feng, Vishal Misra, and Dan Rubenstein, \"PBS: A unified priority-based scheduler\", Proceedings of ACM SIGMETRICS '07, 2007. (pdf) (ps) An Approach to Software Testing of Machine Learning Applications Christian Murphy, Gail Kaiser, Marta Arias 2007-03-19 Some machine learning applications are intended to learn properties of data sets where the correct answers are not already known to human users. It is challenging to test such ML software, because there is no reliable test oracle. We describe a software testing approach aimed at addressing this problem. We present our findings from testing implementations of two different ML ranking algorithms: Support Vector Machines and MartiRank. (pdf) Design, Implementation, and Validation of a New Class of Interface Circuits for Latency-Insensitive Design Cheng-Hong Li, Rebecca Collins, Sampada Sonalkar, Luca P. Carloni 2007-03-05 With the arrival of nanometer technologies wire \r\ndelays are no longer negligible with respect to gate delays, and \r\ntiming-closure becomes a major challenge to System-on-Chip \r\ndesigners. Latency-insensitive design (LID) has been proposed as \r\na \"correct-by-construction\" design methodology to cope with this \r\nproblem. In this paper we present the design and implementation \r\nof a new and more efficient class of interface circuits to support \r\nLID. Our design offers substantial improvements in terms of logic \r\ndelay over the design originally proposed by Carloni et al. [1] as \r\nwell as in terms of both logic delay and processing throughput \r\nover the synchronous elastic architecture (SELF) recently proposed by Cortadella et al. [2]. These claims are supported by the \r\nexperimental results that we obtained completing semi-custom \r\nimplementations of the three designs with a 90nm industrial \r\nstandard-cell library. We also report on the formal verification \r\nof our design: using the NuSMV model checker we verified that \r\nthe RTL synthesizable implementations of our LID interface \r\ncircuits (relay stations and shells) are correct refinements of the corresponding abstract specifications according to the theory of \r\nLID [3]. (pdf) Evaluating Software Systems via Fault-Injection and Reliability, Availability and Serviceability (RAS) Metrics and Models Rean Griffith 2007-02-28 The most common and well-understood way to evaluate and compare computing systems is via performance-oriented benchmarks. However, numerous other demands are placed on computing systems besides speed. Current generation and next generation computing systems are expected\r\nto be reliable, highly available, easy to manage and able to repair faults and recover from failures with minimal human intervention.\r\nThe extra-functional requirements concerned with reliability, high availability, and serviceability (manageability, repair and recovery) represent an additional set of high-level goals the system is expected to meet or exceed. These goals govern the system’s operation and are codified using policies and service level agreements (SLAs).\r\nTo satisfy these extra-functional requirements, system-designers explore or employ a number of mechanisms geared towards improving the system’s reliability, availability and serviceability (RAS) characteristics. However, to evaluate these mechanisms and their impact, we need something more than performance metrics.\r\n\r\nPerformance-measures are suitable for studying the feasibility of the mechanisms i.e. they can be used to conclude that the level of performance delivered by the system with these mechanisms\r\nactive does not preclude its usage. However, performance numbers convey little about the efficacy of the systems RAS-enhancing mechanisms. Further, they do not allow us to analyze the (expected or actual) impact of individual mechanisms or make comparisons/discuss tradeoffs\r\nbetween mechanisms. \r\n\r\nWhat is needed is an evaluation methodology that is able to analyze the details of the RAS-enhancing mechanisms – the micro-view as well as the high-level goals, expressed as policies, SLAs etc., governing the system’s operation – the macro-view. Further, we must establish a link\r\nbetween the details of the mechanisms and their impact on the high-level goals. This thesis is concerned with developing the tools and applying analytical techniques to enable this kind of evaluation. We make three contributions.\r\n\r\nFirst, we contribute to a suite of runtime fault-injection tools with Kheiron. Kheiron demonstrates a feasible, low-overhead, transparent approach to performing system-adaptations in a variety of execution environments at runtime. We use Kheiron’s runtime-adaptation capability to inject faults into running programs. We present three implementations of Kheiron, each targeting a different execution environment. Kheiron/C manipulates compiled C-programs running in an unmanaged execution environment – comprised of the operating system and the underlying\r\nprocessor. Kheiron/CLR manipulates programs running in Microsoft’s Common Language Runtime (CLR) and Kheiron/JVM manipulates programs running in Sun Microsystems’ Java Virtual Machine (JVM). Kheiron’s operation is transparent to both the application and the execution\r\nenvironment. Further, the overheads imposed by Kheiron on the application and the execution environment are negligible, <5%, when no faults are being injected.\r\n\r\nSecond, we describe analytical techniques based on RAS-models, represented as Markov chains and Markov reward models, to demonstrate their power in evaluating RAS-mechanisms and their impact on the high-level goals governing system-operation. We demonstrate the flexibility of these models in evaluating reactive, proactive and preventative mechanisms as well as their ability to explore the feasibility of yet-to-be-implemented mechanisms. Our analytical techniques focus on remediations rather than observed mean time to failures (MTTF). Unlike hardware, where the laws of physics govern the failure rates of mechanical and electrical parts, there\r\nare no such guarantees for software failure rates. Software failure-rates can however be influenced using fault-injection, which we employ in our experiments. In our analysis we consider a number\r\nof facets of remediations, which include, but go beyond mean time to recovery (MTTR). For example we consider remediation success rates, the (expected) impact of preventative-maintenance and the degradation-impact of remediations in our efforts to establish a framework for reasoning\r\nabout the tradeoffs (the costs versus the benefits) of various remediation mechanisms.\r\n\r\nFinally, we distill our experiences developing runtime fault-injection tools, performing fault-injection experiments and constructing and analyzing RAS-models into a 7-step process for evaluating\r\ncomputing systems – the 7U-evaluation methodology. Our evaluation method succeeds in establishing the link between the details of the low-level mechanisms and the high-level goals governing the system’s operation. It also highlights the role of environmental constraints and\r\npolicies in establishing meaningful criteria for scoring and comparing these systems and their RAS-enhancing mechanisms. (pdf) Privacy-Preserving Distributed Event Corroboration Janak J. Parekh 2007-02-26 Event correlation is a widely-used data processing methodology for a broad variety of applications, and is especially useful in the context of distributed monitoring for software faults and vulnerabilities. However, most existing solutions have typically been focused on \"intra-organizational\" correlation; organizations typically employ privacy policies that prohibit the exchange of information outside of the organization. At the same time, the promise of \"inter-organizational\" correlation is significant given the broad availability of Internet-scale communications, and its potential role in both software fault maintenance and software vulnerability detection.\r\n\r\nIn this thesis, I present a framework for reconciling these opposing forces via the use of privacy preservation integrated into the event processing framework. I introduce the notion of event corroboration, a reduced yet flexible form of correlation that enables collaborative verification, without revealing sensitive information. By accommodating privacy policies, we enable the corroboration of data across different organizations without actually releasing sensitive information. The framework supports both source anonymity and data privacy, yet allows for temporal corroboration of a broad variety of data. The framework is designed as a lightweight collection of components to enable integration with existing COTS platforms and distributed systems. I also present an implementation of this framework: Worminator, a collaborative Intrusion Detection System, based on an earlier platform, XUES (XML Universal Event Service), an event processor used as part of a software monitoring platform called KX (Kinesthetics eXtreme).\r\n\r\nKX comprised a series of components, connected together with a publish-subscribe content-based routing event subsystem, for the autonomic software monitoring, reconfiguration, and repair of complex distributed systems. Sensors were installed in legacy systems; XUES' two modules then performed event processing on sensor data: information was collected and processed by the Event Packager, and correlated using the Event Distiller. While XUES itself was not privacy-preserving, it laid the groundwork for this thesis by supporting event typing, the use of publish-subscribe and extensibility support via pluggable event transformation modules. I also describe techniques by which corroboration and privacy preservation could optionally be \"retrofitted\" onto XUES without breaking the correlation applications and scenarios described.\r\n\r\nWorminator is a ground-up rewrite of the XUES platform to fully support privacy-preserving event types and algorithms in the context of a Collaborative Intrusion Detection System (CIDS), whereby sensor alerts can be exchanged and corroborated without revealing sensitive information about a contributor's network, services, or even external sources, as required by privacy policies. Worminator also fully anonymizes source information, allowing contributors to decide their preferred level of information disclosure. Worminator is implemented as a monitoring framework on top of a collection of non-collaborative COTS and in-house IDS sensors, and demonstrably enables the detection of not only worms but also \"broad and stealthy\" scans; traditional single-network sensors either bury such scans in large volumes or miss them entirely. Worminator supports corroboration for packet and flow headers (metadata), packet content, and even aggregate models of network traffic using a variety of techniques.\r\n\r\nThe contributions of this thesis include the development of a cross-application-domain event processing framework with native privacy-preserving types, the use and validation of privacy-preserving corroboration, and the establishment of a practical deployed collaborative security system. The thesis also quantifies Worminator's effectiveness at attack detection, the overhead of privacy preservation and the effectiveness of our approach against adversaries, be they \"honest-but-curious\" or actively malicious. (pdf) Distributed Algorithms for Secure Multipath Routing in Attack-Resistant Networks Patrick Pak-Ching Lee, Vishal Misra, Dan Rubenstein 2007-02-16 To proactively defend against intruders from readily jeopardizing single-path data sessions, we propose a {\\em distributed secure multipath solution} to route data across multiple paths so that intruders require much more resources to mount successful attacks. Our work exhibits several important properties that include: (1) routing decisions are made locally by network nodes without the centralized information of the entire network topology, (2) routing decisions minimize throughput loss under a single-link attack with respect to\r\ndifferent session models, and (3) routing decisions address multiple link attacks via lexicographic optimization. We devise two algorithms\r\ntermed the {\\em Bound-Control algorithm} and the {\\em Lex-Control algorithm}, both of which provide provably optimal solutions.  Experiments show that the Bound-Control algorithm is more effective to prevent the worst-case single-link attack when compared to the single-path approach, and that the Lex-Control algorithm further enhances the Bound-Control algorithm by countering severe single-link attacks and various types of multi-link attacks. Moreover, the Lex-Control algorithm offers prominent protection after only a few execution rounds, implying that we can sacrifice minimal routing\r\nprotection for significantly improved algorithm performance.  Finally, we examine the applicability of our proposed algorithms in a specialized\r\ndefensive network architecture called the attack-resistant network and analyze how the algorithms address resiliency and security in different network settings. (pdf) MutaGeneSys: Making Diagnostic Predictions Based on Genome-Wide Genotype Data in Association Studies Julia Stoyanovich, Itsik Pe'er 2007-02-16 Summary: We present MutaGeneSys: a system that uses genomewide\r\ngenotype data for disease prediction. Our system integrates\r\nthree data sources: the International HapMap project, whole-genome\r\nmarker correlation data and the Online Mendelian Inheritance in Man\r\n(OMIM) database. It accepts SNP data of individuals as query input\r\nand delivers disease susceptibility hypotheses even if the original set\r\nof typed SNPs is incomplete. Our system is scalable and flexible: it\r\noperates in real time and can be configured on the fly to produce\r\npopulation, technology, and confidence-specific predictions.\r\nAvailability: Efforts are underway to deploy our system as part of the\r\nNCBI Reference Assembly. Meanwhile, the system may be obtained\r\nfrom the authors.\r\nContact: jds1@cs.columbia.edu (pdf) Data Sanitization: Improving the Forensic Utility of Anomaly Detection Systems Gabriela F. Cretu, Angelos Stavrou, Salvatore J. Stolfo, Angelos D. Keromytis 2007-02-15 Anomaly Detection (AD) sensors have become an invaluable\r\ntool for forensic analysis and intrusion detection.\r\nUnfortunately, the detection performance of all\r\nlearning-based ADs depends heavily on the quality of the\r\ntraining data. In this paper, we extend the training phase\r\nof an AD to include a sanitization phase. This phase significantly\r\nimproves the quality of unlabeled training data\r\nby making them as ”attack-free” as possible in the absence\r\nof absolute ground truth. Our approach is agnostic\r\nto the underlying AD, boosting its performance based\r\nsolely on training-data sanitization. Our approach is to\r\ngenerate multiple AD models for content-based AD sensors\r\ntrained on small slices of the training data. These\r\nAD “micro-models” are used to test the training data,\r\nproducing alerts for each training input. We employ voting\r\ntechniques to determine which of these training items\r\nare likely attacks. Our preliminary results show that sanitization\r\nincreases 0-day attack detection while in most\r\ncases reducing the false positive rate. We analyze the performance\r\ngains when we deploy sanitized versus unsanitized\r\nAD systems in combination with expensive hostbased\r\nattack-detection systems. Finally, we show that\r\nour system incurs only an initial modest cost, which can\r\nbe amortized over time during online operation. (pdf) Accelerating Service Discovery in Ad-Hoc Zero Configuration Networking Se Gi Hong, Suman Srinivasan, Henning Schulzrinne 2007-02-12 Zero Configuration Networking (Zeroconf) assigns IP addresses and host names, and discovers service without a central server. Zeroconf can be used in wireless mobile ad-hoc networks which are based on IEEE 802.11 and IP. However, Zeroconf has problems in mobile ad-hoc networks as it cannot detect changes in the network topology. In highly mobile networks, Zeroconf causes network overhead while discovering new services. In this paper, we propose an algorithm to accelerate service discovery for mobile ad-hoc networks. Our algorithm involves the monitoring of network interface changes that occur when a device with IEEE 802.11 enabled joins a new network area. This algorithm allows users to discover network topology changes and new services in real-time while minimizing network overhead. (pdf) From STEM to SEAD: Speculative Execution for Automated Defense Michael Locasto, Angelos Stavrou, Gabriela F. Cretu, Angelos D. Keromytis 2007-02-10 Most computer defense systems crash the process that they protect as\r\npart of their response to an attack.  In contrast, self-healing\r\nsoftware recovers from an attack by automatically repairing the\r\nunderlying vulnerability. Although recent research explores the\r\nfeasibility of the basic concept, self-healing faces four major\r\nobstacles before it can protect legacy applications and COTS\r\nsoftware. Besides the practical issues involved in applying the system\r\nto such software (e.g., not modifying source code), self-healing\r\nhas encountered a number of problems: knowing when to engage, knowing\r\nhow to repair, and handling communication with external entities.\r\n\r\nOur previous work on a self-healing system, STEM, left these\r\nchallenges as future work.  STEM provides self-healing by\r\nspeculatively executing ``slices'' of a process. This paper improves\r\nSTEM's capabilities along three lines: (1) applicability of the\r\nsystem to COTS software (STEM does not require source code, and it\r\nimposes a roughly 73% performance penalty on Apache's normal\r\noperation), (2) semantic correctness of the repair (we introduce\r\nvirtual proxies and repair policy to assist the healing\r\nprocess), and (3) creating a behavior profile based on aspects of\r\ndata and control flow. (pdf) Topology-Based Optimization of Maximal Sustainable Throughput in a Latency-Insensitive System Rebecca Collins, Luca Carloni 2007-02-06 We consider the problem of optimizing the performance of a latency-insensitive system (LIS) where the addition of backpressure has caused throughput degradation.  Previous works have addressed the problem of LIS performance in different ways.  In particular, the insertion of relay stations and the sizing of the input queues in the shells are the two main optimization techniques that have been proposed.\r\nWe provide a unifying framework for this problem by outlining which approaches work for different system topologies, and highlighting counterexamples where some solutions do not work.  We also observe that in the most difficult class of topologies, instances with the greatest throughput degradation are typically very amenable to simplifications. The contributions of this paper include a characterization of topologies that maintain optimal throughput with fixed-size queues and a heuristic for sizing queues that produces solutions close to optimal in a fraction of the time. (pdf) On the infeasibility of Modeling Polymorphic Shellcode for Signature Detection Yingbo Song, Michael E. Locasto, Angelos Stavrou, Angelos D. Keromytis, Salvatore J. Stolfo 2007-02-04 POlymorphic malcode remains one of the most troubling threats for information security and intrusion defense systems. The ability for malcode to be automatically transformed into a semantically equivalent variant frustrates attemtps to construct a single, simple, easily verifiable representation. We present a quantitative analysis of the strentghs and limitations of shellcode polymorphism and consider the impact of this analysis on the current practices in intrusion detection.\r\n\r\nOur examination focuses on the nature of shellcode \"decoding routines\", and the empirical evidence we gather illustrate our mail result: that the challenge of modeling the class of self-modifying code is  likely intractable - even when the size of the instruction sequence (i.e. the decoder) is relatively small. We develop metrics to gauge the power of polymorphic engines and use them to provide insight into the strengths and weaknesses of some popular engines. We believe this analysis supplies a novel and useful way to understand the limitations of the current generation of signature-based techniques. We analyze some contemporary polymorphic techniques, explore ways to improve them in order to forecast the nature of future threats, and present our suggestions for countermeasures. Our resulsts indicate that the class of polymorphic behavior is too greatly spread and varied to model effectively. We conclude that modeling normal content is ulatimately a more promising defense mechanism than modeling malicious or abnormal content. (pdf) Combining Ontology Queries with Text Search in Service Discovery Knarig Arabshian, Henning Schulzrinne 2007-01-21 We present a querying mechanism for service discovery which combines\r\nontology queries with text search. The underlying service discovery\r\narchitecture used is GloServ. GloServ uses the Web Ontology Language\r\n(OWL) to classify services in an ontology and map knowledge obtained\r\nby the ontology onto a hierarchical peer-to-peer network. Initially,\r\nan ontology-based first order predicate logic query is issued in order\r\nto route the query to the appropriate server and to obtain exact and\r\nrelated service data.  Text search further enhances querying by\r\nallowing services to be described not only with ontology attributes,\r\nbut with plain text so that users can query for them using key\r\nwords. Currently, querying is limited to either simple attribute-value\r\npair searches, ontology queries or text search. Combining ontology\r\nqueries with text search enhances current service discovery\r\nmechanisms. (pdf) A Model for Automatically Repairing Execution Integrity Michael Locasto, Gabriela F. Cretu, Angelos Stavrou, Angelos D. Keromytis 2007-01-20 Many users value applications that continue execution in the face of\r\nattacks. Current software protection techniques typically abort a\r\nprocess after an intrusion attempt ({\\it e.g.}, a code injection\r\nattack). We explore ways in which the security property of integrity\r\ncan support availability. We extend the Clark-Wilson Integrity Model\r\nto provide primitives and rules for specifying and enforcing repair\r\nmechanisms and validation of those repairs. Users or administrators\r\ncan use this model to write or automatically synthesize \\emph{repair\r\npolicy}.  The policy can help customize an application's response to\r\nattack.  We describe two prototype implementations for transparently\r\napplying these policies without modifying source code. (pdf) Using Functional Independence Conditions to Optimize the Performance of Latency-Insensitive Systems Cheng-Hong Li, Luca Carloni 2007-01-11 In latency-insensitive design shell modules are used to encapsulate system components (pearls) in order to interface them with the given latency-insensitive protocol and dynamically control their operations. In particular, a shell stalls a pearl whenever new valid data are not available on its input channels. We study how functional independence \r\nconditions (FIC) can be applied to the performance optimization of a latency-insensitive system by avoiding unnecessary stalling of their pearls. We present a novel circuit design of a generic shell template that can exploit FICs. We also provide an automatic procedure for the logic synthesis of a shell instance that is only based on the particular local characteristics of its corresponding pearl and does not require any input from the designers. We conclude reporting on a set of experimental results that illustrate the beneits and overhead of the proposed technique. (pdf) Whitepaper: The Value of Improving the Separation of Concerns Marc Eaddy, Alan Cyment, Pierre van de Laar, Fabian Schmied, Wolfgang Schult 2007-01-09 Microsoft's enterprise customers are demanding better ways to modularize their software systems. They look to the Java community, where these needs are being met with language enhancements, improved developer tools and middleware, and better runtime support. We present a business case for why Microsoft should give priority to supporting better modularization techniques, also known as advanced separation of concerns (ASOC), for the .NET platform, and we provide a roadmap for how to do so. (pdf) An Implementation of a Renesas H8/300 Microprocessor with a  Cycle-Level Timing Extension Chen-Chun Huang, Javier Coca, Yashket Gupta, Stephen A. Edwards 2006-12-30 We describe an implementation of the Renesas H8/300 16-bit processor\r\nin VHDL suitable for synthesis on an FPGA.  We extended the ISA\r\nslightly to accomodate cycle-accurate timers accessible from the\r\ninstruction set, designed to provide more precise real-time control.\r\n\r\nWe describe the architecture of our implementation in detail, describe\r\nour testing strategy, and finally show how to built a cross compilation\r\ntoolchain under Linux. (pdf) Embedded uClinux, the Altera DE2, and the SHIM Compiler Wei-Chung Hsu, David Lariviere, Stephen A. Edwards 2006-12-28 SHIM is a concurrent deterministic language focused on embedded\r\nsystem. Although SHIM has undergone substantial evolution, it\r\ncurrently does not have a code generator for a true embedded\r\nenvironment.\r\n\r\nIn this project, we built an embedded environment that we intend to\r\nuse as a target for the SHIM compiler. We add the uClinux operating\r\nsystem between hardware devices and software programs.  Our long-term\r\ngoal is to have the SHIM compiler generate both user-space and\r\nkernel/module programs for this environment.  This project is a first\r\nstep: we manually explored what sort of code we ultimately want the\r\nSHIM compiler to produce.\r\n\r\nIn this report, we provide instructions on how to build and install\r\nuClinux into an Altera DE2 board and example programs, including a\r\nuser-space program, a kernel module, and a simple device driver for\r\nthe buttons on the DE2 board that includes an interrupt handler. (pdf) (ps) A JPEG Decoder in SHIM Nalini Vasudevan, Stephen A. Edwards 2006-12-25 Image compression plays an important role in multimedia systems,\r\ndigital systems, handheld systems and various other devices.\r\nEfficient image processing techniques are needed to make images\r\nsuitable for use in embedded systems.\r\n\r\nThis paper describes an implementation of a JPEG decoder in the SHIM\r\nprogramming language. SHIM is a software/hardware integration language\r\nwhose aim is to provide communication between hardware and software\r\nwhile providing deterministic concurrency.\r\n\r\nThe paper shows that a JPEG decoder is a good application and\r\nreasonable test case for the SHIM language and illustrates the ease\r\nwith which conventional sequential decoders can be modified to achieve\r\nconcurrency. (pdf) (ps) Arrays in SHIM: A Proposal Smridh Thapar, Olivier Tardieu, Stephen A. Edwards 2006-12-23 The use of multiprocessor configurations over uniprocessor is rapidly\r\nincreasing to exploit parallelism instead of frequency scaling for\r\nbetter compute capacity. The multiprocessor architectures being\r\ndeveloped will have a major impact on existing software. Current\r\nlanguages provide facilities for concurrent and distributed\r\nprogramming, but are prone to races and non-determinism. SHIM, a\r\ndeterministic concurrent language, guarantees the behavior of its\r\nprograms are independent of the scheduling of concurrent\r\noperations. The language currently supports atomic arrays only, i.e.,\r\nparts of arrays cannot be sent to concurrent processes for evaluation\r\n(and edition). In this report, we propose a way to add non-atomic\r\narrays to SHIM and describe the semantics that should be considered\r\nwhile allowing concurrent processes to edit parts of the same array. (pdf) (ps) High Quality, Efficient Hierarchical Document Clustering using Closed Interesting Itemsets Hassan Malik, John Kender 2006-12-18 High dimensionality remains a significant challenge for document clustering. Recent approaches used frequent itemsets and closed frequent itemsets to reduce dimensionality, and to improve the efficiency of hierarchical document clustering. In this paper, we introduce the notion of “closed interesting” itemsets (i.e. closed itemsets with high interestingness). We provide heuristics such as “super item” to efficiently mine these itemsets and show that they provide significant dimensionality reduction over closed frequent itemsets. \r\n\r\nUsing “closed interesting” itemsets, we propose a new hierarchical document clustering method that outperforms state of the art agglomerative, partitioning and frequent-itemset based methods both in terms of FScore and Entropy, without requiring dataset specific parameter tuning. We evaluate twenty interestingness measures on nine standard datasets and show that when used to generate “closed interesting” itemsets, and to select parent nodes, Mutual Information, Added Value, Yule’s Q and Chi-Square offers best clustering performance, regardless of the characteristics of underlying dataset. We also show that our method is more scalable, and results in better run-time performance as compare to leading approaches. On a dual processor machine, our method scaled sub-linearly and was able to cluster 200K documents in about 40 seconds. (pdf) LinkWidth: A Method to measure Link Capacity and Available Bandwidth using Single-End Probes Sambuddho Chakravarty, Angelos Stavrou, Angelos D. Keromytis 2006-12-15 We introduce LinkWidth, a method for estimating capacity and\r\navailable bandwidth using single-end controlled TCP packet probes.\r\nTo estimate capacity, we generate a train of TCP RST packets “sandwiched” between two TCP SYN packets. Capacity is obtained by\r\nend-to-end packet dispersion of the received TCP RST/ACK packets corresponding to the TCP SYN packets. Our technique is significantly different from the rest of the packet-pair-based measurement techniques, such as CapProbe, pathchar and pathrate, because the long packet trains minimize errors due to bursty cross-traffic. TCP RST packets do not generate additional ICMP replies preventing cross-traffic interference with our probes. In addition, we use TCP packets for all our probes to prevent some types of QoS-related traffic shaping from affecting our measurements. We extend the Train of Packet Pairs technique to approximate the available link capacity. We use pairs of TCP packets with variable intra-pair delays and sizes. This is the first attempt to implement this technique using single-end TCP probes, tested on a wide range of real networks with variable cross-traffic. We compare our prototype with pathchirp and pathload, which require control of both\r\nends, and demonstrate that in most cases our method gives approximately the same results. (pdf) Deriving Utility from a Self-Healing Benchmark Report Ritika Virmani, Rean Griffith, Gail Kaiser 2006-12-15 Autonomic systems, specifically self-healing systems, currently lack an objective and relevant methodology for their evaluation. Due to their focus on problem detection, diagnosis and remediation any evaluation methodology should facilitate an objective evaluation and/or comparison of these activities. Measures of “raw” performance are easily quantified and hence facilitate measurement and comparison on the basis of numbers. However, classifying a system better at problem detection, diagnosis and remediation purely on the basis of performance measures is not useful. The proposed evaluation methodology devised will differ from traditional benchmarks, which are primarily concerned with measures of performance. In order to develop this methodology we rely on a set of experiments which will enable us to compare the self-healing capabilities of one system versus another. As currently we do not have available “real” self-healing systems, we will simulate the behavior of some target self-healing systems, system faults and the operational and repair activities of target systems. Further, we will use the results derived from the simulation experiments to answer questions relevant to the utility of a benchmark report. (pdf) Measurements of DNS Stability Omer Boyaci, Henning Schulzrinne 2006-12-14 In this project, we measured the stability of DNS servers based on the most popular 500 domains. In the first part of the project, DNS server replica counts and maximum DNS server separation are found for each domain. In the second part, these domains are queried for a one-month period in order to find their uptime percentages. (pdf) Cooperation Between Stations in Wireless Networks Andrea G. Forte, Henning Schulzrinne 2006-12-07 In a wireless network, mobile nodes (MNs) repeatedly\r\nperform tasks such as layer 2 (L2) handoff, layer 3 (L3)\r\nhandoff and authentication. These tasks are critical, particularly\r\nfor real-time applications such as VoIP. We propose a novel\r\napproach, namely Cooperative Roaming (CR), in which MNs\r\ncan collaborate with each other and share useful information\r\nabout the network in which they move.\r\nWe show how we can achieve seamless L2 and L3 handoffs\r\nregardless of the authentication mechanism used and without any\r\nchanges to either the infrastructure or the protocol. In particular,\r\nwe provide a working implementation of CR and show how, with\r\nCR, MNs can achieve a total L2+L3 handoff time of less than\r\n16 ms in an open network and of about 21 ms in an IEEE 802.11i\r\nnetwork. We consider behaviors typical of IEEE 802.11 networks,\r\nalthough many of the concepts and problems addressed here\r\napply to any kind of mobile network. (pdf) Throughput and Fairness in CSMA/CA Wireless Networks Hoon Chang, Vishal Misra, Dan Rubenstein 2006-12-07 While physical layer capture has been observed in real implementations of wireless devices accessing the channel like 802.11, log-utility fair allocation algorithms based on accurate channel models describing the phenomenon have not been developed. In this paper, using a general physical channel model, we develop an allocation algorithm for log-utility fairness. To maximize the aggregate utility, our algorithm determines channel access attempt probabilities of nodes using partial derivatives of the utility. Our algorithm is verified through extended simulations. The results indicate that our algorithm could quickly\r\nachieve allocations close to the optimum with 8.6% accuracy error on average. (pdf) A Case for P2P Delivery of Paid Content Alex Sherman, Angelos Stavrou, Jason Nieh, Cliff Stein, Angelos D. Keromytis 2006-11-28 P2P file sharing provides a powerful content distribution model\r\nby leveraging users' computing and bandwidth resources.  However,\r\ncompanies have been reluctant to rely on P2P systems for paid content\r\ndistribution due to their inability to limit the exploitation of these\r\nsystems for free file sharing.  We present \\sname, a system that\r\ncombines the more cost-effective and scalable distribution capabilities of\r\nP2P systems with a level of trust and control over content\r\ndistribution similar to direct download content delivery networks.  \\sname\\\r\nuses two key mechanisms that can be layered on top of existing P2P\r\nsystems.  First, it provides strong authentication to prevent free\r\nfile sharing in the system.  Second, it introduces a new notion of trusted\r\nauditors to detect and limit malicious attempts to gain information\r\nabout participants in the system to facilitate additional out-of-band\r\nfree file sharing.  We analyze \\sname\\ by modeling it as a novel game\r\nbetween malicious users who try to form free file sharing clusters\r\nand trusted auditors who curb the growth of such clusters.  Our analysis\r\nshows that a small fraction of trusted auditors\r\nis sufficient to protect the P2P system against unauthorized file\r\nsharing.  Using a simple economic model, we further show that\r\n\\sname\\ provides a more cost-effective content distribution solution,\r\nresulting in higher profits for a content provider even in the\r\npresence of a large percentage of malicious users.  Finally, we\r\nimplemented \\sname\\ on top of BitTorrent and use PlanetLab to show\r\nthat our system can provide trusted P2P f (pdf) Presence Traffic Optimization Techniques Vishal Singh, Henning Schulzrinne, Markus Isomaki, Piotr Boni 2006-11-02 With the growth of presence-based services, it is important to provision the network to support high traffic and load generated by presence services. Presence event distribution systems amplify a single incoming PUBLISH message into possibly numerous outgoing NOTIFY messages from the server. This can increase the network load on inter-domain links and can potentially disrupt other QoS-sensitive applications. In this document, we present existing as well as new techniques that can be used to reduce presence traffic both in inter-domain and intra-domain scenarios. Specifically, we propose two new techniques: sending common NOTIFY for multiple watchers and batched notifications. We also propose some generic heuristics that can be used to reduce network traffic due to presence. (pdf) A Common Protocol for Implementing Various DHT Algorithms Salman Abdul Baset, Henning Schulzrinne, Eunsoo Shim 2006-10-22 This document defines DHT-independent and DHT-dependent features of DHT algorithms and presents a comparison of Chord, Pastry and Kademlia. It then describes key DHT operations and their information requirements. (pdf) A survey on service creation by end users Xiaotao Wu, Henning Schulzrinne 2006-10-15 We conducted a survey on end users’ willingness and capability to create their desired communication services. The survey is based on the graphical service creation tool we implemented for the Language for End System Services (LESS). We call the tool CUTE, which stands for Columbia University Telecommunication service Editor. This report demonstrates our survey result and shows that relatively inexperienced users are willing and capable to create their desired communication services, and CUTE fits their needs. (pdf) A VoIP Privacy Mechanism and its Application in VoIP Peering for Voice Service Provider Topology and Identity Hiding Charles Shen, Henning Schulzrinne 2006-10-03 Voice Service Providers (VSPs) participating in VoIP peering frequently want to withhold their\r\nidentity and related privacy-sensitive information from other parties during the VoIP communication.\r\nA number of existing documents on VoIP privacy exist, but most of them focus on end user privacy.\r\nBy summarizing and extending existing work, we present a unified privacy mechanism for both VoIP\r\nusers and service providers. We also show a case study on how VSPs can use this mechanism for\r\nidentity and topology hiding in VoIP peering. (pdf) Evaluation and Comparison of BIND, PDNS and Navitas as ENUM Server Charles Shen, Henning Schulzrinne 2006-09-27 ENUM is a protocol standard developed by the Internet Engineering Task\r\nForce (IETF) for translating the E.164 phone numbers into Internet\r\nUniversal Resource Identifiers (URIs). It plays an increasingly\r\nimportant role as the bridge between Internet and traditional\r\ntelecommunications services. ENUM is based on the Domain Name System\r\n(DNS), but places unique performance requirements on DNS server. In\r\nparticular, ENUM server needs to host a huge number of records,\r\nprovide high query throughput for both existing and non-existing\r\nrecords in the server, maintain high query performance under update\r\nload, and answer queries within a tight latency budget. In this\r\nreport, we evaluate and compare performance of serving ENUM queries by\r\nthree servers, namely BIND, PDNS and Navitas. Our objective is to\r\nanswer whether and how these servers can meet the unique performance\r\nrequirements of ENUM.  Test results show that the ENUM query response\r\ntime on our platform has always been on the order of a few\r\nmilliseconds or less, so this is likely not a concern. Throughput then\r\nbecomes the key.  The throughput of BIND degrades linearly as the\r\nrecord set size grows, so BIND is not suitable for ENUM. PDNS delivers\r\nhigher performance than BIND in most cases, while the commercial\r\nNavitas server presents even better ENUM performance than PDNS. Under\r\nour 5M-record set test, Navitas server with its default configuration\r\nconsumes one tenth to one sixth the memory of PDNS, achieves six times\r\nhigher throughput for existing records and an order of two magnitudes\r\nhigher throughput for non-existing records than the bottom line PDNS\r\nserver without caching.  The throughput of Navitas is also the highest\r\namong the tested servers when the database is being updated in the\r\nbackground. We investigated ways to improve PDNS performance. For\r\nexample, doubling CPU processing power by putting PDNS and its backend\r\ndatabase in two separate machines can increase PDNS throughput for\r\nexisting records by 45% and that for nonexisting records by 40%. Since\r\nPDNS is open source, we also instrumented the source code to obtain a\r\ndetailed profile of contributions of various systems components to the\r\noverall latency.  We found that when the server is within its normal\r\nload range, the main component of server processing latency is caused\r\nby backend database lookup operations.  Excessive number of backend\r\ndatabase lookups is the reason that makes PDNS throughput for\r\nnon-existing records its key weakness. We studied using PDNS caching\r\nto reduce the number of database lookups. With a full packet cache and\r\na modified cache maintenance mechanism, the PDNS throughput for\r\nexisting records can be improved by 100%. This brings the value to one\r\nthird of its Navitas counterpart. After enabling the PDNS negative\r\nquery cache, we improved PDNS throughput for non-existing records to\r\nthe level comparable to its throughput for existing records, but this\r\nresult is still an order of magnitude lower than the corresponding\r\nvalue in Navitas. Further improvements of PDNS throughput for\r\nnon-existing records will require optimization of related processing\r\nmechanism in its implementation. (pdf) Specifying Confluent Processes Olivier Tardieu, Stephen A. Edwards 2006-09-22 We address the problem of specifying concurrent processes that can make local\r\nnondeterministic decisions without affecting global system\r\nbehavior---the sequence of events communicated along each\r\ninter-process communication channel.  Such nondeterminism can be used\r\nto cope with unpredictable execution rates and communication delays.\r\n\r\nOur model\r\nresembles Kahn's, but does not include unbounded buffered\r\ncommunication, so it is much simpler to reason about and implement.\r\nAfter formally characterizing these so-called confluent processes, we\r\npropose a collection of operators, including sequencing, parallel, and\r\nour own creation, confluent choice, that guarantee confluence by\r\nconstruction.\r\n\r\nThe result is a set of primitive constructs that form the formal basis\r\nof a concurrent programming language for both hardware and software\r\nsystems that gives deterministic behavior regardless of the relative\r\nexecution rates of the processes.  Such a language greatly simplifies\r\nthe verification task because any correct implementation of such a\r\nsystem is guaranteed to have the same behavior, a property rarely\r\nfound in concurrent programming environments. (pdf) (ps) MacShim: Compiling MATLAB to a Scheduling-Independent Concurrent Language Neesha Subramaniam, Ohan Oda, Stephen A. Edwards 2006-09-22 Nondeterminism is a central challenge in most concurrent models of\r\ncomputation.  That programmers must worry about races and other\r\ntiming-dependent behavior is a key reason that parallel programming\r\nhas not been widely adopted.  The SHIM concurrent language,\r\nintended for hardware/software codesign applications, avoids this\r\nproblem by providing deterministic (race-free) concurrency, but does\r\nnot support automatic parallelization of sequential algorithms.\r\n\r\nIn this paper, we present a compiler able to parallelize a simple\r\nMATLAB-like language into concurrent SHIM processes.  From a\r\nuser-provided partitioning of arrays to processes, our compiler\r\ndivides the program into coarse-grained processes and schedules and\r\nsynthesizes inter-process communication.  We demonstrate the\r\neffectiveness of our approach on some image-processing algorithms. (pdf) (ps) SHIM: A Deterministic Approach to Programming with Threads Olivier Tardieu, Stephen A. Edwards 2006-09-21 Concurrent programming languages should be a good fit for embedded\r\nsystems because they match the intrinsic parallelism of their\r\narchitectures and environments.  Unfortunately, most concurrent\r\nprogramming formalisms are prone to races and nondeterminism, despite\r\nthe presence of mechanisms such as monitors.\r\n\r\nIn this paper, we propose SHIM, the core of a concurrent language with\r\ndisciplined shared variables that remains deterministic, meaning the\r\nbehavior of a program is independent of the scheduling of concurrent\r\noperations. SHIM does not sacrifice power or flexibility to achieve\r\nthis determinism. It supports both synchronous and asynchronous\r\nparadigms---loosely and tightly synchronized threads---the dynamic\r\ncreation of threads and shared variables, recursive procedures, and\r\nexceptions.\r\n\r\nWe illustrate our programming model with examples including\r\nbreadth-first-search algorithms and pipelines. By construction, they\r\nare race-free. We provide the formal semantics of SHIM and a\r\npreliminary implementation. (pdf) (ps) Debugging Woven Code Marc Eaddy, Alfred Aho, Weiping Hu, Paddy McDonald, Julian Burger 2006-09-20 The ability to debug woven programs is critical to the adoption of Aspect Oriented Programming (AOP).  Nevertheless, many AOP systems lack adequate support for debugging, making it difficult to diagnose faults and understand the program's structure and control flow.  We discuss why debugging aspect behavior is hard and how harvesting results from related research on debugging optimized code can make the problem more tractable.  We also specify general debugging criteria that we feel all AOP systems should support.\r\n\r\nWe present a novel solution to the problem of debugging aspect-enabled programs.  Our Wicca system is the first dynamic AOP system to support full source-level debugging of woven code.  It introduces a new weaving strategy that combines source weaving with online byte-code patching.  Changes to the aspect rules, or base or aspect source code are rewoven and recompiled on-the-fly.  We present the results of an experiment that show how these features provide the programmer with a powerful interactive debugging experience with relatively little overhead. (pdf) A Framework for Quality Assurance of Machine Learning Applications Christian Murphy, Gail Kaiser, Marta Arias 2006-09-15 Some machine learning applications are intended to learn properties\r\nof data sets where the correct answers are not already known to\r\nhuman users. It is challenging to test and debug such ML software,\r\nbecause there is no reliable test oracle. We describe a framework\r\nand collection of tools aimed to assist with this problem. We\r\npresent our findings from using the testing framework with three\r\nimplementations of an ML ranking algorithm (all of which had bugs). (pdf) Throughput and Fairness in Random Access Networks Hoon Chang, Vishal Misra, Dan Rubenstein 2006-08-24 This paper present an throughput analysis of log-utility and max-min fairness. Assuming all nodes interfere with each other, completely or partially, log-utility fairness significantly enhances the total throughput compared to max-min fairness since the nodes should have the same throughput in max-min fairness. The improvement is enlarged especially when the effect of cumulated interference from multiple senders cannot be ignored. (pdf) Linear Approximation of Optimal Attempt Rate in Random Access Networks HOON CHANG, VISHAL MISRA, DAN RUBENSTEIN 2006-08-01 While packet capture has been observed in real implementations of wireless devices randomly accessing shared channels, fair rate control algorithms based on accurate channel models that describe the phenomenon have not been developed. In this paper, using a general physical channel model, we develop the equation for the optimal attemp rate to maximize the aggregate log utility. We use the least squares method to approximate the equation to a linear function of the attempt rate. Our analysis on the approximation error shows that the linear function obtained is close enough to the original with the square of the residuals more than 0.9. (pdf) Complexity and tractability of the heat equation Arthur G. Werschulz 2006-07-27 In a previous paper, we developed a general framework for establishing\r\ntractability and strong tractability for quasilinear multivariate\r\nproblems in the worst case setting.  One important example of such a\r\nproblem is the solution of the heat equation $u_t = \\Delta u - qu$ in\r\n$I^d\\times(0,T)$, where $I$ is the unit interval and $T$ is a maximum\r\ntime value.  This problem is to be solved subject to homogeneous\r\nDirichlet boundary conditions, along with the initial conditions\r\n$u(\\cdot,0)=f$ over~$I^d$.  The solution~$u$ depends linearly on~$f$,\r\nbut nonlinearly on~$q$.  Here, both $f$ and~$q$ are $d$-variate\r\nfunctions from a reproducing kernel Hilbert space with finite-order\r\nweights of order~$\\omega$.  This means that, although~$d$ can be\r\narbitrary large, $f$ and~$q$ can be decomposed as sums of functions of\r\nat most $\\omega$~variables, with $\\omega$ independent of~$d$.\r\n\r\nIn this paper, we apply our previous general results to the heat\r\nequation.  We study both the absolute and normalized error criteria.\r\nFor either error criterion, we show that the problem is \\emph.  That\r\nis, the number of evaluations of $f$ and~$q$ needed to obtain an\r\n$\\varepsilon$-approximation is polynomial in~$\\varepsilon$ and~$d$,\r\nwith the degree of the polynomial depending linearly on~$\\omega$.  In\r\naddition, we want to know when the problem is \\emph{strongly\r\ntractable}, meaning that the dependence is polynomial only\r\nin~$\\varepsilon$, independently of~$d$.  We show that if the sum of\r\nthe weights defining the weighted reproducing kernel Hilbert space is\r\nuniformly bounded in~$d$ and the integral of the univariate kernel is\r\npositive, then the heat equation is strongly tractable. (pdf) Projection Volumetric Display using Passive Optical Scatterers Shree K. Nayar, Vijay N. Anand 2006-07-25 In this paper, we present a new class of volumetric displays that can\r\nbe used to display 3D objects. The basic approach is to trade-off the\r\nspatial resolution of a digital projector (or any light engine) to gain\r\nresolution in the third dimension. Rather than projecting an image\r\nonto a 2D screen, a depth-coded image is projected onto a 3D cloud\r\nof passive optical scatterers. The 3D point cloud is realized using a\r\ntechnique called Laser Induced Damage (LID), where each scatterer\r\nis a physical crack embedded in a block of glass or plastic. We show\r\nthat when the point cloud is randomized in a specific manner, a very\r\nlarge fraction of the points are visible to the viewer irrespective of\r\nhis/her viewing direction. We have developed an orthographic projection\r\nsystem that serves as the light engine for our volumetric displays.\r\nWe have implemented several types of point clouds, each one\r\ndesigned to display a specific class of objects. These include a cloud\r\nwith uniquely indexable points for the display of true 3D objects, a\r\ncloud with an independently indexable top layer and a dense extrusion\r\nvolume to display extruded objects with arbitrarily textured top\r\nplanes and a dense cloud for the display of purely extruded objects.\r\nIn addition, we show how our approach can be used to extend simple\r\nvideo games to 3D. Finally, we have developed a 3D avatar in which\r\nvideos of a face with expression changes are projected onto a static\r\nsurface point cloud of the face. (pdf) Practical Preference Relations for Large Data Sets Kenneth Ross, Peter Stuckey, Amelie Marian 2006-06-16 User-defined preferences allow personalized ranking of query results.\r\nA user provides a declarative specification of his/her preferences, and\r\nthe system is expected to use that specification to give more\r\nprominence to preferred answers.  We study constraint formalisms for\r\nexpressing user preferences as base facts in a partial order.  We\r\nconsider a language that allows comparison and a limited form of\r\narithmetic, and show that the transitive closure computation required\r\nto complete the partial order terminates.  We consider various ways of\r\ncomposing partial orders from smaller pieces, and provide results on\r\nthe size of the resulting transitive closures.  We introduce the\r\nnotion of ``covering composition,'' which solves some semantic problems\r\napparent in previous notions of composition.  Finally, we show how\r\npreference queries within our language can be supported by suitable\r\nindex structures for efficient evaluation over large data sets.  Our\r\nresults provide guidance about when complex preferences can be\r\nefficiently evaluated, and when they cannot. (pdf) Feasibility of Voice over IP on the Internet Alex Sherman, Jason Nieh, Yoav Freund 2006-06-09 VoIP (Voice over IP) services are using the Internet infrastructure to\r\nenable new forms of communication and collaboration.  A growing number\r\nof VoIP service providers such as Skype, Vonage, Broadvoice, as well\r\nas many cable services are using the Internet to offer telephone\r\nservices at much lower costs.  However, VoIP services rely on the\r\nuser's Internet connection, and this can often translate into lower\r\nquality communication.  Overlay networks offer a potential solution to\r\nthis problem by improving the default Internet routing and overcome\r\nfailures.\r\n\r\nTo assess the feasibility of using overlays to improve VoIP\r\non the Internet, we have conducted a detailed experimental study to\r\nevaluate the benefits of using an overlay on PlanetLab nodes for\r\nimproving voice communication connectivity and performance around the\r\nworld.  Our measurements demonstrate that an overlay architecture\r\ncan significantly improve VoIP communication across most regions\r\nand provide their greatest benefit for locations with poorer default\r\nInternet connectivity.  We explore overlay topologies and show that a\r\nsmall number of well-connected intermediate nodes is sufficient to\r\nimprove VoIP performance.  We show that there is significant\r\nvariation over time in the best overlay routing paths and argue for\r\nthe need for adaptive routing to account for this variation to deliver\r\nthe best performance. (pdf) Exploiting Temporal Coherence for Pre-computation Based Rendering Ryan Overbeck 2006-05-23 Precomputed radiance transfer (PRT) generates impressive images with complex illumi-\r\nnation, materials and shadows with real-time interactivity. These methods separate the\r\nscene’s static and dynamic components allowing the static portion to be computed as a\r\npreprocess. In this work, we hold geometry static and allow either the lighting or BRDF\r\nto be dynamic. To achieve real-time performance, both static and dynamic components\r\nare compressed by exploiting spatial and angular coherence. Temporal coherence of the\r\ndynamic component from frame to frame is an important, but unexplored additional form\r\nof coherence. In this thesis, we explore temporal coherence of two forms of all-frequency\r\nPRT: BRDF material editing and lighting design. We develop incremental methods for\r\napproximating the differences in the dynamic component between consecutive frames. For\r\nBRDF editing, we find that a pure incremental approach allows quick convergence to an\r\nexact solution with smooth real-time response.\r\nFor relighting, we observe vastly differing degrees of temporal coherence accross levels of\r\nthe lighting’s wavelet hierarchy. To address this, we develop an algorithm that treats each\r\nlevel separately, adapting to available coherence. The proposed methods are othogonal to\r\nother forms of coherence, and can be added to almost any PRT algorithm with minimal\r\nimplementation, computation, or memory overhead. We demonstrate our technique within\r\nexisting codes for nonlinear wavelet approximation, changing view with BRDF factorization,\r\nand clustered PCA. Exploiting temporal coherence of dynamic lighting yields a 3×–4× per-\r\nformance improvement, e.g., all-frequency effects are achieved with 30 wavelet coefficients,\r\nabout the same as low-frequency spherical harmonic methods. Distinctly, our algorithm\r\nsmoothly converges to the exact result within a few frames of the lighting becoming static. (pdf) Speculative Execution as an Operating System Service Michael Locasto, Angelos Keromytis 2006-05-12 Software faults and vulnerabilities continue to present significant\r\nobstacles to achieving reliable and secure software. In an effort to\r\novercome these obstacles, systems often incorporate self-monitoring\r\nand self-healing functionality. Our hypothesis is that internal\r\nmonitoring is not an effective long-term strategy.  However,\r\nmonitoring mechanisms that are completely external lose the advantage\r\nof application-specific knowledge available to an inline monitor. To\r\nbalance these tradeoffs, we present the design of VxF, an environment\r\nwhere both supervision and automatic remediation can take place by\r\nspeculatively executing \"slices\" of an application. VxF introduces\r\nthe concept of an endolithic kernel by providing execution as\r\nan operating system service: execution of a process slice takes place\r\ninside a kernel thread rather than directly on the system\r\nmicroprocessor. (pdf) Privacy-Preserving Payload-Based Correlation for Accurate Malicious Traffic Detection Janak Parekh, Ke Wang, Salvatore Stolfo 2006-05-09 With the increased use of botnets and other techniques to obfuscate attackers' command-and-control centers, Distributed Intrusion Detection Systems (DIDS)  that focus on attack source IP addresses or other header information can only portray a limited view of distributed scans and attacks. Packet payload sharing techniques hold far more promise, as they can convey exploit vectors and/or malcode used upon successful exploit of a target system, irrespective of obfuscated source addresses.  However, payload sharing has had minimal success due to regulatory or business-based privacy concerns of transmitting raw or even sanitized payloads. The currently accepted form of content exchange has been limited to the exchange of known-suspicious content, e.g., packets captured by honeypots; however, signature generation assumes that each site receives enough traffic in order to correlate a meaningful set of payloads from which common content can be derived, and places fundamental and computationally stressful requirements on signature generators that may miss particularly stealthy or carefully-crafted polymorphic malcode.\r\n\r\nInstead, we propose a new approach to enable the sharing of suspicious payloads via privacy-preserving technologies. We detail the work we have done with two example payload anomaly detectors, PAYL and Anagram, to support generalized payload correlation and signature generation without releasing identifiable payload data and without relying on single-site signature generation. We present preliminary results of our approaches and suggest how such deployments may practically be used for not only cross-site, but also cross-domain alert sharing and its implications for profiling threats. (pdf) PBS: A Unified Priority-Based CPU Scheduler Hanhua Feng, Vishal Misra, Dan Rubenstein 2006-05-01 A novel CPU scheduling policy is designed and implemented. It is a configurable policy in the sense that a tunable parameter is provided to change its behavior. With different settings of the parameter, this policy  can emulate the first-come first-serve, the processing sharing, or the feedback policies, as well as different levels of their mixtures. This policy is implemented in the Linux kernel as a replacement of the default scheduler. The drastic changes of behaviors as the parameter changes are analyzed and simulated. Its performance is measured with the real systems by the workload generators and benchmarks. (pdf) (ps) A First Order Analysis of Lighting, Shading, and Shadows Ravi Ramamoorthi, Dhruv Mahajan, Peter Belhumeur 2006-04-30 The shading in a scene depends on a combination of\r\nmany factors---how the lighting varies spatially across a surface, how\r\nit varies along different directions, the geometric curvature and\r\nreflectance properties of objects, and the locations of soft shadows.\r\nIn this paper, we conduct a complete first order or gradient analysis of\r\nlighting, shading and shadows, showing how each factor separately\r\ncontributes to scene appearance, and when it is important.  Gradients\r\nare well suited for analyzing the intricate combination of appearance\r\neffects, since each gradient term corresponds directly to variation in\r\na specific factor.  First, we show how the spatial {\\em and}\r\ndirectional gradients of the light field change, as light interacts\r\nwith curved objects.  This extends the recent frequency analysis of\r\nDurand et al.\\ to gradients, and has many advantages for operations,\r\nlike bump-mapping, that are difficult to analyze in the\r\nFourier domain.  Second, we consider the individual terms responsible\r\nfor shading gradients, such as lighting variation, convolution with\r\nthe surface BRDF, and the object's curvature.  This analysis indicates\r\nthe relative importance of various terms, and shows precisely how they\r\ncombine in shading.  As one practical application, our theoretical\r\nframework can be used to adaptively sample images in high-gradient\r\nregions for efficient rendering.  Third, we understand the effects of\r\nsoft shadows, computing accurate visibility gradients.  We generalize\r\nprevious work to arbitrary curved occluders, and develop a \r\nlocal framework that is easy to integrate with conventional\r\nray-tracing methods.  Our visibility gradients can be directly used in\r\npractical gradient interpolation methods for efficient rendering. (pdf) (ps) Quantifying Application Behavior Space for Detection and Self-Healing Michael Locasto, Angelos Stavrou, Gabriela G. Cretu, Angelos D. Keromytis, Salvatore J. Stolfo 2006-04-08 The increasing sophistication of software attacks has created the need\r\nfor increasingly finer-grained intrusion and anomaly detection\r\nsystems, both at the network and the host level. We believe that the\r\nnext generation of defense mechanisms will require a much more\r\ndetailed dynamic analysis of application behavior than is currently\r\ndone.  We also note that the same type of behavior analysis is needed\r\nby the current embryonic attempts at self-healing systems. Because\r\nsuch mechanisms are currently perceived as too expensive in terms of\r\ntheir performance impact, questions relating to the feasibility and\r\nvalue of such analysis remain unexplored and unanswered.\r\n\r\nWe present a new mechanism for profiling the behavior space of an\r\napplication by analyzing all function calls made by the process,\r\nincluding regular functions and library calls, as well as system\r\ncalls. We derive behavior from aspects of both control and data flow.\r\nWe show how to build and check profiles that contain this information\r\nat the binary level -- that is, without making changes to the\r\napplication's source, the operating system, or the compiler. This\r\ncapability makes our system, Lugrind, applicable to a variety of\r\nsoftware, including COTS applications. Profiles built for the\r\napplications we tested can predict behavior with 97% accuracy given a\r\ncontext window of 15 functions. Lugrind demonstrates the\r\nfeasibility of combining binary-level behavior profiling with\r\ndetection and automated repair. (pdf) Seamless Layer-2 Handoff using Two Radios in IEEE 802.11 Wireless Networks Sangho Shin, Andrea G. Forte, Henning Schulzrinne 2006-04-08 —We propose a layer-2 handoff using two radios and\r\nachieves seamless handoff. Also, We reduces the false handoff\r\nprobability signi\u0002cantly by introducing selective passive scanning. (pdf) Anagram: A Content Anomaly Detector Resistant to Mimicry Attack Ke Wang, Janak Parekh, Salvatore Stolfo 2006-04-07 In this paper, we present Anagram, a content anomaly detector that  \r\nmodels a mixture of high-order n-grams (n > 1) designed to detect  \r\nanomalous and ^Ósuspicious^Ô network packet payloads. By using higher- \r\norder n-grams, Anagram can detect significant anomalous byte  \r\nsequences and generate robust signatures of validated malicious  \r\npacket content. The Anagram content models are implemented using  \r\nhighly efficient Bloom filters, reducing space requirements and  \r\nenabling privacy-preserving cross-site correlation. The sensor models  \r\nthe distinct content flow of a network or host using a semi- \r\nsupervised training regimen. Previously known exploits, extracted  \r\nfrom the signatures of an IDS, are likewise modeled in a Bloom filter  \r\nand are used during training as well as detection time. We demon- \r\nstrate that Anagram can identify anomalous traffic with high accuracy  \r\nand low false positive rates. Anagram^Òs high-order n-gram analysis  \r\ntechnique is also resil-ient against simple mimicry attacks that  \r\nblend exploits with ^Ónormal^Ô appearing byte padding, such as the  \r\nblended polymorphic attack recently demonstrated in [1]. We discuss  \r\nrandomized n-gram models, which further raises the bar and makes it  \r\nmore difficult for attackers to build precise packet structures to  \r\nevade Anagram even if they know the distribution of the local site  \r\ncontent flow. Finally, Ana-gram^Òs speed and high detection rate makes  \r\nit valuable not only as a standalone sensor, but also as a network  \r\nanomaly flow classifier in an instrumented fault-tolerant host-based  \r\nenvironment; this enables significant cost amortization and the  \r\npossibility of a ^Ósymbiotic^Ô feedback loop that can improve accuracy  \r\nand reduce false positive rates over time. (pdf) Bloodhound: Searching Out Malicious Input in Network Flows for Automatic Repair Validation Michael Locasto, Matthew Burnside, Angelos D. Keromytis 2006-04-05 Many current systems security research efforts focus on mechanisms for\r\nIntrusion Prevention and Self-Healing Software.  Unfortunately, such\r\nsystems find it difficult to gain traction in many deployment\r\nscenarios.  For self-healing techniques to be realistically employed,\r\nsystem owners and administrators must have enough confidence in the\r\nquality of a generated fix that they are willing to allow its\r\nautomatic deployment. \r\n\r\nIn order to increase the level of confidence in these systems, the efficacy of a 'fix' must be tested and validated after it\r\nhas been automatically developed, but before it is actually\r\ndeployed. Due to the nature of attacks, such verification must proceed\r\nautomatically.  We call this problem Automatic Repair Validation\r\n(ARV). As a way to illustrate the difficulties faced by ARV, we\r\npropose the design of a system, Bloodhound, that tracks and\r\nstores malicious network flows for later replay in the validation\r\nphase for self-healing software. (pdf) Streak Seeding Automation Using Silicon Tools Atanas Georgiev, Sergey Vorobiev, William Edstrom, Ting Song, Andrew Laine, John Hunt 2006-03-31 This report presents an approach to automation of a protein\r\ncrystallography task called streak seeding.  The approach is based\r\non novel and unique custom-designed silicon microtools, which we\r\nexperimentally verified to produce results similar to the results\r\nfrom traditionally used boar bristles.  The advantage to using\r\nsilicon is that it allows the employment of state-of-the-art\r\nmicro-electro-mechanical-systems (MEMS) technology to produce\r\nmicrotools of various shapes and sizes and thatit is rigid and can\r\nbe easily adopted as an accurately calibrated end-effector on a\r\nmicrorobotic system.  A working prototype of an automatic streak\r\nseeding system is presented, which has been successfully applied\r\nfor protein crystallization. (pdf) PalProtect: A Collaborative Security Approach to Comment Spam Benny Wong, Michael Locasto, Angelos D. Keromytis 2006-03-22 Collaborative security is a promising solution to many types of security\r\nproblems. Organizations and individuals often have a limited amount of \r\nresources to detect and respond to the threat of automated attacks.\r\nEnabling them to take advantage of the resources of their peers by sharing information related to such threats is a major step towards automating defense systems.\r\n\r\nIn particular, comment spam posted on blogs as a way for attackers to\r\ndo Search Engine Optimization (SEO) is a major annoyance. Many measures\r\nhave been proposed to thwart such spam, but all such measures are currently enacted and operate within one administrative domain. We propose and implement a system for cross-domain information sharing to improve the quality and speed of defense against such spam. (pdf) Using Angle of Arrival (Bearing) Information in Network Localization Tolga Eren, Walter Whiteley, Peter N. Belhumeur 2006-03-18 In this paper, we consider using angle of arrival information\r\n(bearing) for network localization and control in two different\r\nfields of multi-agent systems: (i) wireless sensor networks; (ii)\r\nrobot networks. The essential property we require in this paper is\r\nthat a node can infer heading information from its neighbors. We\r\naddress the uniqueness of network localization solutions by the\r\ntheory of globally rigid graphs. We show that while the parallel\r\nrigidity problem for formations with bearings is isomorphic to the\r\ndistance case, the global rigidity of the formation is simpler (in\r\nfact identical to the simpler rigidity case) for a network with\r\nbearings, compared to formations with distances. We provide the\r\nconditions of localization for networks in which the neighbor\r\nrelationship is not necessarily symmetric. (pdf) (ps) A Theory of Spherical Harmonic Identities for BRDF/Lighting Transfer and Image Consistency Dhruv Mahajan, Ravi Ramamoorthi, Brian Curless 2006-03-17 We develop new mathematical results based on the spherical harmonic\r\nconvolution framework for reflection from a curved surface. We derive novel identities, which are the angular frequency domain analogs to common spatial domain invariants such as reflectance ratios. They apply in a number of canonical cases, including single and multiple images of objects under the same and different lighting conditions. One important case we consider is two different glossy objects in two different lighting environments. While this paper is primarily theoretical,\r\nit has the potential to lay the mathematical foundations for two important practical applications. First, we can develop more general algorithms for inverse rendering problems, which can directly relight and change material properties by transferring the BRDF or lighting from another object or illumination. Second, we can check the consistency of an image, to detect tampering or image splicing. (pdf) Passive Duplicate Address Detection for Dynamic Host Configuration Protocol (DHCP) Andrea G. Forte, Sangho Shin, Henning Schulzrinne 2006-03-07 During a layer-3 handoff, address acquisition via\r\nDHCP is often the dominant source of handoff delay, duplicate\r\naddress detection (DAD) being responsible for most of the delay.\r\nWe propose a new DAD algorithm, passive DAD (pDAD), which\r\nwe show to be effective, yet introduce only a few milliseconds of\r\ndelay. Unlike traditional DAD, pDAD also detects the unauthorized\r\nuse of an IP address before it is assigned to a DHCP client. (pdf) Evaluating an Evaluation Method: The Pyramid Method Applied to 2003 Document Understanding Conference (DUC) Data Rebecca Passonneau 2006-03-03 A pyramid evaluation dataset was created for DUC 2003 in order to\r\ncompare results with DUC 2005, and to provide an independent test\r\nof the evaluation metric. The main differences between\r\nDUC 2003 and 2005 datasets pertain to the document length, cluster\r\nsizes, and model summary length. For five of the DUC 2003 document sets,\r\ntwo pyramids each were\r\nconstructed by annotators working independently. Scores of the same\r\npeer using different pyramids were highly correlated. Sixteen systems\r\nwere evaluated on eight document sets.\r\nAnalysis of variance using Tukey's Honest Significant Difference\r\nmethod showed significant differences among all eight document sets,\r\nand more significant differences among the sixteen systems than for DUC 2005. (pdf) Rigid Formations with Leader-Follower Architecture Tolga Eren, Walter Whiteley, Peter N. Belhumeur 2006-02-25 This paper is concerned with information structures used in rigid\r\nformations of autonomous agents that have leader-follower\r\narchitecture. The focus of the paper is on sensor/network\r\ntopologies to secure control of rigidity. This papers extends the\r\nprevious rigidity based approaches for formations with symmetric\r\nneighbor relations to include formations with leader-follower\r\narchitecture. We provide necessary and sufficient conditions for\r\nrigidity of directed formations, with or without cycles. We\r\npresent the directed Henneberg constructions as a sequential\r\nprocess for all guide rigid digraphs. We refine those results for\r\nacyclic formations, where guide rigid formations had a simple\r\nconstruction. The analysis in this paper confirms that acyclicity\r\nis not a necessary condition for stable rigidity. The cycles are\r\nnot the real problem, but rather the lack of guide freedom is the\r\nreason behind why cycles have been seen as a problematic topology.\r\nTopologies that have cycles within a larger architecture can be\r\nstably rigid, and we conjecture that all guide rigid formations\r\nare stably rigid for internal control. We analyze how the external\r\ncontrol of guide agents can be integrated into stable rigidity of\r\na larger formation. The analysis in the paper also confirms the\r\ninconsistencies that result from noisy measurements in redundantly\r\nrigid formations. An algorithm given in the paper establishes a\r\nsequential way of determining the directions of links from a given\r\nundirected rigid formation so that the necessary and sufficient\r\nconditions are fulfilled. (pdf) (ps) Using an External DHT as a SIP Location Service Kundan Singh, Henning Schulzrinne 2006-02-22 Peer-to-peer Internet telephony using the Session Initiation Protocol\r\n(P2P-SIP) can exhibit two different architectures: an existing P2P\r\nnetwork can be used as a replacement for lookup and updates, or a P2P\r\nalgorithm can be implemented using SIP messages.  In this paper, we\r\nexplore the first architecture using the OpenDHT service as an\r\nexternally managed P2P network. We provide design details such as\r\nencryption and signing using pseudo-code and examples to provide\r\nP2P-SIP for various deployment components such as P2P client, proxy\r\nand adaptor, based on our implementation. The design can be used with\r\nother distributed hash tables (DHTs) also. (pdf) (ps) Synthesis of On-Chip Interconnection Structures:From Point-toPoint Links to Networks-on-Chip Alessandro Pinto, Luca P. Carloni, Alberto L. Sangiovanni-Vincentelli 2006-02-20 Packet-switched networks-on-chip (NOC) have been advocated as the solution to the challenge of organizing efficient and reliable communication structures among the components of a system-on-chip (SOC). A critical issue in designing a NOC is to determine its topology given the set of point-to-point communication requirements among these\r\ncomponents. We present a novel approach to on-chip communication synthesis that is based on the iterative combination of two efficient computational steps: (1) an application of the k-Median algorithm to coarsely determine the global communication structure (which may turned out not be a network after all), and a (2) a variation of the shortest-path algorithm in order to finely tune the data flows on the communication channels. The application of our method to case\r\nstudies taken from the literature shows that we can automatically synthesize optimal NOC topologies for multi-core on-chip processors and it offers new insights on why NOC are not necessarily a value proposition for some classes of applcation-specific SOCs. (pdf) Theoretical Bounds on Control-Plane Self-Monitoring in Routing Protocols Raj Kumar Rajendran, Vishal Misra, Dan Rubenstein 2006-02-15 Routing protocols rely on the cooperation of nodes in the network to\r\nboth forward packets and to select the forwarding routes.\r\nThere have been several instances in which an entire\r\nnetwork's routing collapsed simply because a seemingly insignificant\r\nset of nodes reported erroneous routing information to their\r\nneighbors. It may have been possible for other nodes to trigger an automated response\r\nand prevent the problem by analyzing received routing information for\r\ninconsistencies that revealed the errors. Our theoretical study seeks to\r\nunderstand when nodes can detect the existence of errors in the\r\nimplementation of route\r\nselection elsewhere in the network through monitoring their own\r\nrouting states for inconsistencies. We start by constructing a\r\nmethodology, called Strong-Detection, that helps answer the\r\nquestion. We then apply Strong-Detection to three classes of routing\r\nprotocols: distance-vector, path-vector, and link-state.  For each\r\nclass, we derive low-complexity, self-monitoring algorithms that use\r\nthe routing state created by these routing protocols to identify any\r\ndetectable anomalies. These algorithms are then used\r\nto compare and contrast the self-monitoring power these various\r\nclasses of protocols possess. We also study the trade-off between\r\ntheir state-information complexity and ability to identify routing anomalies. (pdf) (ps) A Survey of Security Issues and Solutions in Presence Vishal Kumar Singh, Henning Schulzrinne 2006-02-10 With the growth of presence based services, it is important to securely manage and distribute sensitive presence information such as user location. We survey techniques that are used for security and privacy of presence information. In particular, we describe the SIMPLE based presence specific authentication, integrity and confidentiality. We also discuss the IETF’s common policy for geo-privacy, presence authorization for presence information privacy and distribution of different levels of presence information to different watchers. Additionally, we describe an open problem of getting the aggregated presence from the trusted server without the server knowing the presence information, and propose a solution. Finally, we discuss denial of service attacks on the presence system and ways to mitigate them. (pdf) SIMPLEstone - Benchmarking Presence Server Performance Vishal Kumar Singh, Henning Schulzrinne 2006-02-10 Presence is an important enabler for communication in Internet telephony systems. Presence-based services depend on accurate and timely delivery of presence information. Hence, presence systems need to be appropriately dimensioned to meet the growing number of users, varying number of devices as presence sources, the rate at which they update presence information to the network and the rate at which network distributes the user’s presence information to the watchers. SIMPLEstone is a set of metrics for benchmarking the performance of presence systems based on SIMPLE. SIMPLEstone benchmarks a presence server by generating requests based on a work load specification. It measures server capacity in terms of request handling capacity as an aggregate of all types of requests as well as individual request types. The benchmark treats different configuration modes in which presence server interoperates with the Session Initiation protocol (SIP) server as one block. (pdf) Grouped Distributed Queues: Distributed Queue, Proportional Share Multiprocessor Scheduling Bogdan Caprita, Jason Nieh, Clifford Stein 2006-02-07 We present Grouped Distributed Queues (GDQ), the first proportional share scheduler for multiprocessor systems that, by using a distributed queue architecture, scales well with a large number of processors and processes.  GDQ achieves accurate proportional fairness scheduling with only O(1) scheduling overhead.\r\n\r\nGDQ takes a novel approach to distributed queuing: instead of creating per-processor queues that need to be constantly balanced to achieve any measure of proportional sharing fairness, GDQ uses a simple grouping strategy to organize processes into groups based on similar processor time allocation rights, and then assigns processors to groups based on aggregate group shares. Group membership of processes is static, and fairness is achieved by dynamically migrating processors among groups. The set of processors working on a group use simple, low-overhead round-robin queues, while processor reallocation among groups is achieved using a new multiprocessor adaptation of the well-known Weighted Fair Queuing algorithm. By commoditizing processors and decoupling their allocation from process scheduling, GDQ provides, with only constant scheduling cost, fairness within a constant of the ideal generalized processor sharing model for process weights with a fixed upper bound.\r\n\r\nWe have implemented GDQ in Linux and measured its performance. Our experimental results show that GDQ has low overhead and scales well with the number of processors. (pdf) W3Bcrypt: Encryption as a Stylesheet Angelos Stavrou, Michael Locasto, Angelos D. Keromytis 2006-02-06 While web communications are increasingly protected by transport\r\nlayer cryptographic operations (SSL/TLS), there are many\r\nsituations where even the communications infrastructure provider\r\ncannot be trusted. The end-to-end (E2E) encryption of data\r\nbecomes increasingly important in these trust models to protect\r\nthe confidentiality and integrity of the data against snooping\r\nand modification by the communications provider.\r\n\r\nWe introduce W3Bcrypt, an extension to the Mozilla Firefox web\r\nplatform that enables application-level cryptographic protection\r\nfor web content. In effect, we view cryptographic operations as\r\na type of style to be applied to web content along with layout\r\nand coloring operations. Among the main benefits of using\r\nencryption as a stylesheet are $(a)$ reduced workload on a web\r\nserver, $(b)$ targeted content publication, and $(c)$ greatly\r\nincreased privacy. This paper discusses our implementation for\r\nFirefox, but the core ideas are applicable to most current\r\nbrowsers. (pdf) A Runtime Adaptation Framework for Native C and Bytecode Applications Rean Griffith, Gail Kaiser 2006-01-27 The need for self-healing software to respond with a\r\nreactive, proactive or preventative action as a result of\r\nchanges in its environment has added the non-functional\r\nrequirement of adaptation to the list of facilities expected\r\nin self-managing systems. The adaptations we are concerned\r\nwith assist with problem detection, diagnosis and\r\nremediation. Many existing computing systems do not include\r\nsuch adaptation mechanisms, as a result these systems\r\neither need to be re-designed to include them or there\r\nneeds to be a mechanism for retro-fitting these mechanisms.\r\nThe purpose of the adaptation mechanisms is to ease the\r\njob of the system administrator with respect to managing\r\nsoftware systems. This paper introduces Kheiron, a framework\r\nfor facilitating adaptations in running programs in a\r\nvariety of execution environments without requiring the redesign\r\nof the application. Kheiron manipulates compiled\r\nC programs running in an unmanaged execution environment\r\nas well as programs running in Microsoft’s Common\r\nLanguage Runtime and SunMicrosystems’ Java VirtualMachine.\r\nWe present case-studies and experiments that demonstrate\r\nthe feasibility of using Kheiron to support self-healing\r\nsystems. We also describe the concepts and techniques used\r\nto retro-fit adaptations onto existing systems in the various\r\nexecution environments. (pdf) Binary-level Function Profiling for Intrusion Detection and Smart Error Virtualization Michael Locasto, Angelos Keromytis 2006-01-26 Most current approaches to self-healing software (SHS) suffer\r\nfrom semantic incorrectness of the response mechanism. To\r\nsupport SHS, we propose Smart Error Virtualization (SEV),\r\nwhich treats functions as transactions but provides a way to\r\nguide the program state and remediation to be a more correct\r\nvalue than previous work.\r\n\r\nWe perform runtime binary-level profiling on unmodified \r\napplications to learn both good return values and error return \r\nvalues (produced when the program encounters ``bad'' input). \r\nThe goal is to ``learn from mistakes'' by converting malicious \r\ninput to the program's notion of ``bad'' input.\r\n\r\nWe introduce two implementations of this system that support\r\nthree major uses: function profiling for regression testing, \r\nfunction profiling for host-based anomaly detection \r\n(envinroment-specialized fault detection), and function profiling\r\nfor automatic attack remediation via SEV. Our systems do not\r\nrequire access to the source code of the application to enact\r\na fix. Finally, this paper is, in part, a critical examination of \r\nerror virtualization in order to shed light on how to approach\r\nsemantic correctness. (pdf) (ps) Converting from Spherical to Parabolic Coordinates Aner Ben-Artzi 2006-01-20 A reference for converting directly between Spherical Coordinates and Parabolic Coordinates without using the intermediate Cartesian Coordinates. (pdf) Multi Facet Learning in Hilbert Spaces Imre Risi Kondor, Gabor Csanyi, Sebastian E. Ahnert, Tony Jebara 2005-12-31 We extend the kernel based learning framework to learning from linear \r\nfunctionals, such as partial derivatives. \r\nThe learning problem is formulated as\r\na generalized regularized risk minimization problem, possibly \r\ninvolving several different functionals. \r\nWe show how to reduce this to  \r\nconventional kernel based learning methods \r\nand explore a specific application in Computational \r\nCondensed Matter Physics. (pdf) (ps) A Lower Bound for the Sturm-Liouville Eigenvalue Problem on a Quantum Computer Arvid J. Bessen 2005-12-14 We study the complexity of approximating the smallest eigenvalue of a univariate Sturm-Liouville problem on a quantum computer. This general problem includes the special case of solving a one-dimensional Schroedinger equation with a given potential for the ground state energy. \r\nThe Sturm-Liouville problem depends on a function q, which, in the case of the Schroedinger equation, can be identified with the potential function V. Recently Papageorgiou and Wozniakowski proved that quantum computers achieve an exponential reduction in the number of queries over the number needed in the classical worst-case and randomized settings for smooth functions q. Their method uses the (discretized) unitary propagator and arbitrary powers of it as a query (\"power queries\"). They showed that the Sturm-Liouville equation can be solved with O(log(1/e)) power queries, while the number of queries in the worst-case and randomized settings on a classical computer is polynomial in 1/e. This proves that a quantum computer with power queries achieves an exponential reduction in the number of queries compared to a classical computer. \r\nIn this paper we show that the number of queries in Papageorgiou's and Wozniakowski's algorithm is asymptotically optimal. In particular we prove a matching lower bound of log(1/e) power queries, therefore showing that log(1/e) power queries are sufficient and necessary. Our proof is based on a frequency analysis technique, which examines the probability distribution of the final state of a quantum algorithm and the dependence of its Fourier transform on the input. (pdf) (ps) Dynamic Adaptation of Temporal Event Correlation Rules Rean Griffith, Gail Kaiser, Joseph Hellerstein, Yixin Diao 2005-12-10 Temporal event correlation is essential to realizing self-managing distributed systems. Autonomic controllers often require\r\nthat events be correlated across multiple components using rule patterns with timer-based transitions, e.g., to detect denial\r\nof service attacks and to warn of staging problems with business critical applications. This short paper discusses automatic\r\nadjustment of timer values for event correlation rules, in particular compensating for the variability of event propagation\r\ndelays due to factors such as contention for network and server resources. We describe a corresponding Management Station\r\narchitecture and present experimental studies on a testbed system that suggest that this approach can produce results at least\r\nas good as an optimal fixed setting of timer values. (pdf) Qubit Complexity of Continuous Problems Anargyros Papageorgiou, Joseph Traub 2005-12-09 The number of qubits used by a quantum algorithm will be a crucial computational resource\r\nfor the foreseeable future. We show how to obtain the classical query complexity for continuous\r\nproblems. We then establish a simple formula for a lower bound on the qubit complexity in terms\r\nof the classical query complexity. (pdf) An Event System Architecture for Scaling Scale-Resistant Services Philip Gross 2005-12-09 Large organizations are deploying ever-increasing numbers of networked compute devices, from utilities installing smart controllers on electricity distribution cables, to the military giving PDAs to soldiers, to corporations putting PCs on the desks of employees.  These computers are often far more capable than is needed to accomplish their primary task, whether it be guarding a circuit breaker, displaying a map, or running a word processor.  These devices would be far more useful if they had some awareness of the world around them: a controller that resists tripping a switch, knowing that it would set off a cascade failure, a PDA that warns its owner of imminent danger, a PC that exchanges reports of suspicious network activity to its peers to identify stealthy computer crackers.  \r\n\r\nIn order to provide these higher-level services, the devices need a model of their environment.  The controller needs a model of the distribution grid, the PDA needs a model of the battlespace, and the PC needs a model of the network and of normal network and user behavior. Unfortunately, not only might models such as these require substantial computational resources, but generating and updating them is even more demanding. Modelbuilding algorithms tend to be bad in three ways: requiring large amounts of CPU and memory to run, needing large amounts of data from the outside to stay up to date, and running so slowly that can’t keep up with any fast changes in the environment that might occur.  \r\n\r\nWe can solve these problems by reducing the scope of the model to the immediate locale of the device, since reducing the size of the model makes the problem of model generation much more tractable. But such models are also much less useful, having no knowledge of the wider system.  \r\n\r\nThis thesis proposes a better solution to this problem called Level of Detail, after the computer graphics technique of the same name. Instead of simplifying the representation of distant objects, however, we simplify less-important data. Compute devices in the system receive streams of data that is a mixture of detailed data from devices that directly affect them and data summaries (aggregated data) from less directly influential devices. The degree to which the data is aggregated (i.e., how much it is reduced) is determined by calculating an influence metric between the target device and the remote device. The smart controller thus receives a continuous stream of raw data from the adjacent transformer, but only an occasional small status report summarizing all the equipment in a neighborhood in another part of the city.  \r\n\r\nThis thesis describes the data distribution system, the aggregation functions, and the influence metrics that can be used to implement such a system. I also describe my current towards establishing a test environment and validating the concepts, and describe the next steps in the research plan. (pdf) Tree Dependent Identically Distributed Learning Tony Jebara, Philip M. Long 2005-12-06 We view a dataset of points or samples as having an underlying, yet\r\nunspecified, tree structure and exploit this assumption in learning\r\nproblems. Such a tree structure assumption is equivalent to treating a\r\ndataset as being tree dependent identically distributed or tdid and\r\npreserves exchange-ability. This extends traditional iid assumptions\r\non data since each datum can be sampled sequentially after being\r\nconditioned on a parent. Instead of hypothesizing a single best tree\r\nstructure, we infer a richer Bayesian posterior distribution over tree\r\nstructures from a given dataset. We compute this posterior over\r\n(directed or undirected) trees via the Laplacian of conditional\r\ndistributions between pairs of input data points. This posterior\r\ndistribution is efficiently normalized by the Laplacian's\r\ndeterminant and also facilitates novel maximum likelihood estimators,\r\nefficient expectations and other useful inference computations. In a\r\nclassification setting, tdid assumptions yield a criterion that\r\nmaximizes the determinant of a matrix of conditional distributions\r\nbetween pairs of input and output points. This leads to a novel\r\nclassification algorithm we call the Maximum Determinant\r\nMachine. Unsupervised and supervised experiments are shown. (pdf) (ps) Micro-speculation, Micro-sandboxing, and Self-Correcting Assertions: Support for Self-Healing Software and Application Communities Michael Locasto 2005-12-05 Software faults and vulnerabilities continue to present significant \r\nobstacles to achieving reliable and secure software. The critical\r\nproblem is that systems currently lack the capability to respond \r\nintelligently and automatically to attacks -- especially attacks \r\nthat exploit previously unknown vulnerabilities or are delivered by\r\npreviously unseen inputs. Therefore, the goal of this thesis is to \r\nprovide an environment where both supervision and automatic \r\nremediation can take place. Also provided is a mechanism to guide \r\nthe supervision environment in detection and repair activities.\r\n\r\nThis thesis supports the notion of Self-Healing Software by\r\nintroducing three novel techniques: \\emph{micro-sandboxing}, \r\n\\emph{micro-speculation}, and \\emph{self-correcting assertions}. These\r\ntechniques are combined in a kernel-level emulation framework to \r\nspeculatively execute code that may contain faults or \r\nvulnerabilities and automatically repair such faults or exploited \r\nvulnerabilities. The framework, VPUF, introduces the concept of\r\ncomputation as an operating system service by providing control\r\nfor an array of virtual processors in the Linux kernel (creating\r\nthe concept of an \\emph{endolithic} kernel). This thesis introduces\r\nROAR (Recognize, Orient, Adapt, Respond) as a conceptual\r\nworkflow for Self-healing Software systems.\r\n\r\nThis thesis proposal outlines a 17 month program for developing the \r\nmajor components of the proposed system, implementing them on a \r\nCOTS operating system and programming language, subjecting them to \r\na battery of evaluations for performance and efficacy, and publishing \r\nthe results. In addition, this proposal looks forward to several \r\nareas of follow-on work, including implementing some of the proposed \r\ntechniques in hardware and leveraging the general kernel-level \r\nframework to support Application Communities. (pdf) (ps) A Control Theory Foundation for Self-Managing Computing Systems Yixin Diao, Joseph Hellerstein, Sujay Parekh, Rean Griffith, Gail Kaiser, Dan Phung 2005-12-05 The high cost of operating large computing installations has motivated\r\na broad interest in reducing the need for human intervention by making\r\nsystems self-managing. This paper explores the extent to which control\r\ntheory can provide an architectural and analytic foundation for\r\nbuilding self-managing systems. Control theory provides a rich set of\r\nmethodologies for building automated self-diagnosis and self-repairing\r\nsystems with properties such as stability, short settling times, and\r\naccurate regulation. However, there are challenges in applying control\r\ntheory to computing systems, such as developing effective resource\r\nmodels, handling sensor delays, and addressing lead times in effector\r\nactions. We propose a deployable testbed for autonomic computing\r\n(DTAC) that we believe will reduce the barriers to addressing research\r\nproblems in applying control theory to computing systems. The initial\r\nDTAC architecture is described along with several problems that it can\r\nbe used to investigate. (pdf) A New Routing Metric for High Throughput in Dense Ad Hoc Networks Hoon Chang, Vishal Misra, Dan Rubenstein 2005-12-01 Routing protocols in most ad hoc networks use the length of paths as the routing metric. Recent findings have revealed that the minimum-hop metric can not achieve the maximum throughput because it tries to reduce the number of hops by containing long range links, where packets need to be transmitted at the lowest transmission rate. In this paper, we investigate the tradeoff between transmission rates and throughputs and show that in dense networks with uniform-distributed traffic, there exists the optimal rate that may not be the lowest rate. Based on our observation, we propose a new routing metric, which measures the expected capability of a path assuming the per-node fairness. We develop a routing protocol based on DSDV and demonstrate that the routing metric enhances the system throughput by 20% compared to the original DSDV. (pdf) Effecting Runtime Reconfiguration in Managed Execution Environments Rean Griffith, Giuseppe Valetto, Gail Kaiser 2005-11-21 Managed execution environments such as Microsoft’s Common Language Runtime (CLR) and Sun Microsystems’ Java Virtual Machine (JVM) provide a number of services – including but not limited to application isolation, security sandboxing, garbage collection and structured exception handling – that are aimed primarily at enhancing the robustness of managed applications. However, none of these services directly enables performing reconfigurations, repairs or diagnostics on the managed\r\napplications and/or its constituent subsystems and components.\r\nIn this paper we examine how the facilities of a managed execution environment can be leveraged to support runtime system adaptations, such as reconfigurations and repairs. We describe an adaptation framework we have developed, which uses these facilities to dynamically attach/detach an engine capable of performing reconfigurations and repairs on a target system while it executes. Our adaptation framework is lightweight, and transparent to the application and the managed execution environment: it does not require recompilation of the application nor specially compiled\r\nversions of the managed execution runtime. Our prototype was implemented for the CLR. To evaluate our framework beyond toy examples, we searched on SourceForge for potential target systems already implemented on the CLR that might benefit from runtime adaptation. We report on our experience using our prototype to effect runtime reconfigurations in a system that was developed and is in use by others: the Alchemi enterprise Grid Computing System developed at the University of Melbourne, Australia. (pdf) (ps) Adaptive Synchronization of Semantically Compressed Instructional Videos for Collaborative Distance Learning Dan Phung, Giuseppe Valetto, Gail Kaiser, Tiecheng Liu, John Kender 2005-11-21 The increasing popularity of online courses has highlighted the need for collaborative learning tools for student groups. In addition, the introduction of lecture videos into the online curriculum has drawn attention to the disparity in the network resources available to students. We present an e-Learning architecture and adaptation model called AI2TV (Adaptive Interactive Internet Team Video), which allows groups of students to collaboratively view a video in synchrony. AI2TV upholds the invariant that each student will view semantically equivalent content at all times.  A semantic compression model is developed to provide instructional videos at different level-of-details to accommodate dynamic network conditions and users’ system requirements.  We take advantage of the semantic compression algorithm’s ability to provide different layers of semantically equivalent video by adapting the client to play at the appropriate layer that provides the client with the richest possible viewing experience.  Video player actions, like play, pause and stop, can be initiated by any group member and and the results of those actions are synchronized with all the other students.  These features allow students to review a lecture video in tandem, facilitating the learning process. Experimental trials show that AI2TV successfully synchronizes instructional videos for distributed students while concurrently optimizing the video quality, even under conditions of fluctuating bandwidth, by adaptively adjusting the quality level for each student while still maintaining the invariant. (pdf) A Genre-based Clustering Approach to Content Extraction Suhit Gupta, Hila Becker, Gail Kaiser, Salvatore Stolfo 2005-11-11 The content of a webpage is usually contained within a small\r\nbody of text and images, or perhaps several articles on the same page;\r\nhowever, the content may be lost in the clutter (defined as cosmetic\r\nfeatures such as animations, menus, sidebars, obtrusive banners).\r\nAutomatic content extraction has many applications, including browsing on small cell phone and PDA screens, speech rendering for the visually\r\nimpaired, and reducing noise for information retrieval systems. We have\r\ndeveloped a framework, Crunch, which employs various heuristics for\r\ncontent extraction in the form of filters applied to the webpage's DOM\r\ntree; the filters aim to prune or transform the clutter, leaving only the content. Crunch allows users to tune what we call \"settings\", consisting of thresholds for applying a particular filter and/or for toggling a filter on/off, because the HTML components that characterize clutter can vary significantly from website to website. However, we have found that the same settings tend to work well across different websites of the same genre, e.g., news or shopping, since the designers often employ similar page layouts. In particular, Crunch could obtain the settings for a previously unknown website by automatically classifying it as sufficiently similar to a cluster of known websites with previously adjusted settings. We present our approach to clustering a large corpus of websites into genres, using their pre-extraction textual material augmented by the snippets generated by searching for the website's domain name in web search engines. Including these snippets increases the frequency of function words needed for clustering. We use existing Manhattan distance measure and hierarchical clustering techniques, with some modifications, to pre-classify the corpus into genres offline. Our method does not require prior knowledge of the set of genres that websites fit into, but to be useful a priori settings must be available for some member of each cluster or a nearby cluster (otherwise defaults are used). Crunch classifies newly encountered websites online in linear-time, and then applies the corresponding filter settings, with no noticeable delay added by our content-extracting web proxy. (pdf) Privacy-Preserving Distributed Event Correlation Janak Parekh 2005-11-07 Event correlation is a widely-used data processing methodology for a broad variety of applications, and is especially useful in the context of distributed monitoring for software faults and vulnerabilities. However, most existing solutions have typically been focused on \"intra-organizational\" correlation; organizations typically employ privacy policies that prohibit the exchange of information outside of the organization. At the same time, the promise of \"inter-organizational\" correlation is significant given the broad availability of Internet-scale communications, and its potential role in both software maintenance and software vulnerability exploits.\r\n\r\nIn this proposal, I present a framework for reconciling these opposing forces in event correlation via the use of privacy preservation integrated into the event processing framework. By integrating flexible privacy policies, we enable the correlation of organizations' data without actually releasing sensitive information. The framework supports both source anonymity and data privacy, yet allows for the time-based correlation of a broad variety of data. The framework is designed as a lightweight collection of components to enable integration with existing COTS platforms and distributed systems. I also present two different implementations of this framework: XUES (XML Universal Event Service), an event processor used as part of a software monitoring platform called KX (Kinesthetics eXtreme), and Worminator, a collaborative Intrusion Detection System.\r\n\r\nKX comprised a series of components, connected together with a publish-subscribe content-based routing event subsystem, for the autonomic software monitoring of complex distributed systems. Sensors were installed in legacy systems. XUES' two modules then performed event processing on sensor data: information was collected and processed by the Event Packager, and correlated using the Event Distiller. While XUES itself was not privacy-preserving, it laid the groundwork for this thesis by supporting event typing, the use of publish-subscribe and extensibility support via pluggable event transformation modules.\r\n\r\nWorminator, the second implementation, extends the XUES platform to fully support privacy-preserving event types and algorithms in the context of a Collaborative Intrusion Detection System (CIDS), whereby sensor alerts can be exchanged and corroborated--a reduced form of correlation that enables collaborative verification--without revealing sensitive information about a contributor's network, services, or even external sources as required. Worminator also fully anonymizes source information, allowing contributors to decide their preferred level of information disclosure. Worminator is implemented as a monitoring framework on top of a COTS IDS sensor, and demonstrably enables the detection of not only worms but also \"broad and stealthy\" scans; traditional single-network sensors either bury such scans in large volumes or miss them entirely. Worminator has been successfully deployed at 5 collaborating sites and work is under way to scale it up further.\r\n\r\nThe contributions of this thesis include the development of a cross-application-domain event correlation framework with native privacy-preserving types, the use and validation of privacy-preserving corroboration, and the establishment of a practical deployed collaborative security system. I also outline the next steps in the thesis research plan, including the development of evaluation metrics to quantify Worminator's effectiveness at long-term scan detection, the overhead of privacy preservation and the effectiveness of our approach against adversaries, be they \"honest-but-curious\" or actively malicious. This thesis has broad future work implications, including privacy-preserving signature detection and distribution, distributed stealthy attacker profiling, and \"application community\"-based software vulnerability detection. (pdf) Tractability of quasilinear problems.  II: Second-order elliptic problems A. G. Werschulz, H. Wozniakowski 2005-11-01 In a previous paper, we developed a general framework for establishing\r\ntractability and strong tractability for quasilinear multivariate\r\nproblems in the worst case setting.  One important example of such a\r\nproblem is the solution of the Poisson equation $-\\Delta u + qu = f$\r\nin the $d$-dimensional unit cube, in which $u$ depends linearly\r\non~$f$, but nonlinearly on~$q$.  Here, both $f$ and~$q$ are\r\n$d$-variate functions from a reproducing kernel Hilbert space with\r\nfinite-order weights of order~$\\omega$.  This means that, although~$d$\r\ncan be arbitrary large, $f$ and~$q$ can be decomposed as sums of\r\nfunctions of at most $\\omega$~variables, with $\\omega$ independent\r\nof~$d$.\r\n\r\nIn this paper, we apply our previous general results to the Poisson\r\nequation, subject to either Dirichlet or Neumann homogeneous boundary\r\nconditions.  We study both the absolute and normalized error criteria.\r\nFor all four possible combinations of boundary conditions and error\r\ncriteria, we show that the problem is \\emph{tractable}.  That is, the\r\nnumber of evaluations of $f$ and~$q$ needed to obtain an\r\n$\\e$-approximation is polynomial in~$\\e^{-1}$ and~$d$, with the degree\r\nof the polynomial depending linearly on~$\\omega$.  In addition, we\r\nwant to know when the problem is \\emph{strongly tractable}, meaning\r\nthat the dependence is polynomial only in~$\\e^{-1}$, independently\r\nof~$d$.  We show that if the sum of the weights defining the weighted\r\nreproducing kernel Hilbert space is uniformly bounded in~$d$ and the\r\nintegral of the univariate kernel is positive, then the Poisson\r\nequation is strongly tractable for three of the four possible\r\ncombinations of boundary conditions and error criterion, the only\r\nexception being the Dirichlet boundary condition under the normalized\r\nerror criterion. (pdf) TCP-Friendly Rate Control with Token Bucket for VoIP Congestion Control Miguel Maldonado, Salman Abdul Baset, Henning Schulzrinne 2005-10-17 TCP Friendly Rate Control (TFRC) is a congestion control algorithm that provides a smooth transmission rate for real-time\r\nnetwork applications. TFRC refrains from halving the sending rate on every packet drop, instead it is adjusted as a function of\r\nthe loss rate during a single round trip time. TFRC has been proven to be fair when competing with TCP flows over congested\r\nlinks, but it lacks quality-of-service parameters to improve the performance of real-time traffic. A problem with TFRC is that it\r\nuses additive increase to adjust the sending rate during periods with no congestion. This leads to short term congestion that can\r\ndegrade the quality of voice applications.\r\n\r\nWe propose two changes to TFRC that improve the performance of VoIP applications. Our implementation, TFRC with Token\r\nBucket (TFRC-TB), uses discrete calculated bit rates based on audio codec bandwidth usage to increase the sending rate. Also, it\r\nuses a token bucket to control the sending rate during congestion periods. We have used ns2, the network simulator, to compare\r\nour implementation to TFRC in a wide range of network conditions. Our results suggest that TFRC-TB can provide a quality of\r\nservice (QoS) mechanism to voice applications while competing fairly with other traffic over congested links. (pdf) (ps) Performance and Usability Analysis of Varying Web Service Architectures Michael Lenner, Henning Schulzrinne 2005-10-14 We tested the performance of four web\r\napplication architectures, namely CGI, PHP, Java\r\nservlets, and Apache Axis SOAP. All four\r\narchitectures implemented a series of typical web\r\napplication tasks. Our findings indicated that PHP\r\nproduced the smallest delay, while the SOAP\r\nimplementation produces the largest. (pdf) Square Root Propagation Andrew Howard, Tony Jebara 2005-10-07 We propose a message propagation scheme for numerically stable inference in Gaussian graphical models which can otherwise be susceptible to errors caused by finite numerical precision. We adapt square root algo­rithms, popular in Kalman filtering, to graphs with arbitrary topologies. The method consists of maintaining potentials and generating messages that involve the square root of precision matrices. Combining this with the machinery of the junction tree algorithm leads to an efficient and nu­merically stable algorithm. Experiments are presented to demonstrate the robustness of the method to numerical errors that can arise in com­plex learning and inference problems. (ps) Approximating the Reflection Integral as a Summation: Where did the delta go? Aner Ben-Artzi 2005-10-07 In this note, I explore why the the common approximation of the reflection integral is not written with a delta omega-in ( ) to replace the differential omega-in ( ).  After that, I go on to discover what really happens when the sum over all directions is reduced to a sum over a small number of directions.  In the final section, I make recommendations for correctly approximating the reflection sum, and briefly suggest a possible framework for multiple importance sampling on both lighting and brdf. (pdf) DotSlash: Providing Dynamic Scalability to Web Applications with On-demand Distributed Query Result Caching Weibin Zhao, Henning Schulzrinne 2005-09-29 Scalability poses a significant challenge for today's web applications,\r\nmainly due to the large population of potential users. To effectively\r\naddress the problem of short-term dramatic load spikes caused by web\r\nhotspots, we developed a self-configuring and scalable rescue system\r\ncalled DotSlash. The primary goal of our system is to provide dynamic\r\nscalability to web applications by enabling a web site to obtain\r\nresources dynamically, and use them autonomically without any\r\nadministrative intervention. To address the database server bottleneck,\r\nDotSlash allows a web site to set up on-demand distributed query result\r\ncaching, which greatly reduces the database workload for read mostly\r\ndatabases, and thus increases the request rate supported at a\r\nDotSlash-enabled web site. The novelty of our work is that our query\r\nresult caching is on demand, and operated based on load conditions.\r\nThe caching remains inactive as long as the load is normal, but is\r\nactivated once the load is heavy. This approach offers good data\r\nconsistency during normal load situations, and good scalability with\r\nrelaxed data consistency for heavy load periods. We have built a\r\nprototype system for the widely used LAMP configuration, and evaluated\r\nour system using the RUBBoS bulletin board benchmark. Experiments show\r\nthat a DotSlash-enhanced web site can improve the maximum request rate\r\nsupported by a factor of 5 using 8 rescue servers for the RUBBoS\r\nsubmission mix, and by a factor of 10 using 15 rescue servers for\r\nthe RUBBoS read-only mix. (pdf) (ps) The Pseudorandomness of Elastic Block Ciphers Debra Cook, Moti Yung, Angelos Keromytis 2005-09-28 We investigate elastic block ciphers, a method for constructing\r\nvariable length block ciphers, from a theorectical perspective. We\r\nview the underlying structure of an elastic block cipher as a network,\r\nwhich we refer to as an elastic network, and analyze the network in a manner similar to the analysis performed by Luby and Rackoff on Fesitel networks. We prove that a three round elastic network is a pseudorandom permutation and a four round network is a strong pseudorandom permutation when the round functions are pseudorandom permutations. (pdf) (ps) A General Analysis of the Security of Elastic Block Ciphers Debra Cook, Moti Yung, Angelos Keromytis 2005-09-28 We analyze the security of elastic block ciphers in general to\r\nshow that an attack on an elastic version of block cipher implies\r\na polynomial time related attack on the fixed-length version of\r\nthe block cipher. We relate the security of the elastic version\r\nof a block cipher to the fixed-length version by forming a reduction\r\nbetween the versions. Our method is independent of the specific\r\nblock cipher used. The results imply that if the fixed-length version\r\nof a block cipher is secure against attacks which attempt key recovery then the elastic version is also secure against such attacks. (pdf) (ps) On Elastic Block Ciphers and Their Differential and Linear Cryptanalyses Debra Cook, Moti Yung, Angelos Keromytis 2005-09-28 Motivated by applications such as databases with nonuniform field lengths,\r\nwe introduce the concept of an elastic block cipher, a new\r\napproach to variable length block ciphers which incorporates fixed\r\nsized cipher components into a new network structure.  Our scheme\r\nallows us to dynamically \"stretch\" the supported block size of\r\na block cipher up to a length double the original block size, while\r\nincreasing the computational workload proportionally to the\r\nblock size. We show that traditional attacks against an elastic block\r\ncipher are impractical if the original cipher is secure. In this paper\r\nwe focus on differential and linear attacks. Specifically, we employ\r\nan elastic version of Rijndael supporting block sizes of 128 to 256\r\nbits as an example, and show it is resistant to both differential and\r\nlinear attacks. In particular, employing a different method than what\r\nis employed in Rijndael design, we show that the probability of any\r\ndifferential characteristic for the elastic version of Rijndael is\r\n<= 2^-(block size). We further prove that both linear and\r\nnonlinear attacks are computationally infeasible for any elastic block\r\ncipher if the original cipher is not subject to such an attack and\r\ninvolves a block size for which an exhaustive plaintext search is\r\ncomputationally infeasible (as is the case for Rijndael). (pdf) (ps) PachyRand: SQL Randomization for the PostgreSQL JDBC Driver Michael Locasto, Angelos D. Keromytis 2005-08-26 Many websites are driven by web applications that deliver dynamic content stored in SQL databases. Such systems take input directly from the client via HTML forms. Without proper input validation, these systems are vulnerable to SQL injection attacks.\r\n\r\nThe predominant defense against such attacks is to implement better input validation. This strategy is unlikely to succeed on its own. A better approach is to protect systems against SQL injection automatically and not rely on manual supervision or testing strategies (which are incomplete by nature). SQL randomization is a technique that defeats SQL injection attacks by transforming the language of SQL statements in a web application such that an attacker needs to guess the transformation in order to successfully inject his code.\r\n\r\nWe present PachyRand, an extension to the PostgreSQL JDBC driver that performs SQL randomization. Our system is easily portable to most other JDBC drivers, has a small performance impact, and makes SQL injection attacks infeasible. (pdf) (ps) Parsing Preserving Techniques in Grammar Induction Smaranda Muresan 2005-08-20 In this paper we present the theoretical foundation of the search\r\nspace for learning a class of constraint-based grammars, which\r\npreserve the parsing of representative examples. We prove that under\r\nseveral assumptions the search space is a complete grammar lattice,\r\nand the lattice top element is a grammar that can always be learned\r\nfrom a set of representative examples and a sublanguage used to\r\nreduce the grammar semantics. This complete grammar lattice\r\nguarantees convergence of solutions of any learning algorithm that\r\nobeys the given assumptions. (pdf) (ps) Generic Models for Mobility Management in Next Generation Networks Maria Luisa Cristofano, Andrea G. Forte, Henning Schulzrinne 2005-08-08 In the network community different mobility management techniques have\r\nbeen proposed over the years. However, many of these techniques share a\r\nsurprisingly high number of similarities. In this technical report we analyze and\r\nevaluate the most relevant mobility management techniques, pointing out\r\ndifferences and similarities. For macro-mobility we consider Mobile IP (MIP),\r\nthe Session Initiation Protocol (SIP) and mobility management techniques\r\ntypical of a GSM network; for micro-mobility we describe and analyze several\r\nprotocols such as: Hierarchical MIP, TeleMIP, IDMP, Cellular IP and HAWAII. (pdf) Pointer Analysis for C Programs Through AST Traversal Marcio Buss, Stephen Edwards, Bin Yao, Daniel Waddington 2005-08-04 We present a pointer analysis algorithm designed for source-to-source transformations. Existing techniques for pointer analysis apply a collection of inference rules to a dismantled intermediate form of the source program, making them difficult to apply to source-to-source tools that generally work on abstract syntax trees to preserve details of the source program. Our pointer analysis algorithm operates directly on the abstract syntax tree of a C program and uses a form of standard dataflow analysis to compute the desired points-to information. We have implemented our algorithm in a source-to-source translation framework and experimental results show that it is practical on real-world examples. (pdf) Adaptive Interactive Internet Team Video Dan Phung, Giuseppe Valetto, Gail Kaiser 2005-08-04 The increasing popularity of distance learning and online courses has highlighted the lack of collaborative tools for student groups. In addition, the introduction of lecture videos into the online curriculum has drawn attention to the disparity in the network resources used by students. We present an e-Learning architecture and adaptation model called AI2TV (Adaptive Internet Interactive Team Video), a system that allows borderless, virtual students, possibly some or all disadvantaged in network resources, to collaboratively view a video in synchrony. AI2TV upholds the invariant that each student will view semantically equivalent content at all times. Video player actions, like play, pause and stop, can be initiated by any of the students and the results of those actions are seen by all the other students. These features allow group members to review a lecture video in tandem to facilitate the learning process. We show in experimental trials that our system can successfully synchronize video for distributed students while, at the same time, optimizing the video quality given actual (fluctuating) bandwidth by adaptively adjusting the quality level for each student. (pdf) Tractability of Quasilinear Problems  I: General Results Arthur Werschulz, Henryk Wozniakowski 2005-08-04 The tractability of multivariate problems has usually been studied only for the approximation of linear operators.  In this paper we study the tractability of quasilinear multivariate problems.  That is, we wish to approximate nonlinear operators~$S_d(\\cdot,\\cdot)$ that depend linearly on the first argument and satisfy a Lipschitz condition with respect to both arguments.  Here, both arguments are functions of $d$~variables.  Many computational problems of practical importance have this form.  Examples inlude the solution of specific Dirichlet, Neumann, and Schr\\\"odinger problems.  We show, under appropriate assumptions, that quasilinear problems, whose domain spaces are equipped with product or finite-order weights, are tractable or strongly tractable in the worst case setting.\r\n\r\nThis paper is the first part in a series of papers.  Here, we present tractability results for quasilinear problems under general assumptions on quasilinear operators and weights.  In future papers, we shall verify these assumptions for quasilinear problems such as the solution of specific Dirichlet, Neumann, and Schr\\\"odinger problems. (pdf) Agnostically Learning Halfspaces Adam Kalai, Adam Klivans, Yishay Mansour, Rocco A.  Servedio 2005-08-02 We consider the problem of learning a halfspace in the agnostic framework \r\nof Kearns et al., where a learner is given access to a distribution on labelled examples but the labelling may be arbitrary. The learner's goal is to output a hypothesis which performs almost as well as the optimal halfspace with respect to future draws from this distribution. Although the agnostic learning framework does not explicitly deal with noise, it is closely related to learning in worst-case noise models such as malicious noise.\r\n\r\nWe give the first polynomial-time algorithm for agnostically learning halfspaces with respect to several distributions, such as the uniform distribution over the $n$-dimensional Boolean cube {0,1}^n or unit sphere in n-dimensional Euclidean space, as well as any log-concave distribution in n-dimensional Euclidean space.  Given any constant additive factor eps>0, our algorithm runs in poly(n) time and constructs a hypothesis whose error rate is within an additive eps of the optimal halfspace. We also show this algorithm agnostically learns Boolean disjunctions in time roughly 2^{\\sqrt{n}} with respect to any distribution; this is the first subexponential-time algorithm for this problem. Finally, we obtain a new algorithm for PAC learning halfspaces under the uniform distribution on the unit sphere which can tolerate the highest level of malicious noise of any algorithm to date.\r\n\r\nOur main tool is a polynomial regression algorithm which finds a polynomial that best fits a set of points with respect to a particular metric.  We show that, in fact, this algorithm is an arbitrary-distribution generalization of the well known ``low-degree'' Fourier algorithm of Linial, Mansour, & Nisan and has excellent noise tolerance properties when minimizing with respect to the L_1 norm. We apply this algorithm in conjunction with a non-standard Fourier transform (which does not use the traditional parity basis) for learning halfspaces over the uniform distribution on the unit sphere; we believe this technique is of independent interest. (pdf) (ps) Learning mixtures of product distributions over discrete domains Jon Feldman, Ryan O'Donnell, Rocco A. Servedio 2005-07-28 We consider the problem of learning mixtures of product distributions\r\nover discrete domains in the distribution learning framework\r\nintroduced by Kearns et al.  We give a $\\poly(n/\\eps)$ time algorithm\r\nfor learning a mixture of $k$ arbitrary product distributions over the\r\n$n$-dimensional Boolean cube $\\{0,1\\}^n$ to accuracy $\\eps$, for any\r\nconstant $k$. Previous polynomial time algorithms could only achieve\r\nthis for $k = 2$ product distributions; our result answers an open\r\nquestion stated independently by Cryan and by Freund and Mansour.  We\r\nfurther give evidence that no polynomial time algorithm can succeed\r\nwhen $k$ is superconstant, by reduction from a notorious open problem\r\nin PAC learning.  Finally, we generalize our $\\poly(n/\\eps)$ time\r\nalgorithm to learn any mixture of $k = O(1)$ product distributions\r\nover $\\{0,1, \\dots, b\\}^n$, for any $b = O(1)$. (pdf) (ps) Incremental Algorithms for Inter-procedural Analysis of Safety Properties Christopher L. Conway, Kedar Namjoshi, Dennis Dams, Stephen A. Edwards 2005-07-10 Automaton-based static program analysis has proved to be an effective tool for bug finding. Current tools generally\r\nre-analyze a program from scratch in response to a change in the code, which can result in much duplicated\r\neffort. We present an inter-procedural algorithm that analyzes incrementally in response to program changes and\r\npresent experiments for a null-pointer dereference analysis. It shows a substantial speed-up over re-analysis from\r\nscratch, with a manageable amount of disk space used to store information between analysis runs. (pdf) (ps) Lexicalized Well-Founded Grammars: Learnability and Merging Smaranda Muresan, Tudor Muresan, Judith Klavans 2005-06-30 This paper presents the theoretical foundation of a new type of\r\nconstraint-based grammars, Lexicalized Well-Founded Grammars, which are\r\nadequate for modeling human language and are learnable. These features\r\nmake the grammars suitable for developing robust and scalable natural language understanding systems. Our grammars capture both syntax and semantics and have two types of constraints at the rule level: one for semantic composition and one for ontology-based semantic interpretation. We prove that these grammars can always be learned from a small set of\r\nsemantically annotated, ordered representative examples, using a\r\nrelational learning algorithm. We introduce a new semantic\r\nrepresentation for natural language, which is suitable for an\r\nontology-based interpretation and allows us to learn the compositional\r\nconstraints together with the grammar rules. Besides the learnability\r\nresults, we give a principle for grammar merging. The experiments\r\npresented in this paper show promising results for the adequacy of\r\nthese grammars in learning natural language. Relatively simple\r\nlinguistic knowledge is needed to build the small set of semantically\r\nannotated examples required for the grammar induction. (pdf) (ps) A Uniform Programming Abstraction for Effecting Autonomic Adaptations onto Software Systems Giuseppe Valetto, Gail Kaiser, Dan Phung 2005-06-05 Most general-purpose work towards autonomic or self-managing systems\r\nhas emphasized the front end of the feedback control loop, with some\r\nalso concerned with controlling the back end enactment of runtime\r\nadaptations -- but usually employing an effector technology peculiar\r\nto one type of target system. While completely generic \"one size fits\r\nall\" effector technologies seem implausible, we propose a general\r\npurpose programming model and interaction layer that abstractsaway\r\nfrom the peculiarities of target specific effectors,enabling a uniform\r\napproach to controlling and coordinatingthe low-level execution of\r\nreconfigurations, repairs,micro-reboots, etc (pdf) The Appearance of Human Skin Takanori Igarashi, Ko Nishino, Shree K. Nayar 2005-05-31 Skin is the outer most tissue of the human body. As a result, people\r\nare very aware of, and very sensitive to, the appearance of their\r\nskin. Consequently, skin appearance has been a subject of great\r\ninterest in various fields of science and technology. \r\nResearch on skin appearance has been\r\nintensely pursued in the fields of medicine, cosmetology, computer\r\ngraphics and computer vision.  Since the goals of these fields are\r\nvery different, each field has tended to focus on specific aspects of\r\nthe appearance of skin. The goal of this work is to present a\r\ncomprehensive survey that includes the most prominent results related\r\nto skin in these different fields and show how these seemingly\r\ndisconnected studies are related to one another. (pdf) Time-Varying Textures Sebastian Enrique, Melissa Koudelka, Peter Belhumeur, Julie Dorsey, Shree Nayar, Ravi Ramamoorthi 2005-05-25 Essentially all computer graphics rendering assumes that the\r\nreflectance and texture of surfaces is a static phenomenon.  Yet,\r\nthere is an abundance of materials in nature whose appearance varies\r\ndramatically with time, such as cracking paint, growing grass, or\r\nripening banana skins.  In this paper, we take a significant step\r\ntowards addressing this problem, investigating a new class of\r\ntime-varying textures.  We make three contributions.  First, we\r\ndescribe the carefully controlled acquisition of datasets of a variety\r\nof natural processes including the growth of grass, the accumulation\r\nof snow, and the oxidation of copper.  Second, we show how to adapt\r\nquilting-based methods to time-varying texture synthesis, addressing\r\nthe important challenges of maintaining temporal coherence, efficient\r\nsynthesis on large time-varying datasets, and reducing visual\r\nartifacts specific to time-varying textures.  Finally, we show how\r\nsimple procedural techniques can be used to control the evolution of\r\nthe results, such as allowing for a faster growth of grass in well lit\r\n(as opposed to shadowed) areas. (pdf) (ps) Merging Globally Rigid Formations of Mobile Autonomous Agents Tolga Eren, Brian Anderson, Walter Whiteley, Stephen Morse, Peter Belhumeur 2005-05-19 This paper is concerned with merging globally rigid formations\r\nof mobile autonomous agents. A key element in all\r\nfuture multi-agent systems will be the role of sensor and\r\ncommunication networks as an integral part of coordination.\r\nNetwork topologies are critically important for autonomous\r\nsystems involving mobile underwater, ground and\r\nair vehicles and for sensor networks. This paper focuses on\r\ndeveloping techniques and strategies for the analysis and\r\ndesign of sensor and network topologies required to merge\r\nglobally rigid formations for cooperative tasks. Central to\r\nthe development of these techniques and strategies will be\r\nthe use of tools from rigidity theory, and graph theory. (pdf) Optimal State-Free, Size-aware Dispatching for Heterogeneous $M/G/$-type systems Hanhua Fengand Vishal Misra, Dan Rubenstein, Dan Rubenstein 2005-05-04 We consider a cluster of heterogeneous servers, modeled as $M/G/1$ queues with different processing speeds. The scheduling policies for these servers can be either processor-sharing or first-come first-serve. Furthermore, a dispatcher that assigns jobs to the servers takes as input only the size of the arriving job and the overall job-size distribution.\nThis general model captures the behavior of a variety of real systems, such as web server clusters. Our goal is to identify assignment strategies that the dispatcher can perform to minimize expected completion time and waiting time. We show that there exist optimal strategies that are deterministic, fixing the server to which jobs of particular sizes are always sent. We prove that the optimal strategy for systems with identical servers assigns a non-overlapping interval range of job sizes to each server. We then prove that when server processing speeds differ, it is necessary to assign each server a distinct set of intervals of job sizes in order to minimize expected waiting or response times. We explore some of the practical challenges of identifying the optimal strategy, and also study a related problem that uses our model of how to provision server processing speeds to minimize waiting and completion time given a job size distribution and fixed aggregate processing power. (pdf) (ps) A Hybrid Approach to Topological Mobile Robot Localization Paul Blaer, Peter K. Allen 2005-04-27 We present a hybrid method for localizing a mobile robot in a complex environment. The method combines the use of multiresolution histograms with a signal strength analysis of existing wireless networks. We tested this localization procedure on the campus of Columbia University with our mobile robot, the Autonomous Vehicle for Exploration and Navigation of Urban Environments. Our results indicate that localization accuracy is significantly improved when five levels of resolution are used instead of one in color histogramming. We also find that incorporating wireless signal strengths into the method further improves reliability and helps to resolve ambiguities which arise when different regions have similar visual appearances. (pdf) Classical and Quantum Complexity of the Sturm-Liouville Eigenvalue Problem A. Papageorgiou, H. Wozniakowski 2005-04-22 We study the approximation of the smallest eigenvalue of a Sturm-Liouville problem in the classical and quantum settings. We consider a univariate Sturm-Liouville eigenvalue problem with a nonnegative function $q$ from the class $C^2([0,1])$ and study the minimal number $n(\\e)$ of function evaluations or queries that are necessary to compute an $\\e$-approximation of the smallest eigenvalue. We prove that $n(\\e)=\\Theta(\\e^{-1/2})$ in the (deterministic) worst case setting, and $n(\\e)=\\Theta(\\e^{-2/5})$ in the randomized setting. The quantum setting offers a polynomial speedup with {\\it bit} queries and an exponential speedup with {\\it power} queries. Bit queries are similar to the oracle calls used in Grover's algorithm appropriately extended to real valued functions. Power queries are used for a number of problems including phase estimation. They are obtained by considering the propagator of the discretized system at a number of different time moments. They allow us to use powers of the unitary matrix $\\exp(\\tfrac12 {\\rm i}M)$, where $M$ is an $n\\times n$ matrix obtained from the standard discretization of the Sturm-Liouville differential operator. The quantum implementation of power queries by a number of elementary quantum gates that is polylog in $n$ is an open issue. We study the approximation of the smallest eigenvalue of a Sturm-Liouville problem in the classical and quantum settings. We consider a univariate Sturm-Liouville eigenvalue problem with a nonnegative function $q$ from the class $C^2([0,1])$ and study the minimal number $n(\\e)$ of function evaluations or queries that are necessary to compute an $\\e$-approximation of the smallest eigenvalue. We prove that $n(\\e)=\\Theta(\\e^{-1/2})$ in the (deterministic) worst case setting, and $n(\\e)=\\Theta(\\e^{-2/5})$ in the randomized setting. The quantum setting offers a polynomial speedup with {\\it bit} queries and an exponential speedup with {\\it power} queries. Bit queries are similar to the oracle calls used in Grover's algorithm appropriately extended to real valued functions. Power queries are used for a number of problems including phase estimation. They are obtained by considering the propagator of the discretized system at a number of different time moments. They allow us to use powers of the unitary matrix $\\exp(\\tfrac12 {\\rm i}M)$, where $M$ is an $n\\times n$ matrix obtained from the standard discretization of the Sturm-Liouville differential operator. The quantum implementation of power queries by a number of elementary quantum gates that is polylog in $n$ is an open issue. (pdf) (ps) Improving Database Performance on Simultaneous Multithreading Processors Jingren Zhou, John Cieslewicz, Kenneth A. Ross, Mihir Shah 2005-04-18 Simultaneous multithreading (SMT) allows multiple threads to supply instructions to the instruction pipeline of a superscalar processor. Because threads share processor resources, an SMT system is inherently different from a multiprocessor system and, therefore, utilizing multiple threads on an SMT processor creates new challenges for database implementers.\nWe investigate three thread-based techniques to exploit SMT architectures on memory-resident data. First, we consider running independent operations in separate threads, a technique applied to conventional multiprocessor systems. Second, we describe a novel implementation strategy in which individual operators are implemented in a multi-threaded fashion. Finally, we introduce a new data-structure called a work-ahead set that allows us to use one of the threads to aggressively preload data into the cache for use by the other thread.\nWe evaluate each method with respect to its performance, implementation complexity, and other measures. We also provide guidance regarding when and how to best utilize the various threading techniques. Our experimental results show that by taking advantage of SMT technology we achieve a 30\\% to 70\\% improvement in throughput over single threaded implementations on in-memory database operations. (pdf) Quantum algorithms and complexity for certain continuous and related discrete problems Marek Kwas 2005-04-14 The thesis contains an analysis of two computational problems. The first problem is discrete quantum Boolean summation. This problem is a building block of quantum algorithms for many continuous problems, such as integration, approximation, di®erential equations and path integration. The second problem is continuous multivariate\r\nFeynman-Kac path integration, which is a special case of path integration. \r\n\r\nThe quantum Boolean summation problem can be solved by the quantum summation (QS) algorithm of Brassard, Høyer, Mosca and Tapp, which approximates the arithmetic mean of a Boolean function. We improve the error bound of Brassard et al. for the worst-probabilistic setting. Our error bound is sharp. We also present new sharp error bounds in the average-probabilistic and worst-average settings. Our average-probabilistic error bounds prove the optimality of the QS algorithm for a certain choice of its parameters. The study of the worst-average error shows that the QS algorithm is not optimal in this setting; we need to use a certain number of repetitions to regain its optimality.\r\n\r\nThe multivariate Feynman-Kac path integration problem for smooth multivariate functions su®ers from the provable curse of dimensionality in the worst-case deterministic setting, i.e., the minimal number of function evaluations needed to compute an approximation depends exponentially on the number of variables. We show that in\r\nboth the randomized and quantum settings the curse of dimensionality is vanquished, i.e., the minimal number of function evaluations and/or quantum queries required to compute an approximation depends only polynomially on the reciprocal of the desired accuracy and has a bound independent of the number of variables. The exponents\r\nof these polynomials are 2 in the randomized setting and 1 in the quantum setting. These exponents can be lowered at the expense of the dependence on the number of variables. Hence, the quantum setting yields exponential speedup over the worst-case\r\ndeterministic setting, and quadratic speedup over the randomized setting. (pdf) A Hybrid Hierarchical and Peer-to-Peer Ontology-based Global Service Discovery System Knarig Arabshian, Henning Schulzrinne 2005-04-06 Current service discovery systems fail to span across the globe and they use simple attribute-value pair or interface matching for service description and querying. We propose a global service discovery system, GloServ, that uses the description logic Web Ontology Language (OWL DL). The GloServ architecture spans both local and wide area networks. It maps knowledge obtained by the service classification ontology to a structured peer-to-peer network such as a Content Addressable Network (CAN). GloServ also performs automated and intelligent registration and querying by exploiting the logical relationships within the service ontologies. (pdf) (ps) Multi-Language Edit-and-Continue for the Masses Marc Eaddy, Steven Feiner 2005-04-05 We present an Edit-and-Continue implementation that allows regular source files to be treated like interactively updatable, compiled scripts, coupling the speed of compiled na-tive machine code, with the ability to make changes without restarting. Our implementa-tion is based on the Microsoft .NET Framework and allows applications written in any .NET language to be dynamically updatable. Our solution works with the standard ver-sion of the Microsoft Common Language Runtime, and does not require a custom com-piler or runtime. Because no application changes are needed, it is transparent to the appli-cation developer. The runtime overhead of our implementation is low enough to support updating real-time applications (e.g., interactive 3D graphics applications). (pdf) (ps) Similarity-based Multilingual Multi-Document Summarization David Kirk Evans,  Kathleen McKeown,  Judi, Kathleen McKeown, Judith L. Klavans 2005-03-31 We present a new approach for summarizing clusters of documents on the same event, some of which are machine translations of foreign-language documents and some of which are English. Our approach to multilingual multi-document summarization uses text similarity to choose sentences from English documents based on the content of the machine translated documents. A manual evaluation shows that 68\\% of the sentence replacements improve the summary, and the overall summarization approach outperforms first-sentence extraction baselines in automatic ROUGE-based evaluations. (pdf) (ps) 802.11b Throughput with Link Interference Hoon Chang, Vishal Misra 2005-03-29 IEEE 802.11 MAC is a CSMA/CA protocol and uses RTS/CTS exchanges to avoid the hidden terminal problem. Recent findings have revealed that the carriersensing range set in current major implementations does not detect and prevent all interference signals even using RTS/CTS access method together. In this paper, we investigate the effect of interference and develop a mathematical model for it. We demonstrate that the 802.11 DCF does not properly act on the interference channel due to the small size and the exponential increment of backoff windows. The accuracy of our model is verified via simulations. Based on an insight from our model, we present a simple protocol that operates on the top of 802.11 MAC layer and achieves more throughput than rate-adjustment schemes. (pdf) (ps) A Lower Bound for Quantum Phase Estimation Arvid J. Bessen 2005-03-22 We obtain a query lower bound for quantum algorithms solving the phase estimation problem. Our analysis generalizes existing lower bound approaches to the case where the oracle Q is given by controlled powers Q^p of Q, as it is for example in Shor's order finding algorithm. In this setting we will prove a log (1/epsilon) lower bound for the number of applications of Q^p1, Q^p2, ... This bound is tight due to a matching upper bound. We obtain the lower bound using a new technique based on frequency analysis. (pdf) (ps) The Power of Various Real-Valued Quantum Queries Arvid J. Bessen 2005-03-22 The computation of combinatorial and numerical problems on quantum computers is often much faster than on a classical computer in numbers of queries. A query is a procedure by which the quantum computer gains information about the specific problem. Different query definitions were given and our aim is to review them and to show that these definitions are not equivalent. To achieve this result we will study the simulation and approximation of one query type by another. While approximation is easy in one direction, we will show that it is hard in the other direction by a lower bound for the numbers of queries needed in the simulation. The main tool in this lower bound proof is a relationship between quantum algorithms and trigonometric polynomials that we will establish. (pdf) (ps) Rigid Formations with Leader-Follower Architecture Tolga Eren, Walter Whiteley, Peter N. Belhumeur 2005-03-14 This paper is concerned with information\r\nstructures used in rigid formations of autonomous agents that\r\nhave leader-follower architecture. The focus of this paper is\r\non sensor/network topologies to secure control of rigidity. We\r\nextend our previous approach for formations with symmetric\r\nneighbor relations to include formations with leader-follower\r\narchitecture. Necessary and sufficient conditions for stably\r\nrigid directed formations are given including both cyclic and\r\nacyclic directed formations. Some useful steps for creating\r\ntopologies of directed rigid formations are developed. An\r\nalgorithm to determine the directions of links to create\r\nstably rigid directed formations from rigid undirected\r\nformations is presented. It is shown that k-cycles (k > 2) do\r\nnot cause inconsistencies when measurements are noisy, while\r\n2-cycles do. Simulation results are presented for (i) a rigid\r\nacyclic formation, (i) a flexible formation, and (iii) a rigid\r\nformation with cycles. (pdf) (ps) P2P Video Synchronization in a Collaborative Virtual Environment Suhit Gupta, Gail Kaiser 2005-02-25 We have previously developed a collaborative virtual environment (CVE) for small-group virtual classrooms, intended for distance learning by geographically dispersed students. The CVE employs a peer-to-peer approach to the frequent real-time updates to the 3D virtual worlds required by avatar movements (fellow students in the same room are depicted by avatars). This paper focuses on our extension to the P2P model to support group viewing of lecture videos, called VECTORS, for Video Enhanced Collaboration for Team Oriented Remote Synchronization. VECTORS supports synchronized viewing of lecture videos, so the students all see \"the same thing at the same time\", and can pause, rewind, etc. in synchrony while discussing the lecture material via \"chat\". We are particularly concerned with the needs of the technologically disenfranchised, e.g., whose only Web/Internet access if via dialup or other relatively low-bandwidth networking. Thus VECTORS employs semantically compressed videos with meager bandwidth requirements. Further, the videos are displayed as a sequence of JPEGs on the walls of a 3D virtual room, requiring fewer local multimedia resources than full motion MPEGs. (pdf) A Study on NSIS Interaction with Internet Route Changes Charles Shen, Henning Schulzrinne, Sung-Hyuck Lee, Jong Ho Bang 2005-02-24 Design of Next Step In Signaling (NSIS) protocol and IP routing interaction requires a good understanding of today's Internet routing behavior. In this report we present a routing measurement experiment to characterize current Internet dynamics, including routing pathology, routing prevalence and routing persistence. The focus of our study is route change. We look at the types, duration and likely causes of different route changes and discuss their impact to the design of NSIS. We also review common route change detection methods and investigate rules to determine whether a route change happened in a node's forward-looking or backward-looking direction is detectable. We introduce typical NSIS deployment models and discuss specific categories of route changes that should be considered in each of these models. With the NSIS deployment models in mind, we further give experimental evaluation of two route change detection methods - the packet TTL monitoring method and a new delay variation monitoring method. (pdf) (ps) Adding Self-healing capabilities to the Common Language Runtime Rean Griffith, Gail Kaiser 2005-02-23 Self-healing systems require that repair\tmechanisms are available to resolve problems that arise while\tthe system executes. Managed execution environments such as\tthe Common Language Runtime (CLR) and Java Virtual Machine\t(JVM) provide a number of application services (application\tisolation, security sandboxing, garbage collection and\tstructured exception handling) which are geared primarily at\tmaking managed applications more robust. However, none of\tthese services directly enables applications to perform\trepairs or consistency checks of their components. From a\tdesign and implementation standpoint, the preferred way to\tenable repair in a self-healing system is to use an\texternalized repair/adaptation architecture rather than\thardwiring adaptation logic inside the system where it is\tharder to analyze, reuse and extend. We present a framework\tthat allows a repair engine to dynamically attach and detach\tto/from a managed application while it executes essentially\tadding repair mechanisms as another application service\tprovided in the execution environment. (pdf) (ps) Manipulating Managed Execution Runtimes to Support Self-Healing Systems Rean Griffith, Gail Kaiser 2005-02-23 Self-healing systems require that repair mechanisms are available to resolve problems that arise while the system executes. Managed execution environments such as the Common Language Runtime (CLR) and Java Virtual Machine (JVM) provide a number of application services (application isolation, security sandboxing, garbage collection and structured exception handling) which are geared primarily at making managed applications more robust. However, none of these services directly enables applications to perform repairs or consistency checks of their components. From a design and implementation standpoint, the preferred way to enable repair in a self-healing system is to use an externalized repair/adaptation architecture rather than hardwiring adaptation logic inside the system where it is harder to analyze, reuse and extend. We present a framework that allows a repair engine to dynamically attach and detach to/from a managed application while it executes essentially adding repair mechanisms as another application service provided in the execution environment. (pdf) (ps) Genre Classification of Websites Using Search Engine Snippets Suhit Gupta, Gail Kaiser, Salvatore Stolfo, Hila Becker 2005-02-03 Web pages often contain clutter (such as ads, unnecessary images and extraneous links) around the body of an article, which distracts a user from actual content. Automatic extraction of \"useful and relevant\" content from web pages has many applications, including browsing on small cell phone and PDA screens, speech rendering for the visually impaired, and reducing noise for information retrieval systems. Prior work has led to the development of Crunch, a framework which employs various heuristics in the form of filters and filter settings for content extraction. Crunch allows users to tune these settings, essentially the thresholds for applying each filter. However, in order to reduce human involvement in selecting these heuristic settings, we have extended this work to utilize a website's classification, defined by its genre and physical layout. In particular, Crunch would then obtain the settings for a previously unknown website by automatically classifying it as sufficiently similar to a cluster of known websites with previously adjusted settings - which in practice produces better content extraction results than a single one-size-fits-all set of setting defaults. In this paper, we present our approach to clustering a large corpus of websites by their genre, utilizing the snippets generated by sending the website's domain name to search engines as well as the website's own text. We find that exploiting these snippets not only increased the frequency of function words that directly assist in detecting the genre of a website, but also allow for easier clustering of websites. We use existing techniques like Manhattan distance measure and Hierarchical clustering, with some modifications, to pre-classify websites into genres. Our clustering method does not require prior knowledge of the set of genres that websites fit into, but instead discovers these relationships among websites. Subsequently, we are able to classify newly encountered websites in linear-time, and then apply the corresponding filter settings, with no noticeable delay introduced for the content-extracting web proxy. (pdf) (ps) A Uniform Programming Abstraction for Effecting Autonomic Adaptations onto Software Systems Giuseppe Valetto, Gail Kaiser 2005-01-30 Most general-purpose work towards autonomic or self-managing systems has emphasized the front end of the feedback control loop, with some also concerned with controlling the back end enactment of runtime adaptations ^V but usually employing an effector technology peculiar to one type of target system. While completely generic ^Sone size fits all^T effector technologies seem implausible, we propose a general-purpose programming model and interaction layer that abstracts away from the peculiarities of target-specific effectors, enabling a uniform approach to controlling and coordinating the low-level execution of reconfigurations, repairs, micro-reboots, etc. (pdf) Dynamic Adaptation of Rules for Temporal Event Correlation in Distributed Systems Rean Griffith, Joseph L. Hellerstein, Yixin Diao, Gail Kaiser 2005-01-30 Event correlation is essential to realizing self-managing distributed systems. For example, distributed systems often require that events be correlated from multiple systems using temporal patterns to detect denial of service attacks and to warn of problems with business critical applications that run on multiple servers. This paper addresses how to specify timer values for temporal patterns so as to manage the trade-off between false alarms and undetected alarms. A central concern is addressing the variability of event propagation delays due to factors such as contention for network and server resources. To this end, we develop an architecture and an adaptive control algorithm that dynamically compensate for variations in propagation delays. Our approach makes Management Stations more autonomic by avoiding the need for manual adjustments of timer values in temporal rules. Further, studies we conducted of a testbed system suggest that our approach produces results that are at least as good as an optimal fixed setting of timer values. (pdf) The Virtual Device: Expanding Wireless Communication Services Through Service Discovery and Session Mobility Ron Shacham, Henning Schulzrinne, Srisakul Thakolsri, Wolfgang Kellerer 2005-01-12 We present a location-based, ubiquitous service architecture, based on the Session Initiation Protocol (SIP) and a service discovery protocol that enables users to enhance the multimedia communications services available on their mobile devices by discovering other local devices, and including them in their active sessions, creating a \"virtual device.\" We have implemented our concept based on Columbia University's multimedia environment and we show its feasibility by a performance analysis. (pdf) (ps) Autonomic Control for Quality Collaborative Video Viewing Dan Phung, Giuseppe Valetto, Gail Kaiser 2004-12-31 We present an autonomic controller for quality collaborative video viewing, which allows groups of geographically dispersed users with different network and computer resources to view a video in synchrony while optimizing the video quality experienced. The autonomic controller is used within a tool for enhancing distance learning with synchronous group review of online multimedia material. The autonomic controller monitors video state at the clients' end, and adapts the quality of the video according to the resources of each client in (soft) real time. Experimental results show that the autonomic controller successfully synchronizes video for small groups of distributed clients and, at the same time, enhances the video quality experienced by users, in conditions of fluctuating bandwidth and variable frame rate. (pdf) Sequential Challenges in Synthesizing Esterel Cristian Soviani, Jia Zeng, Stephen A. Edwards 2004-12-20 State assignment is a formidable task. As designs written in a hardware description language such as Esterel inherently carry more high level information that a register transfer level model, such information can be used to guide the encoding process. A question arises if the high level information alone is strong enough to suggest an efficient state assignment, allowing low-level details to be ignored.\nThis report suggests that with Esterel's flexibility, most optimization potential is not within the high-level structure. It appears effective state assignment cannot rely solely on high level information. (pdf) (ps) Determining Interfaces using Type Inference Stephen A. Edwards, Chun Li 2004-12-20 Porting software usually requires understanding what library functions the program being ported uses since this functionality must be either found or reproduced in the ported program's new environment. This is usually done manually through code inspections. We propose a type inference algorithm able to infer basic information about the library functions a particular C program uses in the absence of declaration information for the library (e.g., without header files). Based on a simple but efficient inference algorithm, we were able to infer declarations for much of the PalmOS API from the source of a twenty-seven-thousand-line C program. Such a tool will aid in the problem of program understanding when porting programs, especially from poorly-documented or lost legacy environments. (pdf) (ps) Remotely Keyed CryptoGraphics - Secure Remote Display Access Using (Mostly) Untrusted Hardware  - Extended Version Debra L. Cook, Ricardo Baratto, Angelos D. Keromytis 2004-12-11 Software that covertly monitors user actions, also known as {\\it\r\nspyware,} has become a first-level security threat due to its ubiquity\r\nand the difficulty of detecting and removing it. Such software may be\r\ninadvertently installed by a user that is casually browsing the web,\r\nor may be purposely installed by an attacker or even the owner of a\r\nsystem. This is particularly problematic in the case of utility\r\ncomputing, early manifestations of which are Internet cafes and\r\nthin-client computing. Traditional trusted computing approaches offer\r\na partial solution to this by significantly increasing the size of the\r\ntrusted computing base (TCB) to include the operating system and other\r\nsoftware.\r\n\r\nWe examine the problem of protecting a user accessing specific\r\nservices in such an environment. We focus on secure video broadcasts\r\nand remote desktop access when using any convenient, and often\r\nuntrusted, terminal as two example applications.  We posit that, at\r\nleast for such applications, the TCB can be confined to a suitably\r\nmodified graphics processing unit (GPU). Specifically, to prevent\r\nspyware on untrusted clients from accessing the user's data, we\r\nrestrict the boundary of trust to the client's GPU by moving image\r\ndecryption into GPUs. We use the GPU in order to leverage existing\r\ncapabilities as opposed to designing a new component from scratch.\r\nWe discuss the applicability of GPU-based\r\ndecryption in these two sample scenarios and identify the limitations\r\nof the current generation of GPUs. We propose straightforward\r\nmodifications to future GPUs that will allow the realization of the\r\nfull approach. (pdf) (ps) Obstacle Avoidance and Path Planning Using a Sparse Array of Sonars Matei Ciocarlie 2004-12-08 This paper proposes an exploration method for robots equipped with a set of sonar sensors that does not allow for complete coverage of the robot's close surroundings. In such cases, there is a high risk of collision with possible undetected obstacles. The proposed method, adapted for use in urban outdoors environments, minimizes such risks while guiding the robot towards a predefined target location. During the process, a compact and accurate representation of the environment can be obtained. (pdf) End System Service Examples Xiaotao Wu, Henning Schulzrinne 2004-12-07 This technical report investigates services suitable for end systems. We look into ITU Q.1211 services, AT&T 5ESS switch services, services defined in CSTA Phase III, and new services integrating other Internet services, such as presence information. We also explore how to use the Language for End System Services (LESS) to program the services. (pdf) (ps) WebPod: Persistent Web Browsing Sessions with Pocketable Storage Devices Shaya Potter, Jason Nieh 2004-11-19 We present WebPod, a portable device for managing web browsing sessions. WebPod leverages capacity improvements in portable solid state memory devices to provide a consistent environment to access the web. WebPod provides a thin virtualization layer that decouples a user's web session from any particular end-user device, allowing users freedom to move their work environments around. We have implemented a prototype in Linux that works with existing unmodified applications and operating system kernels. Our experimental results demonstrate that WebPod has very low virtualization overhead and can provide a full featured web browsing experience, including support for all helper applications and plug-ins one expects. WebPod is able to efficiently migrate a user's web session. This enables improved user mobility while maintaining a consistent work environment. (pdf) Design and Verification Languages Stephen A. Edwards 2004-11-17 After a few decades of research and experimentation, register-transfer dialects of two standard languages---Verilog and VHDL---have emerged as the industry standard starting point for automatic large-scale digital integrated circuit synthesis. Writing RTL descriptions of hardware remains a largely human process and hence the clarity, precision, and ease with which such descriptions can be coded correctly has a profound impact on the quality of the final product and the speed with which the design can be created.\nWhile the efficiency of a design (e.g., the speed at which it can run or the power it consumes) is obviously important, its correctness is usually the paramount issue, consuming the majority of the time (and hence money) spent during the design process. In response to this challenge, a number of so-called verification languages have arisen. These have been designed to assist in a simulation-based or formal verification process by providing mechanisms for checking temporal properties, generating pseudorandom test cases, and for checking how much of a design's behavior has been exercised by the test cases.\nThrough examples and discussion, this report describes the two main design languages---VHDL and Verilog---as well as SystemC, a language currently used to build large simulation models; SystemVerilog, a substantial extension of Verilog; and OpenVera, e, and PSL, the three leading contenders for becoming the main verification language. (pdf) (ps) Extracting Context To Improve Accuracy For HTML Content Extraction Suhit Gupta, Gail Kaiser, Salvatore Stolfo 2004-11-08 Web pages contain clutter (such as ads, unnecessary images and extraneous links) around the body of an article, which distracts a user from actual content. Extraction of \"useful and relevant\" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, reducing noise for information retrieval systems and to generally improve the web browsing experience. In our previous work [16], we developed a framework that employed an easily extensible set of techniques that incorporated results from our earlier work on content extraction [16]. Our insight was to work with DOM trees, rather than raw HTML markup. We present here filters that reduce human involvement in applying heuristic settings for websites and instead automate the job by detecting and utilizing the physical layout and content genre of a given website. We also present work we have done towards improving the usability and performance of our content extraction proxy as well as the quality and accuracy of the heuristics that act as filters for inferring the context of a webpage. (pdf) (ps) Peer-to-Peer Internet Telephony using SIP Kundan Singh, Henning Schulzrinne 2004-10-31 P2P systems inherently have high scalability, robustness and fault tolerance because there is no centralized server and the network self-organizes itself. This is achieved at the cost of higher latency for locating the resources of interest in the P2P overlay network. Internet telephony can be viewed as an application of P2P architecture where the participants form a self-organizing P2P overlay network to locate and communicate with other participants. We propose a pure P2P architecture for the Session Initiation Protocol (SIP)-based IP telephony systems. Our P2P-SIP architecture supports basic user registration and call setup as well as advanced services such as offline message delivery, voice/video mails and multi-party conferencing. We also provide an overview of practical challenges for P2P-SIP such as firewall, Network Address Translator (NAT) traversal and security. (pdf) (ps) Service Learning in Internet Telephony Xiaotao Wu, Henning Schulzrinne 2004-10-29 Internet telephony can introduce many novel communication services, however, novelty puts learning burden on users. It will be a great help to users if their desired services can be created automatically. We developed an intelligent communication service creation environment which can handle automatic service creation by learning from users' daily communication behaviors. The service creation environment models communication services as decision trees and uses the Incremental Tree Induction (ITI) algorithm for decision tree learning. We use Language for End System Services (LESS) scripts to represent learned results and implemented a simulation environment to verify the learning algorithm. We also noticed that when users get their desired services, they may not be aware of unexpected behaviors that the serivces could introduce, for example, mistakenly rejecting expected calls. In this paper, we also did a comprehensive analysis on communication service fail-safe handling and propose several approaches to create fail-safe services. (pdf) (ps) A Microrobotic System For Protein Streak Seeding Atanas Georgiev, Peter K. Allen, Ting Song, Andrew Laine, William Edstrom, John Hunt 2004-10-28 We present a microrobotic system for protein crystal micromanipulation\r\ntasks.  The focus in this report is on a task called streak seeding,\r\nwhich is used by crystallographers to entice certain protein crystals\r\nto grow.  Our system features a set of custom designed micropositioner\r\nend-effectors we call microshovels to replace traditional tools used\r\nby crystallographers for this task.  We have used micro-electrical\r\nmechanical system (MEMS) techniques to design and manufacture various\r\nshapes and quantities of microshovels.  Visual feedback from a camera\r\nmounted on the microscope is used to control the micropositioner as it\r\nlowers a microshovel into the liquid containing the crystals for poking\r\nand streaking.  We present experimental results that illustrate the\r\napplicability of our approach. (pdf) Preventing Spam For SIP-based Instant Messages and Sessions Kumar Srivastava, Henning Schulzrinne 2004-10-28 As IP telephony becomes more widely deployed and used, tele-marketers or other spammers are bound to start using SIP-based calls and instant messages as a medium for sending spam. As is evident from the fate of email, protection against spam has to be built into SIP systems otherwise they are bound to fall prey to spam. Traditional approaches used to prevent spam in email such as content-based filtering and access lists are not applicable to SIP calls and instant messages in their present form. We propose Domain-based Authentication and Policy-Enforced for SIP (DAPES): a system that can be easily implemented and deployed in existing SIP networks. Our system is capable of determining in real time, whether an incoming call or instant message is likely to be spam or not, while at the same time, supporting communication between both known and unknown parties. DAPES includes the deployment of reputation systems in SIP networks to enable real-time transfer of reputation information between parties to allow communication between entities unknown to each other. (pdf) (ps) Programmable Conference Server Henning Schulzrinne, Kundan Singh, Xiaotao Wu 2004-10-15 Conferencing services for Internet telephony and multimedia can be enhanced by the integration of other Internet services, such as instant messaging, presence notification, directory lookups, location sensing, email and web. These services require a service programming architecture that can easily incorporate new Internet services into the existing conferencing functionalities, such as voice-enabled conference control. W3C has defined the Call Control eXtensible Markup Language (CCXML), along with its VoiceXML, for telephony call control services in a point-to-point call. However, it cannot handle other Internet service events such as presence enabled conferences. In this paper, we propose an architecture combining VoiceXML with our Language for End System Services (LESS) and the Common Gateway Interface (CGI) for multi-party conference service programming that integrates existing Internet services. VoiceXML provides the voice interface to LESS and CGI scripts. Our architecture enables many novel services such as conference setup based on participant location and presence status. We give some examples of the new services and describe our on-going implementation. (pdf) (ps) An Analysis of the Skype Peer-to-Peer Internet Telephony Protocol Salman A. Baset, Henning Schulzrinne 2004-10-11 Skype is a peer-to-peer VoIP client developed by KaZaa in 2003. Skype claims that it can work almost seamlessly across NATs and firewalls and has better voice quality than the MSN and Yahoo IM applications. It encrypts calls end-to-end, and stores user information in a decentralized fashion. Skype also supports instant messaging and conferencing. This report analyzes key Skype functions such as login, NAT and firewall traversal, call establishment, media transfer, codecs, and conferencing under three different network setups. Analysis is performed by careful study of Skype network traffic. (pdf) (ps) Building a Reactive Immune System for Software Services Stelios Sidiroglou, Michael E. Locasto, Stephen W. Boyd, Angelos D. Keromytis 2004-10-10 We propose a new approach for reacting to a wide variety of software failures, ranging from remotely exploitable vulnerabilities to more mundane bugs that cause abnormal program termination (e.g., illegal memory dereference). Our emphasis is in creating \"self-healing\" software that can protect itself against a recurring fault until a more comprehensive fix is applied.\nOur system consists of a set of sensors that monitor applications for various types of failure and an instruction-level emulator that is invoked for selected parts of a program's code. Use of such an emulator allows us to predict recurrences of faults, and recover program execution to a safe control flow. Using the emulator for small pieces of code, as directed by the sensors, allows us to minimize the performance impact on the immunized application.\nWe discuss the overall system architecture and a prototype implementation for the x86 platform. We evaluate the efficacy of our approach against a range of attacks and other software failures and investigate its performance impact on several server-type applications. We conclude that our system is effective in preventing the recurrence of a wide variety of software failures at a small performance cost. (pdf) (ps) Live CD Cluster Performance Haronil Estevez, Stephen A. Edwards 2004-10-04 In this paper, we present a performance comparison of two linux live CD distributions, Knoppix (v.3.3) and Quantian (v 0.4.96). The library used for performance evaluation is the Parallel Image Processing Toolkit (PIPT), a software library that contains several parallel image processing routines. A set of images was chosen and a batch job of PIPT routines were run and timed using both live CD distributions. The point of comparison between the two live CDs was the total time the batch job required for completion. (pdf) (ps) Information Structures to Secure Control of Rigid Formations with Leader-Follower Structure Tolga Eren, Walter Whiteley, Brian D.O. Anderson, A. Stephen Morse, Peter N. Belhumeur 2004-09-29 This paper is concerned with rigid formations of mobile autonomous agents using a leader-follower structure. A formation is a group of agents moving in real 2- or 3- dimensional space. A formation is called rigid if the distance between each pair of agents does not change over time under ideal conditions. Sensing/communication links between agents are used to maintain a rigid formation. Two agents connected by a sensing/communication link are called neighbors. There are two types of neighbor relations in rigid formations. In the first type, the neighbor relation is symmetric. In the second type, the neighbor relation is asymmetric. Rigid formations with a leader-follower structure have the asymmetric neighbor relation. A framework to analyze rigid formations with symmetric neighbor relations is given in our previous work. This paper suggests an approach to analyze rigid formations that have a leader-follower structure. (pdf) (ps) An Investigation Into the Detection of New Information Barry Schiffman, Kathleen R. McKeown 2004-09-29 This paper explores new-information detection, describing a strategy for filtering a stream of documents to present only information that is fresh. We focus on multi-document summarization and seek to efficiently use more linguistic information than is often seen in such systems. We experimented with our linguistic system and with a more traditional sentence-based, vector-space system and found that a combination of the two approaches boosted performance over each one alone. (pdf) (ps) Machine Learning and Text Segmentation in Novelty Detection Barry Schiffman, Kathleen R. McKeown 2004-09-29 This paper explores a combination of machine learning, approximate text segmentation and a vector-space model to distinguish novel information from repeated information. In experiments with the data from the Novelty Track at the Text Retrieval Conference, we show improvements over a variety of approaches, in particular in raising precision scores on this data, while maintaining a reasonable amount of recall. (pdf) (ps) Voice over TCP and UDP Xiaotang Zhang, Henning Schulzrinne 2004-09-28 We compare UDP and TCP when transmitting voice data using PlanetLab where we can do experiments globally. For TCP, we also do experiments using TCP NODELAY which sends out requests immediately. We compare the performance of different protocols by their 90th percentile delay and jitter. The performance of UDP is better than that of TCP NODELAY and the performance TCP NODELAY is better than that of TCP. We also explore the relation between TCP delay time minus the transmission time and the packet loss rate and find there is a linear relationship between them. (pdf) (ps) Using Execution Transactions To Recover From Buffer Overflow Attacks Stelios Sidiroglou, Angelos D. Keromytis 2004-09-13 We examine the problem of containing buffer overflow attacks in a safe and efficient manner. Briefly, we automatically augment source code to dynamically catch stack and heap-based buffer overflow and underflow attacks, and recover from them by allowing the program to continue execution. Our hypothesis is that we can treat each code function as a transaction that can be aborted when an attack is detected, without affecting the application's ability to correctly execute. Our approach allows us to selectively enable or disable components of this defensive mechanism in response to external events, allowing for a direct tradeoff between security and performance. We combine our defensive mechanism with a honeypot-like con guration to detect previously unknown attacks and automatically adapt an application's defensive posture at a negligible performance cost, as well as help determine a worm's signature. The main benefits of our scheme are its low impact on application performance, its ability to respond to attacks without human intervention, its capacity to handle previously unknown vulnerabilities, and the preservation of service availability. We implemented a stand-alone tool, DYBOC, which we use to instrument a number of vulnerable applications. Our performance benchmarks indicate a slow-down of 20% for Apache in full-protection mode, and 1.2% with partial protection. We validate our transactional hypothesis via two experiments: first, by applying our scheme to 17 vulnerable applications, successfully fixing 14 of them; second, by examining the behavior of Apache when each of 154 potentially vulnerable routines are made to fail, resulting in correct behavior in 139 of cases. (pdf) (ps) A Theoretical Analysis of the Conditions for Unambiguous Node Localization in Sensor Networks Tolga Eren, Walter Whiteley, Peter N. Belhumeur 2004-09-13 In this paper we provide a theoretical foundation for the problem of network localization in which some nodes know their locations and other nodes determine their locations by measuring distances or bearings to their neighbors. Distance information is the separation between two nodes connected by a sensing/communication link. Bearing is the angle between a sensing/communication link and the x-axis of a node's local coordinate system. We construct grounded graphs to model network localization and apply graph rigidity theory and parallel drawings to test the conditions for unique localizability and to construct uniquely localizable networks. We further investigate partially localizable networks. (pdf) (ps) Modeling and Managing Content Changes in Text Databases Panagiotis G. Ipeirotis, Alexandros Ntoulas, Junghoo Cho, Luis Gravano 2004-08-12 Large amounts of (often valuable) information are stored in web-accessible text databases. ``Metasearchers'' provide unified interfaces to query multiple such databases at once. For efficiency, metasearchers rely on succinct statistical summaries of the database contents to select the best databases for each query. So far, database selection research has largely assumed that databases are static, so the associated statistical summaries do not need to change over time. However, databases are rarely static and the statistical summaries that describe their contents need to be updated periodically to reflect content changes. In this paper, we first report the results of a study showing how the content summaries of 152 real web databases evolved over a period of 52 weeks. Then, we show how to use ``survival analysis'' techniques in general, and Cox's proportional hazards regression in particular, to model database changes over time and predict when we should update each content summary. Finally, we exploit our change model to devise update schedules that keep the summaries up to date by contacting databases only when needed, and then we evaluate the quality of our schedules experimentally over real web databases. (pdf) (ps) Cross-Dimensional Gestural Interaction Techniques for Hybrid Immersive Environments Hrvoje Benko, Edward W. Ishak, Steven Feiner 2004-08-09 We present a set of interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one- and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We also address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. (pdf) (ps) Group Ratio Round-Robin: O(1) Proportional Share Scheduling for Uniprocessor and Multiprocessor Systems Bogdan Caprita, Wong Chun Chan, Jason Nieh, Clifford Stein, Haoqiang Zheng 2004-07-30 Proportional share resource management provides a\tflexible and useful abstraction for multiplexing time-shared\tresources. We present Group Ratio Round-Robin ($GR^3$), the\tfirst proportional share scheduler that combines accurate\tproportional fairness scheduling behavior with $O(1)$\tscheduling overhead on both uniprocessor and multiprocessor\tsystems. $GR^3$ uses a novel client grouping strategy to\torganize clients into groups of similar processor allocations\twhich can be more easily scheduled. Using this grouping\tstrategy, $GR^3$ combines the benefits of low overhead\tround-robin execution with a novel ratio-based scheduling\talgorithm. $GR^3$ can provide fairness within a constant\tfactor of the ideal generalized processor sharing model for\tclient weights with a fixed upper bound and preserves its\tfairness properties on multiprocessor systems. We have\timplemented $GR^3$ in Linux and measured its performance\tagainst other schedulers commonly used in research and\tpractice, including the standard Linux scheduler, Weighted\tFair Queueing, Virtual-Time Round-Robin, and Smoothed\tRound-Robin. Our experimental results demonstrate that $GR^3$\tcan provide much lower scheduling overhead and much better\tscheduling accuracy in practice than these other approaches. (pdf) THINC: A Remote Display Architecture for Thin-Client Computing Ricardo A. Baratto, Jason Nieh, Leo Kim 2004-07-29 Rapid improvements in network bandwidth, cost, and ubiquity combined with the security hazards and high total cost of ownership of personal computers have created a growing market for thin-client computing. We introduce THINC, a remote display system architecture for high-performance thin-client computing in both LAN and WAN environments. THINC transparently maps high-level application display calls to a few simple low-level commands which can be implemented easily and efficiently. THINC introduces a number of novel latency-sensitive optimization techniques, including offscreen drawing awareness, command buffering and scheduling, non-blocking display operation, native video support, and server-side screen scaling. We have implemented THINC in an XFree86/Linux environment and compared its performance with other popular approaches, including Citrix MetaFrame, Microsoft Terminal Services, SunRay, VNC, and X. Our experimental results on web and video applications demonstrate that THINC can be as much as five times faster than traditional thin-client systems in high latency network environments and is capable of playing full-screen video at full frame rate. (pdf) The Simplicity and Safety of the Language for End System Services (LESS) Xiaotao Wu, Henning Schulzrinne 2004-07-20 This paper analyzes the simplicity and safety of the Language for End System Services (LESS). (pdf) (ps) Efficient Shadows from Sampled Environment Maps Aner Ben-Artzi,  Ravi Ramamoorthi, , Maneesh Agrawala 2004-06-10 This paper addresses the problem of efficiently calculating shadows from environment maps. Since accurate rendering of shadows from environment maps requires hundreds of lights, the expensive computation is determining visibility from each pixel to each light direction, such as by ray-tracing. We show that coherence in both spatial and angular domains can be used to reduce the number of shadow rays that need to be traced. Specifically, we use a coarse-to-fine evaluation of the image, predicting visibility by reusing visibility calculations from four nearby pixels that have already been evaluated. This simple method allows us to explicitly mark regions of uncertainty in the prediction. By only tracing rays in these and neighboring directions, we are able to reduce the number of shadow rays traced by up to a factor of 20 while maintaining error rates below 0.01\\%. For many scenes, our algorithm can add shadowing from hundreds of lights at twice the cost of rendering without shadows. (pdf) (ps) Efficient Algorithms for the Design of Asynchronous Control Circuits Michael Theobald 2004-05-27 Asynchronous (or ``clock-less'') digital circuit design has received much attention over the past few years, including its introduction into consumer products. One major bottleneck to the further advancement of clock-less design is the lack of optimizing CAD (computer-aided design) algorithms and tools. In synchronous design, CAD packages have been crucial to the advancement of the microelectronics industry. In fact, automated methods seem to be even more crucial for asynchronous design, which is widely considered as being much more error-prone.\nThis thesis proposes several new efficient CAD techniques for the design of asynchronous control circuits. The contributions include: (i) two new and very efficient algorithms for hazard-free two-level logic minimization, including a heuristic algorithm, ESPRESSO-HF, and an exact algorithm based on implicit data structures, IMPYMIN; and (ii) a new synthesis and optimization method for large-scale asynchronous systems, which starts from a Control-Dataflow Graph (CDFG), and produces highly-optimized distributed control.\nAs a case study, this latter method is applied to a differential equation solver; the resulting synthesized circuit is comparable in quality to a highly-optimized manual design. (ps) On decision trees, influences, and learning monotone decision trees Ryan O'Donnell, Rocco A. Servedio 2004-05-26 In this note we prove that a monotone boolean function computable by a decision tree of size $s$ has average sensitivity at most $\\sqrt{\\log_2 s}$. As a consequence we show that monotone functions are learnable to constant accuracy under the uniform distribution in time polynomial in their decision tree size. (pdf) (ps) Orchestrating the Dynamic Adaptation of Distributed Software with Process Technology Giuseppe Valetto 2004-05-24 Software systems are becoming increasingly complex to develop, understand, analyze, validate, deploy, configure, manage and maintain. Much of that complexity is related to ensuring adequate quality levels to services provided by software systems after they are deployed in the field, in particular when those systems are built from and operated as a mix of proprietary and non-proprietary components. That translates to increasing costs and difficulties when trying to operate large-scale distributed software ensembles in a way that continuously guarantees satisfactory levels of service.\nA solution can be to exert some form of dynamic adaptation upon running software systems: dynamic adaptation can be defined as a set of automated and coordinated actions that aim at modifying the structure, behavior and performance of a target software system, at run time and without service interruption, typically in response to the occurrence of some condition(s). To achieve dynamic adaptation upon a given target software system, a set of capabilities, including monitoring, diagnostics, decision, actuation and coordination, must be put in place.\nThis research addresses the automation of decision and coordination in the context of an end-to-end and externalized approach to dynamic adaptation, which allows to address as its targets legacy and component-based systems, as well as new systems developed from scratch. In this approach, adaptation provisions are superimposed by a separate software platform, which operates from the outside of and orthogonally to the target application as a whole; furthermore, a single adaptation possibly spans concerted interventions on a multiplicity of target components. To properly orchestrate those interventions, decentralized process technology is employed for describing, activating and coordinating the work of a cohort of software actuators, towards the intended end-to-end dynamic adaptation.\nThe approach outlined above, has been implemented in a prototype, code-named Workflakes, within the Kinesthetics eXtreme project investigating externalized dynamic adaptation, carried out by the Programming Systems Laboratory of Columbia University, and has been employed in a set of diverse case studies. This dissertation discusses and evaluates the concept of process-based orchestration of dynamic adaptation and the Workflakes prototype on the basis of the results of those case studies. (pdf) Elastic Block Ciphers: The Feistel Cipher Case Debra L. Cook,  Moti Yung, , Angelos Keromytis 2004-05-19 We discuss the elastic versions of block ciphers whose round function processes subsets of bits from the data block differently, such as occurs in a Feistel network and in MISTY1. We focus on how specific bits are selected to be swapped after each round when forming the elastic version, using an elastic version of MISTY1 and differential cryptanalysis to illustrate why this swap step must be carefully designed. We also discuss the benefit of adding initial and final key dependent permutations in all elastic block ciphers. The implementation of the elastic version of MISTY1 is analyzed from a performance perspective. (pdf) (ps) Exploiting the Structure in DHT Overlays for DoS Protection Angelos Stavrou,  Angelos Keromytis,  Dan 2004-04-30 Peer to Peer (P2P) systems that utilize Distributed Hash Tables (DHTs) provide a scalable means to distribute the handling of lookups. However, this scalability comes at the expense of increased vulnerability to specific types of attacks. In this paper, we focus on insider denial of service (DoS) attacks on such systems. In these attacks, nodes that are part of the DHT system are compromised and used to flood other nodes in the DHT with excessive request traffic.\nWe devise a distributed lightweight protocol that detects such attacks, implemented solely within nodes that participate in the DHT. Our approach exploits inherent structural invariants of DHTs to ferret out attacking nodes whose request patterns deviate from ``normal'' behavior. We evaluate our protocol's ability to detect attackers via simulation within a Chord network. The results show that our system can detect a simple attacker whose attack traffic deviates by as little as 5\\% from a normal request traffic. We also demonstrate the resiliency of our protocol to coordinated attacks by up to as many as 25\\% of nodes. Our work shows that DHTs can protect themselves from insider flooding attacks, eliminating an important roadblock to their deployment and use in untrusted environments. (pdf) (ps) Host-based Anomaly Detection Using Wrapping File Systems Shlomo Hershkop,  Linh H. Bui,  Ryan Ferst, Salvatore J. Stolfo 2004-04-24 We describe an anomaly detector, called FWRAP, for a Host-based Intrusion Detection System that monitors file system calls to detect anomalous accesses. The system is intended to be used not as a standalone detector but one of a correlated set of host-based sensors. The detector has two parts, a sensor that audits file systems accesses, and an unsupervised machine learning system that computes normal models of those accesses.We report on the architecture of the file system sensor implemented on Linux using the FiST file wrapper technology and results of the anomaly detector applied to experimental data acquired from this sensor. FWRAP employs the Probabilistic Anomaly Detection (PAD) algorithm previously reported in our work on Windows Registry Anomaly Detection. The detector is first trained by operating the host computer for some amount of time and a model specific to the target machine is automatically computed by PAD, intended to be deployed to a real-time detector. In this paper we describe the feature set used to model file system accesses, and the performance results of a set of experiments using the sensor while attacking a Linux host with a variety of malware exploits. The PAD detector achieved impressive detection rates in some cases over 95\\% and about a 2\\% false positive rate when alarming on anomalous processes. (pdf) (ps) Self-Managing Systems: A Control Theory Foundation Yixin Diao, Joseph L. Hellerstein, Sujay Parekh, Rean Griffith, Gail Kaiser, Dan Phung 2004-04-01 The high cost of ownership of computing systems has resulted in a number of industry initiatives to reduce the burden of operations and management.  Examples include IBM's Autonomic Computing, HP's Adaptive Infrastructure, and Microsoft's Dynamic Systems Initiative. All of these efforts seek to reduce operations costs by increased automation, ideally to have systems be self-managing without any human intervention (since operator error has been identified as a major source of system failures). While the concept of automated operations has existed for two decades, as a way to adapt to changing workloads, failures and (more recently) attacks, the scope of automation remains limited. We believe this is in part due to the absence of a fundamental understanding of how automated actions affect system behavior, especially system stability.\r\nOther disciplines such as mechanical, electrical, and aeronautical engineering make use of control theory to design feedback systems. This paper uses control theory as a way to identify a number of requirements for and challenges in building self-managing systems,\r\neither from new components or layering on top of existing components. (pdf) Blurring of Light due to Multiple Scattering by the Medium, a \nPath Integral Approach Michael Ashikhmin,  Simon Premoze,  Ravi R, Shree Nayar 2004-03-31 Volumetric light transport effects are significant for many materials like skin, smoke, clouds or water. In particular, one must consider the multiple scattering of light within the volume. Recently, we presented a path integral-based approach to this problem which identifies the most probable path light takes in the medium and approximates energy transport over all paths by only those surrounding this most probable one. In this report we use the same approach to derive useful expressions for the amount of spacial and angular blurring light experiences as it travels through a medium. (pdf) (ps) Jitter-Camera: High Resolution Video from a Low Resolution Detector Moshe Ben-Ezra,  Assaf Zomet, , Shree K. Nayar 2004-03-25 Video cameras must produce images at a reasonable frame-rate and with a reasonable depth of field. These requirements impose fundamental physical limits on the spatial resolution of the image detector. As a result, current cameras produce videos with a very low resolution. The resolution of videos can be computationally enhanced by moving the camera and applying super-resolution reconstruction algorithms. However, a moving camera introduces motion blur, which limits super-resolution quality. We analyze this effect and derive a theoretical result showing that motion blur has a substantial degrading effect on the performance of super resolution. The conclusion is, that in order to achieve the highest resolution, motion blur should be avoided.\nMotion blur can be minimized by sampling the space-time volume of the video in a specific manner. We have developed a novel camera, called the jitter camera, that achieves this sampling. By applying an adaptive super-resolution algorithm to the video produced by the jitter camera, we show that resolution can be notably enhanced for stationary or slowly moving objects, while it is improved slightly or left unchanged for objects with fast and complex motions. The end result is a video that has a significantly higher resolution than the captured one. (pdf) (ps) Improved Controller Synthesis from Esterel Cristian Soviani,  Jia Zeng, , Stephen A. Edwards 2004-03-22 We present a new procedure for automatically synthesizing controllers from high-level Esterel specifications. Unlike existing \\textsc{rtl} synthesis approaches, this approach frees the designer from tedious bit-level state encoding and certain types of inter-machine communication. Experimental results suggest that even with a fairly primitive state assignment heuristic, our compiler consistently produces smaller, slightly faster circuits that the existing Esterel compiler. We mainly attribute this to a different style of distributing state bits throughout the circuit. Initial results are encouraging, but some hand-optimized encodings suggest room for a better state assignment algorithm. We are confident that such improvements will make our technique even more practical. (pdf) (ps) MobiDesk: Mobile Virtual Desktop Computing Ricardo Baratto,  Shaya Potter,  Gong Su, , Jason Nieh 2004-03-19 We present MobiDesk, a mobile virtual desktop computing hosting infrastructure that leverages continued improvements in network speed, cost, and ubiquity to address the complexity, cost, and mobility limitations of today's personal computing infrastructure. MobiDesk transparently virtualizes a user's computing session by abstracting underlying system resources in three key areas: display, operating system and network. MobiDesk provides a thin virtualization layer that decouples a user's computing session from any particular end user device and moves all application logic from end user devices to hosting providers. MobiDesk virtualization decouples a user's computing session from the underlying operating system and server instance, enabling high availability service by transparently migrating sessions from one server to another during server maintenance or upgrades. We have implemented a MobiDesk prototype in Linux that works with existing unmodified applications and operating system kernels. Our experimental results demonstrate that MobiDesk has very low virtualization overhead, can provide a full-featured desktop experience including full-motion video support, and is able to migrate users' sessions efficiently and reliably for high availability, while maintaining existing network connections. (pdf) (ps) When one Sample is not Enough: Improving Text Database Selection Using Shrinkage Panagiotis G. Ipeirotis, Luis Gravano 2004-03-17 Database selection is an important step when searching over large numbers of distributed text databases. The database selection task relies on statistical summaries of the database contents, which are not typically exported by databases. Previous research has developed algorithms for constructing an approximate content summary of a text database from a small document sample extracted via querying. Unfortunately, Zipf's law practically guarantees that content summaries built this way for any relatively large database will fail to cover many low-frequency words. Incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To improve the coverage of approximate content summaries, we build on the observation that topically similar databases tend to have related vocabularies. Therefore, the approximate content summaries of topically related databases can complement each other and increase their coverage. Specifically, we exploit a (given or derived) hierarchical categorization of the databases and adapt the notion of ``shrinkage'' --a form of smoothing that has been used successfully for document classification-- to the content summary construction task. A thorough evaluation over 315 real web databases as well as over TREC data suggests that the shrinkage-based content summaries are substantially more complete than their ``unshrunk'' counterparts. We also describe how to modify existing database selection algorithms to adaptively decide --at run-time-- whether to apply shrinkage for a query. Our experiments, which rely on TREC data sets, queries, and the associated ``relevance judgments,'' show that our shrinkage-based approach is significantly more accurate than state-of-the-art database selection algorithms, including a recently proposed hierarchical strategy that also exploits database classification. (pdf) (ps) Collaborative Distributed Intrusion Detection Michael E. Locasto,  Janak J. Parekh,  Sal, Vishal Misra 2004-03-08 The rapidly increasing array of Internet-scale threats is a pressing problem for every organization that utilizes the network. Organizations often have limited resources to detect and respond to these threats. The sharing of information related to probes and attacks is a facet of an emerging trend toward \"collaborative security.\" Collaborative security mechanisms provide network administrators with a valuable tool in this increasingly hostile environment.\nThe perceived benefit of a collaborative approach to intrusion detection is threefold: greater clarity about attacker intent, precise models of adversarial behavior, and a better view of global network attack activity. While many organizations see value in adopting such a collaborative approach, several critical problems must be addressed before intrusion detection can be performed on an inter-organizational scale. These obstacles to collaborative intrusion detection often go beyond the merely technical; the relationships between cooperating organizations impose additional constraints on the amount and type of information to be shared.\nWe propose a completely decentralized system that can efficiently distribute alerts to each collaborating peer. The system is composed of two major components that embody the main contribution of our research. The first component, named Worminator, is a tool for extracting relevant information from alert streams and encoding it in Bloom filters. The second component, Whirlpool, is a software system for scheduling correlation relationships between peer nodes. The combination of these systems accomplishes alert distribution in a scalable manner and without violating the privacy of each administrative domain. (pdf) (ps) Failover and Load Sharing in SIP Telephony Kundan Singh, Henning Schulzrinne 2004-03-01 We apply some of the existing web server redundancy techniques for high service availability and scalability to the relatively new IP telephony context. The paper compares various failover and load sharing methods for registration and call routing servers based on the Session Initiation Protocol (SIP). In particular, we consider the SIP server failover techniques based on the clients, DNS (Domain Name Service), database replication and IP address takeover, and the load sharing techniques using DNS, SIP identifiers, network address translators and servers with same IP addresses. Additionally, we present an overview of the failover mechanism we implemented in our test-bed using our SIP proxy and registration server and the open source MySQL database. (pdf) (ps) Virtual Environment for Collaborative Distance Learning With Video Synchronization Suhit Gupta, Gail Kaiser 2004-02-25 We present a 3D collaborative virtual environment, CHIME, in which geographically dispersed students can meet together in study groups or to work on team projects. Conventional educational materials from heterogeneous backend data sources are reflected in the virtual world through an automated metadata extraction and projection process that structurally organizes container materials into rooms and interconnecting doors, with atomic objects within containers depicted as furnishings and decorations. A novel in-world authoring tool makes it easy for instructors to design environments, with additional in-world modification afforded to the students themselves, in both cases without programming. Specialized educational services can also be added to virtual environments via programmed plugins. We present an example plugin that supports synchronized viewing of lecture videos by groups of students with widely varying bandwidths. (pdf) Optimizing Quality for Collaborative Video Viewing Dan Phung,  Giuseppe Valetto,  Gail Kaiser, Suhit Gupta 2004-02-25 The increasing popularity of distance learning and online courses has highlighted the lack of collaborative tools for student groups. In addition, the introduction of lecture videos into the online curriculum has drawn attention to the disparity in the network resources used by the students. We present an architecture and adaptation model called AI2TV (Adaptive Internet Interactive Team Video), a system that allows geographically dispersed participants, possibly some or all disadvantaged in network resources, to collaboratively view a video in synchrony. AI2TV upholds the invariant that each participant will view semantically equivalent content at all times. Video player actions, like play, pause and stop, can be initiated by any of the participants and the results of those actions are seen by all the members. These features allow group members to review a lecture video in tandem to facilitate the learning process. We employ an autonomic (feedback loop) controller that monitors clients' video status and adjusts the quality of the video according to the resources of each client. We show in experimental trials that our system can successfully synchronize video for distributed clients while, at the same time, optimizing the video quality given actual (fluctuating) bandwidth by adaptively adjusting the quality level for each participant. (pdf) (ps) Elastic Block Ciphers Debra L. Cook,  Moti Yung, , Angelos Keromytis 2004-02-25 We introduce a new concept of elastic block ciphers, symmetric-key encryption algorithms that for a variable size input do not expand the plaintext, (i.e., do not require plaintext padding), while maintaining the diffusion property of traditional block ciphers and adjusting their computational load proportionally to the size increase. Elastic block ciphers are ideal for applications where length-preserving encryption is most beneficial, such as protecting variable-length database entries or network packets.\nWe present a general algorithmic schema for converting a traditional block cipher, such as AES, to its elastic version, and analyze the security of the resulting cipher. Our approach allows us to ``stretch'' the supported block size of a block cipher up to twice the original length, while increasing the computational load proportionally to the block size. Our approach does not allow us to use the original cipher as a ``black box'' (i.e., as an ideal cipher or a pseudorandom permutation as is used in constructing modes of encryption). Nevertheless, under some reasonable conditions on the cipher's structure and its key schedule, we reduce the security of the elastic version to that of the fixed size block cipher. This schema and the security reduction enable us to capitalize on secure ciphers and their already established security properties in elastic designs. We note that we are not aware of previous ``reduction type'' proofs of security in the area of concrete (i.e., non ``black-box'') block cipher design. Our implementation of the elastic version of AES, which accepts blocks of all sizes in the range 128 to 255 bits, was measured to be almost twice as fast when encrypting plaintext that is only a few bits longer than a full block (A128 bits), when compared to traditional ``pad and block-encrypt'' approach. (pdf) (ps) DotSlash: A Scalable and Efficient Rescue System for Handling Web Hotspots Weibin Zhao, Henning Schulzrinne 2004-02-07 This paper describes DotSlash, a scalable and efficient rescue system for handling web hotspots. DotSlash allows different web sites to form a mutual-aid community, and use spare capacity in the community to relieve web hotspots experienced by any individual site. As a rescue system, DotSlash intervenes when a web site becomes heavily loaded, and is phased out once the workload returns to normal. It aims to complement existing web server infrastructure such as CDNs to handle short-term load spikes effectively, but is not intended to support a request load constantly higher than a web site's planned capacity. DotSlash is scalable, cost-effective, easy to use, self-configuring, and transparent to clients. It targets small web sites, although large web site can also benefit from it. We have implemented a prototype of DotSlash on top of Apache. Experiments show that DotSlash can provide an order of magnitude improvement for a web server in terms of the request rate supported and the data rate delivered to clients even if only HTTP redirect is used. Parts of this work may be applicable to other services such as the Grid computational services and media streaming. (pdf) (ps) Asymptotic bounds for $M^X/G/1$ processor sharing queues Hanhua Feng, Vishal Misra 2004-02-03 This paper analyzed the asymptotic bounds of an $M/G/1$ processor sharing queue with bulk arrivals. (pdf) (ps) Secure Isolation and Migration of Untrusted Legacy Applications Shaya Potter,  Jason Nieh, , Dinesh Subhraveti 2004-01-27 Sting applications often contain security holes that are not patched until after the system has already been compromised. Even when software updates are applied to address security issues, they often result in system services being unavailable for some time. To address these system security and availability issues, we have developed peas and pods. A pea provides a least privilege environment that can restrict processes to the minimal subset of system resources needed to run. This mechanism enables the creation of environments for privileged program execution that can help with intrusion prevention and containment. A pod provides a group of processes and associated users with a consistent, machine-independent virtualized environment. Pods are coupled with a novel checkpoint-restart mechanism which allows processes to be migrated across minor operating system kernel versions with different security patches. This mechanism allows system administrators the flexibility to patch their operating systems immediately without worrying over potential loss of data or needing to schedule system downtime. We have implemented peas and pods in Linux without requiring any application or operating system kernel changes. Our measurements on real world desktop and server applications demonstrate that peas and pods impose little overhead and enable secure isolation and migration of untrusted applications. (pdf) (ps) Feature Interactions in Internet Telephony End Systems Xiaotao Wu, Henning Schulzrinne 2004-01-24 Internet telephony end systems can offer many services. Different services may interfere with each other, a problem which is known as feature interaction. The feature interaction problem has existed in telecommunication systems for many years. The introduction of Internet telephony helps to solve some interaction problems due to the richness of its signaling information. However, many new feature interaction problems are also introduced in Internet telephony systems, especially in end systems, which are usually dumb in PSTN systems, but highly functional in Internet telephony systems. Internet telephony end systems, such as SIP soft-agents, can run on personal computers. The soft-agents can then perform call control and many other functions, such as presence information handling, instant messaging, and network appliance control. These new functionalities make the end system feature interaction problems more complicated. In this paper, we investigate ways features interact in Internet telephony end systems and propose a potential solution for detecting and avoiding feature interactions. Our solutions are based on the Session Initiation Protocol (SIP) and the Language for End System Services (LESS), which is a markup language specifically for end system service creation. (pdf) (ps) The Complexity of Fredholm Equations of the Second Kind: Noisy Information About Everything Arthur G. Werschulz 2004-01-21 We study the complexity of Fredholm problems of the second kind $u - \\int_\\Omega k(\\cdot,y)u(y)\\,dy = f$. Previous work on the complexity of this problem has assumed that $\\Omega$ was the unit cube~$I^d$. In this paper, we allow~$\\Omega$ to be part of the data specifying an instance of the problem, along with~$k$ and~$f$. More precisely, we assume that $\\Omega$ is the diffeomorphic image of the unit $d$-cube under a $C^{r_1}$ mapping~$\\rho\\:I^d\\to I^l$. In addition, we assume that $k\\in C^{r_2}(I^{2l})$ and $f\\in W^{r_3,p}(I^l)$ with $r_3>l/p$. Our information about the problem data is contaminated by $\\delta$-bounded noise. Error is measured in the $L_p$-sense. We find that the $n$th minimal error is bounded from below by $\\Theta(n^{-\\mu_1}+\\delta)$ and from above by $\\Theta(n^{-\\mu_2}+\\delta)$, where $$\\mu_1 = \\min\\left\\{\\frac{r_1}{d}, \\frac{r_2}{2d}, \\frac{r_3-(d-l)/p}d\\right\\} \\qquad\\text{and}\\qquad \\mu_2 = \\min\\left\\{\\frac{r_1-\\nu}d, \\frac{r_2}{2d}, \\frac{r_3-(l-d)/p}d\\right\\},$$ with $$\\nu = \\begin{cases} \\displaystyle\\frac{d}p & \\text{if $r_1\\ge 2$, $r_2\\ge2$, and $d\\le p$},\\\\ & \\\\ 1 & \\text{otherwise}. \\end{cases}$$ In particular, the $n$th minimal error is proportional to $\\Theta(n^{-\\mu_1}+\\delta)$ when $p=\\infty$. The upper bound is attained by a noisy modified Galerkin method, which can be efficiently implemented using multigrid techniques. We thus find bounds on the $\\varepsilon$-complexity of the problem, these bounds depending on the cost $\\mathbf{c}(\\delta)$ of calculating a $\\delta$-noisy function value. As an example, if $\\mathbf{c}(\\delta)=\\delta^{-b}$, we find that the $\\varepsilon$-complexity is between $(1/\\varepsilon)^{b+1/\\mu_1}$ and $(1/\\varepsilon)^{b+1/\\mu_2}$. (pdf) (ps) Secret Key Cryptography Using Graphics Cards Debra L. Cook,  John Ioannidis,  Angelos D, Jake Luck 2004-01-13 One frequently cited reason for the lack of wide deployment of cryptographic protocols is the (perceived) poor performance of the algorithms they employ and their impact on the rest of the system. Although high-performance dedicated cryptographic accelerator cards have been commercially available for some time, market penetration remains low. We take a different approach, seeking to exploit {\\it existing system resources,} such as Graphics Processing Units (GPUs) to accelerate cryptographic processing.\nWe exploit the ability for GPUs to simultaneously process large quantities of pixels to offload cryptographic processing from the main processor. We demonstrate the use of GPUs for stream ciphers, which can achieve 75\\% the performance of a fast CPU. We also investigate the use of GPUs for block ciphers, discuss operations that make certain ciphers unsuitable for use with a GPU, and compare the performance of an OpenGL-based implementation of AES with implementations utilizing general CPUs. In addition to offloading system resources, the ability to perform encryption and decryption within the GPU has potential applications in image processing by limiting exposure of the plaintext to within the GPU. (pdf) (ps) Automating Content Extraction of HTML Documents Suhit Gupta,  Gail Kaiser,  Peter Grimm,  M, Justin Starren 2004-01-06 Web pages often contain clutter (such as unnecessary images and extraneous links) around the body of an article that distracts a user from actual content. Extraction of \"useful and relevant\" content from web pages has many applications, including cell phone and PDA browsing, speech rendering for the visually impaired, and text summarization. Most approaches to making content more readable involve changing font size or removing HTML and data components such as images, which takes away from a webpage's inherent look and feel. Unlike \"Content Reformatting\", which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses \"Content Extraction\". We have developed a framework that employs an easily extensible set of techniques. It incorporates advantages of previous work on content extraction. Our key insight is to work with DOM trees, a W3C specified interface that allows programs to dynamically access document structure, rather than with raw HTML markup. We have implemented our approach in a publicly available Web proxy to extract content from HTML web pages. This proxy can be used both centrally, administered for groups of users, as well as by individuals for personal browsers. We have also, after receiving feedback from users about the proxy, created a revised version with improved performance and accessibility in mind. (pdf) (ps) AIM Encrypt: A Case Study of the Dangers of Cryptographic Urban Legends Michael E. Locasto 2003-11-26 Like e--mail, instant messaging (IM) has become an integral part of life in a networked society. Until recently, IM software has been lax about providing confidentiality and integrity of these conversations.\nWith the introduction of AOL's version 5.2.3211 of the AIM client, users can optionally encrypt and protect the integrity of their conversation. Taking advantage of the encryption capabilities of the AIM client requires that signed certificates for both parties be available. AIM (through VeriSign) makes such certificates available for purchase. However, in a ``public service'' effort to defray the cost of purchasing personal certificates to protect IM conversations, a website (www.aimencrypt.com) is offering a certificate free of cost for download. Unfortunately, the provided certificate is the same for everyone; this mistake reveals the dangers of a public undereducated about computer security, especially public key cryptography. (pdf) (ps) Countering Network Worms Through Automatic Patch Generation Stelios Sidiroglou, Angelos D. Keromytis 2003-11-19 The ability of worms to spread at rates that effectively preclude human-directed reaction has elevated them to a first-class security threat to distributed systems. We propose an architecture for automatically repairing software flaws that are exploited by zero-day worms.Our approach relies on source code transformations to quickly apply automatically-created (and tested) localized patches to vulnerable segments of the targeted application. To determine these susceptible portions, we use a sandboxed instance of the application as a ``clean room'' laboratory that runs in parallel with the production system and exploit the fact that a worm must reveal its infection vector to achieve its goal ( i.e., further infection). We believe our approach to be the first end-point solution to the problem of malicious self-replicating code. The primary benefits of our approach are (a) its low impact on application performance, (b) its ability to respond to attacks without human intervention, and (c) its capacity to deal with zero-day worms (for which no known patches exist). Furthermore, our approach does not depend on a centralized update repository, which can be the target of a concerted attack similar to the Blaster worm. Finally, our approach can also be used to protect against lower intensity attacks, such as intrusion (``hack-in'') attempts. To experimentally evaluate the efficacy of our approach, we use our prototype implementation to test a number of applications with known vulnerabilities. Our preliminary results indicate a success rate of 82\\%, and a maximum repair time of 8.5 seconds. (pdf) (ps) Dynamical Systems Trees Tony Jebara, Andrew Howard 2003-11-05 We propose dynamical systems trees (DSTs) as a\tfexible model for describing multiple processes that interact\tvia a hierarchy of aggregating processes. DSTs extend\tnonlinear dynamical systems to an interactive group\tscenario. Various individual processes interact as communities\tand sub-communities in a tree structure that is un-rolled in\ttime. To accommodate nonlinear temporal activity, each\tindividual leaf process is modeled as a dynamical system\tcontaining discrete and/or continuous hidden states with\tdiscrete and/or Gaussian emissions. Subsequent, higher level\tparent processes act like hidden Markov models that mediate\tthe interaction between leaf processes or between other parent\tprocesses in the hierarchy. Aggregator chains are parents of\tthe child processes the combine and mediate, yielding a\tcompact overall parameterization. We provide tractable\tinference and learning algorithms for arbitrary DSTs\ttopologies via structured mean field. Experiments are shown\tfor real trajectory data of tracked American football plays\twhere a DST tracks players as dynamical systems mediated by\ttheir team processes mediated in turn by a top-level game\tprocess. (pdf) (ps) Comprehensive Multi-platform Collaboration Kundan Singh,  Xiaotao Wu,  Jonathan Lenno, Henning Schulzrinne 2003-10-26 We describe the architecture and implementation of\tour comprehensive multi-platform collaboration framework known\tas Columbia InterNet Extensible Multimedia Architecture\t(CINEMA). It provides a distributed architecture for\tcollaboration using synchronous communications like multimedia\tconferencing, instant messaging, shared web-browsing, and\tasynchronous communications like discussion forums, shared\tfiles, voice and video mails. It allows seamless integration\twith various communication means like telephones, IP phones,\tweb and electronic mail. In addition, it provides value-added\tservices such as call handling based on location information\tand presence status. The paper discusses the media services\tneeded for collaborative environment, the components provided\tby CINEMA and the interaction among those components. (pdf) (ps) Retrofitting Autonomic Capabilities onto Legacy Systems Janak Parekh,  Gail Kaiser,  Philip Gross, , Giuseppe Valetto 2003-10-16 Autonomic computing - self-configuring,\tself-healing, self-optimizing applications, systems and\tnetworks - is a promising solution to ever-increasing system\tcomplexity and the spiraling costs of human management as\tsystems scale to global proportions. Most results to date,\thowever, suggest ways to architect new software constructed\tfrom the ground up as autonomic systems, whereas in the real\tworld organizations continue to use stovepipe legacy systems\tand/or build \"systems of systems\" that draw from a gamut of\tdisparate technologies from numerous vendors. Our goal is to\tretrofit autonomic computing onto such systems, externally,\twithout any need to understand, modify or even recompile the\ttarget system's code. We present an autonomic infrastructure\tthat operates similarly to active middleware, to explicitly\tadd autonomic services to pre-existing systems via continual\tmonitoring and a feedback loop that performs, as needed,\treconfiguration and/or repair. Our lightweight design and\tseparation of concerns enables easy adoption of individual\tcomponents, independent of the rest of the full\tinfrastructure, for use with a large variety of target\tsystems. This work has been validated by several case studies\tspanning multiple application domains. (pdf) (ps) Evaluating Content Selection in Human- or Machine-Generated Summaries: The Pyramid Scoring Method Rebecca J. Passonneau, Ani Nenkova 2003-09-03 From the outset of automated generation of\tsummaries, the difficulty of evaluation has been widely\tdiscussed. Despite many promising attempts, we believe it\tremains an unsolved problem. Here we present a method for\tscoring the content of summaries of any length against a\tweighted inventory of content units, which we refer to as a\tpyramid. Our method is derived from empirical analysis of\thuman-generated summaries, and provides an informative metric\tfor human or machine-generated summaries. (pdf) (ps) Security testing of SIP implementations Christian Wieser,  Marko Laakso, , Henning Schulzrinne 2003-08-25 The Session Initiation Protocol (SIP) is a\tsignaling protocol for Internet telephony, multimedia\tconferencing and instant messaging. Although SIP\timplementations have not yet been widely deployed, the product\tportfolio is expanding rapidly. We describe a method to assess\tthe robustness of SIP implementation by describing a tool to\tfind vulnerabilities. We prepared the test material and\tcarried out tests against a sample set of existing\timplementations. Results were reported to the vendors and the\ttest suite was made publicly available. Many of the\timplementations available for evaluation failed to perform in\ta robust manner under the test. Some failures had information\tsecurity implications, and should be considered\tvulnerabilities. (pdf) (ps) Integrating Categorization, Clustering, and Summarization for Daily News Browsing Regina Barzilay,  David Evans,  Vasileios, Sergey Sigelman 2003-08-12 Recently, there have been significant advances in\tseveral areas of language technology, including clustering,\ttext categorization, and summarization. However, efforts to\tcombine technology from these areas in a practical system for\tinformation access have been limited. In this paper, we\tpresent a system that integrates cutting-edge technology in\tthese areas to automatically collect news articles from\tmultiple sources, organize them and present them in both\thierarchical and text summary form. Our system is publicly\tavailable and runs daily over real data. Through a sizable\tuser evaluation, we show that users strongly prefer using the\tadvanced features incorporated in our system, and that these\tfeatures help users achieve more efficient browsing of news. (pdf) (ps) 9-1-1 Calls for Voice-over-IP Henning Schulzrinne 2003-08-10 This document enumerates some of the major\topportunities and challenges for providing emergency call\t(9-1-1) services using IP technology. In particular, all VoIP\tdevices are effectively mobile. The same IP telephony device\tworks anywhere in the Internet, keeping the same external\tidentifier such as an E.164 number or URL. (Note: This was\talso submitted as an ex-parte filing to the Federal\tCommunications Commission.) (pdf) (ps) A Holistic Approach to Service Survivability Angelos D. Keromytis,  Janak Parekh,  Phil, Sal Stolfo 2003-07-10 We present SABER (Survivability Architecture:\tBlock, Evade, React), a proposed survivability architecture\tthat blocks, evades and reacts to a variety of attacks by\tusing several security and survivability mechanisms in an\tautomated and coordinated fashion. Contrary to the ad hoc\tmanner in which contemporary survivable systems are\tbuilt--using isolated, independent security mechanisms such as\tfirewalls, intrusion detection systems and software\tsandboxes--SABER integrates several different technologies in\tan attempt to provide a unified framework for responding to\tthe wide range of attacks malicious insiders and outsiders can\tlaunch.\nThis coordinated multi-layer approach will be capable of\tdefending against attacks targeted at various levels of the\tnetwork stack, such as congestion-based DoS attacks,\tsoftware-based DoS or code-injection attacks, and others. Our\tfundamental insight is that while multiple lines of defense\tare useful, most conventional, uncoordinated approaches fail\tto exploit the full range of available responses to\tincidents. By coordinating the response, the ability to\tsurvive even in the face of successful security breaches\tincreases substantially.\nWe discuss the key components of SABER, how they will be\tintegrated together, and how we can leverage on the promising\tresults of the individual components to improve survivability\tin a variety of coordinated attack scenarios. SABER is\tcurrently in the prototyping stages, with several interesting\topen research topics. (pdf) (ps) Using Process Technology to Control and Coordinate Software Adaptation Giuseppe Valetto, Gail Kaiser 2003-07-09 We have developed an infrastructure for end-to-end run-time monitoring, behavior/performance analysis, and dynamic adaptation of distributed software. This infrastructure is primarily targeted to pre-existing systems and thus operates outside the target application, without making assumptions about the target's implementation, internal communication/computation mechanisms, source code availability, etc. This paper assumes the existence of the monitoring and analysis components, presented elsewhere, and focuses on the mechanisms used to control and coordinate possibly complex repairs/reconfigurations to the target system. These mechanisms require lower level effectors somehow attached to the target system, so we briefly sketch one such facility (elaborated elsewhere). Our main contribution is the model, architecture, and implementation of Workflakes, the decentralized process engine we use to tailor, control, coordinate, etc. a cohort of such effectors. We have validated the Workflakes approach with case studies in several application domains. Due to space restrictions we concentrate primarily on one case study, briefly discuss a second, and only sketch others. (pdf) (ps) Kinesthetics eXtreme: An External Infrastructure for Monitoring Distributed Legacy Systems Gail Kaiser,  Janak Parekh,  Philip Gross, Giuseppe Valetto 2003-07-06 Autonomic computing - self-configuring,\tself-healing, self-optimizing applications, systems and\tnetworks - is widely believed to be a promising solution to\tever-increasing system complexity and the spiraling costs of\thuman system management as systems scale to global\tproportions. Most results to date, however, suggest ways to\tarchitect new software constructed from the ground up as\tautonomic systems, whereas in the real world organizations\tcontinue to use stovepipe legacy systems and/or build \"systems\tof systems\" that draw from a gamut of new and legacy\tcomponents involving disparate technologies from numerous\tvendors. Our goal is to retrofit autonomic computing onto\tsuch systems, externally, without any need to understand or\tmodify the code, and in many cases even when it is impossible\tto recompile. We present a meta-architecture implemented as\tactive middleware infrastructure to explicitly add autonomic\tservices via an attached feedback loop that provides continual\tmonitoring and, as needed, reconfiguration and/or repair. Our\tlightweight design and separation of concerns enables easy\tadoption of individual components, as well as the full\tinfrastructure, for use with a large variety of legacy, new\tsystems, and systems of systems. We summarize several\texperiments spanning multiple domains. (pdf) (ps) Group Round Robin: Improving the Fairness and Complexity of Packet Scheduling Bogdan Caprita,  Wong Chun Chan, Jason Nieh 2003-07-02 We introduce Group Round-Robin (GRR) scheduling, a\thybrid scheduling framework based on a novel grouping strategy\tthat narrows down the traditional tradeoff between fairness\tand computational complexity. GRR combines its grouping\tstrategy with a specialized round-robin scheduling algorithm\tthat utilizes the properties of GRR groups to schedule flows\twithin groups in a manner that provides O(1) bounds on\tfairness with only O(1) time complexity. Under the practical\tassumption that GRR employs a small constant number of groups,\twe apply GRR to popular fair queueing scheduling algorithms\tand show how GRR can be used to achieve constant bounds on\tfairness and time complexity for these algorithms. (pdf) A General Framework for Designing Catadioptric Imaging and Projection Systems Rahul Swaminathan,  Michael D. Grossberg, Shree K. Nayar 2003-07-01 New vision applications have been made possible and old ones improved through the creation and design of novel catadioptric systems. Critical to the design of catadioptric imaging is determining the shape of one or more mirrors in the system. Almost all the previously designed mirrors for catadioptric systems used case specific tools and considerable effort on the part of the designer. Recently some new general methods have been proposed to automate the design process. However, all the methods presented so far determine the mirror shape by optimizing its geometric properties, such as surface normal orientations. A more principled approach is to determine a mirror that reduces image errors.\nIn this paper we present a method for finding mirror shapes which meet user determined specifications while minimizing image error. We accomplish this by deriving a first order approximation of the image error. This permits us to compute the mirror shape using a linear approach that provides good results efficiently while avoiding the numerical problems associated with non-linear optimization. Since the design of mirrors can also be applied to projection systems, we also provide a method to approximate projection errors in the scene. We demonstrate our approach on various catadioptric systems and show our approach to provide much more accurate imaging characteristics. In some cases we achieved reduction in image error up to 80 percent. (pdf) Using Prosodic Features of Speech and Audio Localization in Graphical User Interfaces Alex Olwal, Steven Feiner 2003-06-26 We describe several approaches for using prosodic\tfeatures of speech and audio localization to control\tinter-active applications. This information can be used for\tparameter control, as well as for disambiguating speech\trecognition. We discuss how characteristics of the spoken\tsentences can be exploited in the user interface; for example,\tby considering the speed with which the sentence was spoken\tand the presence of extraneous utterances. We also show how\tcoarse audio localization can be used for low-fidelity gesture\ttracking, by inferring the speaker's head position. (pdf) (ps) Statistical Acquisition of Content Selection Rules for Natural Language Generation Pablo A. Duboue, Kathleen R. McKeown 2003-05-30 A Natural Language Generation system produces text\tusing as input semantic data. One of its very first tasks is\tto decide which pieces of information to convey in the output.\tThis task, called Content Selection, is quite domain\tdependent, requiring considerable re-engineering to transport\tthe system from one scenario to another. In (Duboue and\tMcKeown, 2003), we presented a method to acquire content\tselection rules automatically from a corpus of text and\tassociated semantics. Our proposed technique was evaluated by\tcomparing its output with information selected by human\tauthors in unseen texts, where we were able to filter half the\tinput data set without loss of recall. This report contains\tadditional technical information about our system. (pdf) (ps) A platform for multilingual news summarization David Kirk Evans, Judith L. Klavans 2003-05-23 We have developed a multilingual version of\tColumbia Newsblaster as a testbed for multilingual\tmulti-document summarization. The system collects, clusters,\tand summarizes news documents from sources all over the world\tdaily. It crawls news sites in many different countries,\twritten in different languages, extracts the news text from\tthe HTML pages, uses a variety of methods to translate the\tdocuments for clustering and summarization, and produces an\tEnglish summary for each cluster. The system is robust,\trunning daily over real-world data. The multilingual version\tof Columbia Newsblaster provides a platform for testing\tdifferent strategies for multilingual document clustering, and\tapproaches for multilingual multi-document summarization. (pdf) (ps) Querying Faceted Databases Kenneth Ross, Angel Janevski 2003-05-21 Faceted classification allows one to model\tapplications with complex classification hierarchies using\torthogonal dimensions. Recent work has examined the use of\tfaceted classification for browsing and search. In this\tpaper, we go further by developing a general query language,\tcalled the entity algebra, for hierarchically classified data.\tThe entity algebra is compositional, with query inputs and\toutputs being sets of entities. Our language has linear data\tcomplexity in terms of space and quadratic data complexity in\tterms of time. We compare the entity algebra with the\trelational algebra in terms of expressiveness. We also\tdescribe an implementation of the language in the context of\ttwo application domains, one for an archeological database,\tand another for a human anatomy database. (pdf) (ps) Group Ratio Round-Robin: An O(1) Proportional Share Scheduler Wong Chun Chan, Jason Nieh 2003-05-15 Proportional share resource management provides a\tflexible and useful abstraction for multiplexing time-shared\tresources. However, previous proportional share mechanisms\thave either weak proportional sharing accuracy or high\tscheduling overhead. We present Group Ratio Round-Robin\t(GR3), a proportional share scheduler that can provide high\tproportional sharing accuracy with O(1) scheduling overhead.\tUnlike many other schedulers, a low-overhead GR3\timplementation is easy to build using simple data structures.\tWe have implemented GR3 in Linux and measured its performance\tagainst other schedulers commonly used in research and\tpractice, including the standard Linux scheduler, Weighted\tFair Queueing, Virtual-Time Round-Robin, and Smoothed\tRound-Robin. Our experimental results demonstrate that GR3\tcan provide much lower scheduling overhead and better\tscheduling accuracy in practice than these other approaches\tfor large numbers of clients. (pdf) (ps) Approximate String Joins in a Database (Almost) for Free -- Erratum Luis Gravano,  Panagiotis G. Ipeirotis,  H, Divesh Srivastava 2003-05-14 (pdf) (ps) Newsblaster Russian-English Clustering Performance Analysis Lawrence J. Leftin 2003-05-12 The Natural Language Group is developing a\tmulti-language version of Columbia Newsblaster, a program that\tgenerates summaries of news articles collected from web\tsites. Newsblaster currently processes articles in Arabic,\tJapanese,Portuguese, Spanish, and Russian, as well as\tEnglish. This report outlines the Russian language processing\tsoftware,focusing on machine translation and document\tclustering. Russian-English clustering results are analyzed\tand indicate encouraging inter-language and intra-language\tperformance. (pdf) (ps) Design Languages for Embedded Systems Stephen A. Edwards 2003-05-11 Embedded systems are application-specific\tcomputers that interact with the physical world. Each has a\tdiverse set of tasks to perform, and although a very flexible\tlanguage might be able to handle all of them, instead a\tvariety of problem-domain-specific languages have evolved that\tare easier to write, analyze, and compile.\nThis paper surveys some of the more important languages,\tintroducing their central ideas quickly without going into\tdetail. A small example of each is included. (pdf) (ps) Parallel Probing of Web Databases for Top-k Query Processing Am?lie Marian, Luis Gravano 2003-05-09 A top-k query specifies a set of preferred values\tfor the attributes of a relation and expects as a result the k\tobjects that are closest to the given preferences according to\tsome distance function. In many web applications, the relation\tattributes are only available via probes to autonomous\tweb-accessible sources. Probing these sources sequentially to\tprocess a top-k query is inefficient, since web accesses\texhibit high and variable latency. Fortunately, web sources\tcan be probed in parallel, and each source can typically\tprocess concurrent requests, although sources may impose some\trestrictions on the type and number of probes that they are\twilling to accept. These characteristics of web sources\tmotivate the introduction of parallel top-k query processing\tstrategies, which are the focus of this paper. We present\tefficient techniques that maximize source-access parallelism\tto minimize query response time, while satisfying source\taccess constraints. A thorough experimental evaluation over\tboth synthetic and real web sources shows that our techniques\tcan be significantly more efficient than previously proposed\tsequential strategies. In addition, we adapt our parallel\talgorithms for the alternate optimization goal of minimizing\tsource load while still exploiting source-access\tparallelism. (pdf) (ps) A Multi-resolution Block Storage Model for Database Design Jingren Zhou, Kenneth A. Ross 2003-05-05 We propose a new storage model called MBSM\t(Multi-resolution Block Storage Model) for laying out tables\ton disks. MBSM is intended to speed up operations such as\tscans that are typical of data warehouse workloads. Disk\tblocks are grouped into ``super-blocks,'' with a single record\tstored in a partitioned fashion among the blocks in a\tsuper-block. The intention is that a scan operation that\tneeds to consult only a small number of attributes can access\tjust those blocks of each super-block that contain the desired\tattributes. To achieve good performance given the physical\tcharacteristics of modern disks, we organize super-blocks on\tthe disk into fixed-size ``mega-blocks.'' Within a\tmega-block, blocks of the same type (from various\tsuper-blocks) are stored contiguously. We describe the\tchanges needed in a conventional database system to manage\ttables using such a disk organization. We demonstrate\texperimentally that MBSM outperforms competing approaches such\tas NSM (N-ary Storage Model), DSM (Decomposition Storage\tModel) and PAX (Partition Attributes Across), for I/O bound\tdecision-support workloads consisting of scans in which not\tall attributes are required. This improved performance comes\tat the expense of single-record insert and delete performance;\twe quantify the trade-offs involved. Unlike DSM, the cost of\treconstructing a record from its partitions is small. MBSM\tstores attributes in a vertically partitioned manner similar\tto PAX, and thus shares PAX's good CPU cache behavior. We\tdescribe methods for mapping attributes to blocks within\tsuper-blocks in order to optimize overall performance, and\tshow how to tune the super-block and mega-block sizes. (pdf) (ps) A Hybrid Approach for Answering Definitional Questions Sasha Blair-Goldensohn,  Kathleen R. McKe, Andrew Hazen Schlaikjer 2003-04-30 We present DefScriber, a fully implemented system\tthat combines knowledge-based and statistical methods in\tforming multi-sentence answers to open-ended definitional\tquestions of the form, ``What is X?'' We show how a set of\tdefinitional predicates proposed as the knowledge-based side\tof our approach can be used to guide the selection of\tdefinitional sentences. Finally, we present results of an\tevaluation of definitions generated by DefScriber from\tInternet documents. (pdf) (ps) SWAP: A Scheduler With Automatic Process Dependency Detection Haoqiang Zheng, Jason Nieh 2003-04-18 Cooperating processes are increasingly used to\tstructure modern applications in common client-server\tcomputing environments. This cooperation among processes\toften results in dependencies such that a certain process\tcannot proceed until other processes finish some tasks.\tDespite the popularity of using cooperating processes in\tapplication design, operating systems typically ignore process\tdependencies and schedule processes independently. This can\tresult in poor system performance due to the actual scheduling\tbehavior contradicting the desired scheduling policy.\nTo address this problem, we have developed SWAP, a system that\tautomatically detects process dependencies and accounts for\tsuch dependencies in scheduling. SWAP uses system call\thistory to determine possible resource dependencies among\tprocesses in an automatic and fully transparent fashion.\tBecause some dependencies cannot be precisely determined, SWAP\tassociates confidence levels with dependency information that\tare dynamically adjusted using feedback from process blocking\tbehavior. SWAP can schedule processes using this imprecise\tdependency information in a manner that is compatible with\texisting scheduling mechanisms and ensures that actual\tscheduling behavior corresponds to the desired scheduling\tpolicy in the presence of process dependencies. We have\timplemented SWAP in Linux and measured its effectiveness on\tmicrobenchmarks and real applications. Our experiment results\tshow that SWAP has low overhead and can provide substantial\timprovements in system performance in scheduling processes\twith dependencies. (pdf) (ps) Projecting XML Documents Amelie Marian, Jerome Simeon 2003-04-14 XQuery is not only useful to query XML in\tdatabases, but also to applications that must process XML\tdocuments as files or streams. These applications suffer from\tthe limitations of current main-memory XQuery processors which\tbreak for rather small documents. In this paper we propose\ttechniques, based on a notion of projection for XML, which can\tbe used to drastically reduce memory requirements in XQuery\tprocessors. The main contribution of the paper is a static\tanalysis technique that can identify at compile time which\tparts of the input document are needed to answer an arbitrary\tXQuery. We present a loading algorithm that takes the\tresulting information to build a projected document, which is\tsmaller than the original document, and on which the query\tyields the same result. We implemented projection in the\tGalax XQuery processor. Our experiments show that projection\treduces memory requirements by a factor of 20 on average, and\tis effective for a wide variety of queries. In addition,\tprojection results in some speedup during query evaluation. (pdf) (ps) Personalized Search of the Medical Literature: An Evaluation Vasileios Hatzivassililoglou,  Simone Teu, Sergey Sigelman 2003-03-31 We describe a system for personalizing a set of\tmedical journal articles (possibly created as the output of a\tsearch engine) by selecting those documents that specifically\tmatch a patient under care. Key element in our approach is the\tuse of targeted parts of the electronic patient record to\tserve as a readily available user model for the\tpersonalization task. We discuss several enhancements to a\tTF*IDF based approach for measuring the similarity between\tarticles and the patient record. We also present the results\tof an experiment involving almost 3,000 relevance judgments by\tmedical doctors. Our evaluation establishes that the automated\tsystem surpasses in performance alternative methods for\tpersonalizing the set of articles, including keyword-based\tqueries manually constructed by medical experts for this\tpurpose. (pdf) (ps) A Framework for 3D Pushbroom Imaging Naoyuki Ichimura, Shree K. Nayar 2003-03-14 Pushbroom cameras produce one-dimensional images\tof a scene with high resolution at a high frame-rate. As a\tresult, they provide superior data compared to conventional\ttwo-dimensional cameras in cases where the scene of interest\tcan be temporally scanned. In this paper, we consider the\tproblem of recovering the structure of a scene using a set of\tpushbroom cameras. Although pushbroom cameras have been used\tto recover scene structure in the past, the algorithms for\trecovery were developed separately for different camera\tmotions such as translation and rotation. In this paper, we\tpresent a general framework of structure recovery for\tpushbroom cameras with 6 degree-of-freedom motion. We analyze\tthe translation and rotation cases using our framework and\tdemonstrate that several previous results are really special\tcases of our result. Using this framework, we also show that\tthree or more pushbroom cameras can be used to compute scene\tstructure as well as motion of translation or rotation. We\tconclude with a set of experiments that demonstrate the use of\tpushbroom imaging to recover structure from unknown motion. (pdf) (ps) Improving the Coherence of Multi-document Summaries: a Corpus Study for Modeling the Syntactic Realization of Entities Ani Nenkova, Kathleen McKeown 2003-03-04 References included in multi-document summaries\tare often problematic. In this paper, we present a corpus\tstudy performed to derive statistical models for the syntactic\trealization of referential expressions. Our work shows how the\tsyntactic realization of entities can influence the coherence\tof the text and provides a model for rewriting references in\tmulti-document summaries to smooth disfluencies.\nIt shows how the syntactic realization of entities can\tinfluence the coherence of the text and how rewrite change s\tcan smooth the disfluencies. A large corpus study is conducted\tin order to derive initial models for syntactic realization. (pdf) (ps) Protecting SNMP Through MarketNet Marcela Jackowski 2002-12-19 As dependency on information technology becomes more critical so does the need for network computer security. Because of the distributed nature of networks, large-scale information systems are highly vulnerable to negative elements such as intruders and attackers. The types of attack on a system can be diverse and from different sources. Some of the factors contributing to creating an insecure system are the relentless pace of technology, the need for information processing, and the heterogeneity of hardware and software. In addition to these insecurities, the growth and success of e-commerce make networks a desirable target for intruders to steal credit card numbers, bank account balances, and other valuable information. This paper looks at two different security technologies, SNMP v3 and MarketNet, their architectures and how they have been developed to protect network resources and services, such as, internet applications, devices, and other services, against attacks. (pdf) (ps) Survivor: An Approach for Adding Dependability to Legacy Workflow Systems Jean-Denis Gr??ze,  Gail E. Kaiser,  Gaura 2002-12-02 Although they often provide critical services, most workflow systems are not dependable. There has been much literature on dependable/survivable distributed systems; most is concerned with developing new architectures, not adapting pre-existing ones. Additionally, the literature is focused on hardening, security-based defense, as opposed to recovery. For deployed systems, it is often infeasible to completely replace existing infrastructures; what is more pragmatic are ways in which existing distributed systems can be adapted to offer better dependability. In this paper, we outline a general architecture that can easily be retrofitted to legacy workflow systems in order to improve dependability and fault tolerance. We do this by monitoring enactment and replicating partial workflow states as tools for detection, analysis and recovery. We discuss some policies that can guide these mechanisms. Finally, we describe and evaluate our implementation, Survivor, which modified an existing workflow system provided by the Naval Research Lab. (pdf) (ps) CASPER: Compiler-Assisted Securing of Programs at Runtime Gaurav S. Kc,  Stephen A. Edwards,  Gail E 2002-11-19 Ensuring the security and integrity of computer systems deployed on the Internet is growing harder. This is especially true in the case of server systems based on open source projects like Linux, Apache, Sendmail, etc. since it is easier for a hacker to get access to the binary format of deployed applications if the source code to the software is publicly accessible. Often, having a binary copy of an application program is enough to help locate security vulnerabilities in the program. In the case of legacy systems where the source code is not available, advanced reverse-engineering and decompilation techniques can be used to construct attacks.\nThis paper focuses on measures that reduce the effectiveness of hackers at conducting large-scale, distributed attacks. The first line of defense involves additional runtime checks that are able to counteract the majority of hacking attacks. Introducing diversity in deployed systems to severely diminish the probability of propagation to other systems helps to prevent effective attacks like the DDOS attack against the DNS root servers in October 21, 2002. (pdf) (ps) DOM-based Content Extraction of HTML Documents Suhit Gupta,  Gail Kaiser,  David Neistadt 2002-11-15 Web pages often contain clutter around the body of the article as well as distracting features that take away from the true information that the user is pursuing. This can range from pop-up ads to flashy banners to unnecessary images and links scattered around the screen. Extraction of \"useful and relevant\" content from web pages, has many applications ranging from lightweight environments, like cell phone and PDA browsing, to speech rendering for the visually impaired, to text summarization Most approaches to removing the clutter or making the content more readable involves either changing the size of the font or simply removing certain HTML-denoted components like images, thus taking away from the webpage's inherent look and feel. Unlike Content Reformatting, which aims to reproduce the entire webpage in a more convenient form, our solution directly addresses Content Extraction. We have developed a framework that employs an easily extensible set of techniques that incorporate advantages of previous work on content extraction while limiting the disadvantages. Our key insight is to work with the Document Object Model tree (after parsing and correcting the HTML), rather than with raw HTML markup. We have implemented our approach in a publicly available Web proxy that anyone can use to extract content from HTML web pages for their own purposes. (pdf) (ps) On Buffered Clos Switches Santosh Krishnan, Henning Schulzrinne 2002-11-13 There is a widespread interest in switching architectures that can scale in capacity with increasing interface transmission rates and higher port counts. Furthermore, packet switches that provide Quality of Service (QoS), such as bandwidth and delay guarantees, to the served user traffic are also highly desired. This report addresses the issue of constructing a high-capacity QoS-capable, multi-module switching node.\nOutput queued switches provide the best performance in terms of throughput as well as QoS but do not scale. Input queued switches, on the other hand, require complex arbitration procedures to achieve the same level of performance. We enumerate the design constraints in the construction of a packet switch and present several approaches to build a system composed of lower-capacity memory and space elements, and analyze their performance. Towards this goal, we establish a new taxonomy for a class of switches, which we call Buffered Clos switches, and present a formal framework for optimal packet switching performance, in terms of both throughput and QoS.\nWithin the taxonomy, we augment the existing combined input-output queueing (CIOQ) systems with Aggregation and Pipelining techniques. Furthermore, we present the design and analysis of a novel parallel packet switch architecture. For the items in the taxonomy, we present algorithms that provide optimal throughput and QoS, in accordance with the above performance framework. While some of the presented ideas are still in the investigative stage, we believe that the current state of the work, especially the formal treatment of switching, will be beneficial to the ongoing research in the field. (pdf) (ps) An Overview of Information-Based Complexity Arthur G. Werschulz 2002-10-16 \\emph{Computational complexity} has two goals: finding the inherent cost of some problem, and finding optimal algorithms for solving this problem. \\emph{Information-based complexity} (IBC) studies the complexity of problems for which the available information is partial, contaminated by error, and priced. Examples of such problems include integration, approximation, ordinary and partial differential equations, integral equations, and the solution of nonlinear problems such as root-finding and optimization. In this talk, we give a brief overview of IBC. We focus mainly on the integration problem (which is a simple, yet important, problem that can be used to illustrate the main ideas of IBC) and the approximation problem (which will be of most interest to specialists in learning theory). One important issue that we discuss is the ``curse of dimension''---the depressing fact that the worst case complexity of many problems depends exponentially on dimension, rendering them intractable. We explore IBC-based techniques for vanquishing the curse of dimension. In particular, we find that randomization beats intractability for the integration problem but not for the approximation problem; on the other hand, both these problems are tractable in the average case setting under a Wiener sheet measure. (pdf) (ps) An Approach to Autonomizing Legacy Systems Gail Kaiser, Phil Gross, Gaurav Kc, Janak Parekh, Guiseppe Valetto 2002-09-14 Adding adaptation capabilities to existing distributed systems is a major concern. The question addressed here is how to retrofit existing systems with self-healing, adaptation and/or self management capabilities. The problem is obviously intensified for \"systems of systems\" composed of components, whether new or legacy, that may have been developed by different vendors, mixing and matching COTS and \"open source\" components. This system composition model is expected to be increasingly common in high performance computing. The usual approach is to train technicians to understand the complexities of these components and their connections, including performance tuning parameters, so that they can then manually monitor and reconfigure the system as needed. We envision instead attaching a \"standard\" feedback loop infrastructure to existing distributed systems for the purposes of continual monitoring and dynamically adapting their activities and performance. (This approach can also be applied to \"new\" systems, as an alternative to \"building in\" adaptation facilities, but we do not address that here.) Our proposed infrastructure consists of multiple layers with the objectives of probing, measuring and reporting of activity and state within the execution of the legacy system among its components and connectors; gauging, analysis and interpretation of the reported events; and possible feedback to focus the probes and gauges to drill deeper, or when necessary - direct but automatic reconfiguration of the running system. (pdf) (ps) Using Process Technology to Control and Coordinate Software Adaptation Giuseppe Valetto, Gail Kaiser 2002-09-14 We have developed an infrastructure for end-to-end run-time monitoring, behavior / performance analysis, and dynamic adaptation of distributed software applications. This feedback-loop infrastructure is primarily targeted to pre-existing systems and thus operates outside the application itself without making assumptions about the target system's internal communication/computation mechanisms, implementation language/framework, availability of source code, etc. This paper assumes the existence of the monitoring and analysis components, presented elsewhere, and focuses on the mechanisms used to control and coordinate possibly complex repairs/reconfigurations to the target system. These mechanisms require lower-level actuators or effectors somehow attached to the target system, so we briefly sketch one such facility (elaborated elsewhere). The core of the paper is the model, architecture, and implementation of Workflakes, the decentralized process engine we use to tailor, control, coordinate, respond to contingencies, etc. regarding a cohort of such actuators. We have validated our approach and the Workflakes prototype in several case studies, related to different application domains. Due to space restrictions we concentrate primarily on one case study, elaborate with some details a second, and only sketch others. (pdf) (ps) Evaluating Top-k Queries over Web-Accessible Databases Luis Gravano, Amelie Marian, Nicolas Bruno 2002-09-09 A query to a web search engine usually consists of a list of keywords, to which the search engine responds with the best or \"top\" k pages for the query. This top-k query model is prevalent over multimedia collections in general, but also over plain relational data for certain applications. For example, consider a relation with information on available restaurants, including their location, price range for one diner, and overall food rating. A user who queries such a relation might simply specify the user's location and target price range, and expect in return the best 10 restaurants in terms of some combination of proximity to the user, closeness of match to the target price range, and overall food rating. Processing such top-k queries efficiently is challenging for a number of reasons. One critical such reason is that, in many web applications, the relation attributes might not be available other than through external web-accessible form interfaces, which we will have to query repeatedly for a potentially large set of candidate objects. In this paper, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present several new algorithms for processing such queries, and adapt existing techniques to our scenario as well. We also study the execution time of our algorithms analytically and present experimental results using both synthetic and real web-accessible data. (pdf) (ps) A Case Study In Software Adaptation Guiseppe Valetto, Gail Kaiser 2002-09-09 We attach a feedback-control-loop infrastructure to an existing target system, to continually monitor and dynamically adapt its activities and performance. (This approach could also be applied to \"new\" systems, as an alternative to \"building in\" adaptation facilities, but we do not address that here.) Our infrastructure consists of multiple layers with the objectives of 1. probing, measuring and reporting of activity and state during the execution of the target system among its components and connectors; 2. gauging, analysis and interpretation of the reported events; and 3. whenever necessary, feedback onto the probes and gauges, to focus them (e.g., drill deeper), or onto the running target system, to direct its automatic adjustment and reconfiguration. We report on our successful experience using this approach in dynamic adaptation of a large-scale commercial application that requires both coarse and fine grained modifications. (pdf) (ps) Optimizing Top-K Selection Queries over Multimedia Repositories Surajit Chaudhuri, Luis Gravano, Amelie Marian 2002-08-20 Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A query on these attributes will typically request not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with each object, which indicates how well the object matches the selection condition (ranking). Furthermore, unlike in the relational model, users may just want the k top-ranked objects for their selection queries, for a relatively small k. In addition to the differences in the query model, another peculiarity of multimedia repositories is that they may allow access to the attributes of each object only through indexes. In this paper, we investigate how to optimize the processing of top-k selection queries over multimedia repositories. The access characteristics of the repositories and the above query model lead to novel issues in query optimization. In particular, the choice of the indexes used to search the repository strongly influences the cost of processing the filtering condition. We define an execution space that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem of picking an optimal plan in the search-minimal execution space is NP-hard, we present an efficient algorithm that solves the problem optimally when the predicates in the query are independent. We also show that the problem of optimizing top-k selection queries can be viewed, in many cases, as that of evaluating more traditional selection conditions. Thus, both problems can be viewed together as an extended filtering problem to which techniques of query processing and optimization may be adapted. (pdf) (ps) A Flexible and Efficient Protocol for Multi-Scope Service Registry Replication Weibin Zhao, Henning Schulzrinne 2002-07-29 Service registries play an important role in service discovery systems by accepting service registrations and answering service queries; they can serve a wide range of purposes, such as membership services, lookup services, and search services. To provide fault tolerant, and enhance scalability, availability and performance, service registries often need to be replicated. In this paper, we present Swift (Selective anti-entropy WIth FasT update propagation), a flexible and efficient protocol for multi-scope service registry replication. As consistency is a less of concern compared with availability in service registry replication, we choose to build Swift on top of anti-entropy to support high availability replication. Swift makes two contributions as follows. First, it defines a more general and flexible form of anti-entropy called selective anti-entropy, which extends the applicability of anti-entropy from full replication to partial replication by selectively reconciling inconsistent states between two replicas, and improves anti-entropy efficiency by fine controlling update propagation within each subset. Selective anti-entropy is the first that we are aware of in using anti-entropy to support generic partial replication. Secondly, Swift integrates service registry overlay networks with selective anti-entropy. Different topologies, such as full mesh and spanning tree, can be used for constructing service registry overlay networks. These overlay networks are used to propagate new updates quickly so as to minimize inconsistency among replicas. We have implemented Swift for replicating multi-scope Directory Agents in the Service Location Protocol. Our experience shows that Swift is flexible, efficient, and lightweight. (pdf) (ps) The Design and Implementation of Elastic Quotas: A System for Flexible File System Management Ozgur Can Leonard, Jason Nieh, Erez Zadok, Jeffrey Osborn, Ariye Shater, Charles Wright 2002-06-19 We introduce elastic quotas, a disk space management technique that makes disk space an elastic resource like CPU and memory. Elastic quotas allow all users to use unlimited amounts of available disk space while still providing system administrators the ability to control how the disk space is allocated among users. Elastic quotas maintain existing persistent file semantics while supporting user-controlled policies for removing files when the file system becomes too full. We have implemented an elastic quota system in Solaris and measured its performance. The system is simple to implement, requires no kernel modifications, and is compatible with existing disk space management methods. Our results show that elastic quotas are an effective, low-overhead solution for flexible file system management. (pdf) (ps) Distributed Search over the Hidden Web: Hierarchical Database Sampling and Selection Panagiotis G. Ipeirotis, Luis Gravano 2002-06-19 Many valuable text databases on the web have non-crawlable contents that are \"hidden\" behind search interfaces. Metasearchers are helpful tools for searching over many such databases at once through a unified query interface. A critical task for a metasearcher to process a query e??ciently and eRectively is the selection of the most promising databases for the query, a task that typically relies on statistical summaries of the database contents. Unfortunately, web accessible text databases do not generally export content summaries. In this paper, we present an algorithm to derive content summaries from \"uncooperative\" databases by using \"focused query probes,\" which adaptively zoom in on and extract documents that are representative of the topic coverage of the databases. Our content summaries are the first to include absolute document frequency estimates for the database words. We also present a novel database selection algorithm that exploits both the extracted content summaries and a hierarchical classification of the databases, automatically derived during probing, to compensate for potentially incomplete content summaries. Finally, we evaluate our techniques thoroughly using a variety of databases, including 50 real web-accessible text databases. Our experiments indicate that our new content-summary construction technique is effcient and produces more accurate summaries than those from previously proposed strategies. Also, our hierarchical database selection algorithm exhibits significantly higher precision than its flat counterparts. (pdf) (ps) Requirements for Scalable Access Control and Security Management Architectures Angelos D. Keromytis, Jonathan M. Smith 2002-05-16 Maximizing local autonomy has led to a scalable Internet. Scalability and the capacity for distributed control have unfortunately not extended well to resource access control policies and mechanisms. Yet management of security is becoming an increasingly challenging problem, in no small part due to scaling up of measures such as number of users, protocols, applications, network elements, topological constraints, and functionality expectations.\nIn this paper we discuss scalability challenges for traditional access control mechanisms and present a set of fundamental requirements for authorization services in large scale networks. We show why existing mechanisms fail to meet these requirements, and investigate the current design options for a scalable access control architecture.\nWe argue that the key design options to achieve scalability are the choice of the representation of access control policy, the distribution mechanism for policy and the choice of access rights revocation scheme. (pdf) (ps) Using Density Estimation to Improve Text Categorization Carl Sable, Kathleen McKeown, Vassilis Hatzivassiloglou 2002-05-01 This paper explores the use of a statistical technique known as density estimation to potentially improve the results of text categorization systems which label documents by computing similarities between documents and categories. In addition to potentially improving a system's overall accuracy, density estimation converts similarity scores to probabilities. These probabilities provide confidence measures for a system's predictions which are easily interpretable and could potentially help to combine results of various systems. We discuss the results of three complete experiments on three separate data sets applying density estimation to the results of a TF*IDF/Rocchio system, and we compare these results to those of many competing approaches. (pdf) (ps) CINEMA: Columbia InterNet Extensible Multimedia Architecture Kundan Singh, Wenyu Jiang, Jonathan Lennox, Sankaran Narayanan, Henning Schulzrinne 2002-04-26 We describe the architecture and implementation of our Internet telephony system (CINEMA: Columbia InterNet Extensible Multimedia Architecture) intended to replace the departmental PBX (telephone switch). It interworks with the traditional telephone networks via a PSTN/IP gateway. It also serves as a corporate or campus infrastructure for existing and future services like web, email, video and streaming media. Initially intended for a few users, it will eventually replace the plain old telephones from our offices, due to the cost benefit and new services it offers. We also discuss common inter-operability problems between the PBX and the gateway. This paper is intended as a design document of the overall system. (pdf) (ps) Analysis of Routing Algorithms for Secure Overlay Service Debra Cook 2002-04-23 The routing of packets through an overlay network designed to limit DDoS attacks is analyzed. The overlay network structure includes special purpose nodes which affect the routes taken through the overlay. Two main factors are considered: the routing algorithm utilized for the overlay and the method for selecting the special purpose nodes. The routing algorithms considered are based on methods originally defined for peer-to-peer services. A model was developed for the overlay network which allowed altering the routing algorithm, method for selection of special purpose nodes and the underlying ISP structure. The model was used to assess the impact of specific routing algorithms and selection methods on latency and path length. The implications of utilizing a specific method for node selection on the probability of a successful DDoS attack is briefly discussed. (pdf) (ps) Session-Aware Popularity-based Resource Allocation Across Several Differentiated Service Domains Paulo Mendes, Henning Schulzrinne, Edmundo Monteiro 2002-04-17 The Differentiated Services model (DS) maps traffic into network services that have different quality levels. However, inside each service flows can be treated unfairly, since the DS model has no policy to distribute the service bandwidth between all sessions that compose the service aggregate traffic. Our goal is to study a signaling protocol that fairly distributes the resources reserved for each DS service between scalable multimedia sessions in a multicast network environment, where scalable sources divide session traffic in hierarchical layers, sending each layer to different multicast groups. We present a signaling protocol called Session-Aware Popularity Resource Allocation (SAPRA) that allows the distribution of DS services resources along sessions path, based upon the receiver population of each session. Simulations show that SAPRA protocol has small bandwidth overhead, is efficient updating the resources allocated to each session and also supplying receivers with reports about the quality level of their session. (pdf) (ps) Querying Large Text Databases for Efficient Information Extraction Eugene Agichtein, Luis Gravano 2002-03-22 A wealth of data is hidden within unstructured text. This data is often best exploited in structured or relational form, which is suited for sophisticated query processing, for integration with relational databases, and for data mining. Current information extraction techniques extract relations from a text database by examining every document in the database. This exhaustive approach is not practical, or sometimes even feasible, for large databases. In this paper, we develop an efficient query-based technique to identify documents that are potentially useful for the extraction of a target relation. We start by sampling the database to characterize the documents from which an information extraction system manages to extract relevant tuples. Then, we apply machine learning and information retrieval techniques to derive queries likely to match additional useful documents in the database. Finally, we issue these queries to the database to retrieve documents from which the information extraction system can extract the final relation. Our technique requires that databases support only a minimal boolean query interface, and is independent of the choice of the underlying information extraction system. We report a thorough experimental evaluation over more than one million documents that shows that we significantly improve the efficiency of the extraction process by focusing only on promising documents. Our proposed technique could be used to query a standard web search engine, hence providing a building block for efficient information extraction over the web at large. (pdf) (ps) Combining Mobile Agents and Process-based Coordination to Achieve Software Adaptation Guiseppe Valetto, Gail Kaiser 2002-03-17 We have developed a model and a platform for end-to-end run-time monitoring, behavior and performance analysis, and consequent dynamic adaptation of distributed applications. This paper concentrates on how we coordinate and actuate the potentially multi-part adaptation, operating externally to the target systems, that is, without requiring any a priori built-in adaptation facilities on the part of said target systems. The actual changes are performed on the fly onto the target by communities of mobile software agents, coordinated by a decentralized process engine. These changes can be coarse-grained, such as replacing entire components or rearranging the connections among components, or fine-grained, such as changing the operational parameters, internal state and functioning logic of individual components. We discuss our successful experience using our approach in dynamic adaptation of a large-scale commercial application, which requires both coarse and fine grained modifications. (pdf) (ps) Holistic Twig Joins: Optimal XML Pattern Matching Nicolas Bruno, Nick Koudas, Divesh Srivastava 2002-03-12 XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i)~using structural join algorithms to match the binary relationships against the XML database, and (ii)~stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.\nIn this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns. (pdf) (ps) SIPstone - Benchmarking SIP Server Performance Henning Schulzrinne, Sankaran Narayanan, Jonathan Lennox, Michael Doyle 2002-03-11 SIP-based Internet telephony systems need to be appropriately dimensioned, as the call and registration rate can reach several thousand requests a second. This draft proposes an initial simple set of metrics for evaluating and benchmarking the performance of SIP proxy, redirect and registrar servers. The benchmark SIPstone-A expresses a weighted average of these metrics. (pdf) (ps) Performance of information discovery and message relaying in mobile ad hoc networks Maria Papadopouli, Henning Schulzrinne 2002-03-09 This paper presents 7DS, a novel peer-to-peer resource sharing system. 7DS is an architecture, a set of protocols and an implementation enabling the exchange of data among peers that are not necessarily connected to the Internet. Peers can be either mobile or stationary. We focus on three different facets of cooperation, namely, data sharing, message relaying and network connection sharing. 7DS enables wireless devices to discover, disseminate, relay information among each other to increase the data access. We evaluate via extensive simulations the effectiveness of our system for data dissemination and message relaying among mobile devices with a large number of user mobility scenarios. We model several general data dissemination approaches and investigate the effect of the wireless coverage range, 7DS host density, and cooperation strategy among the mobile hosts as a function of time. We also present a power conservation mechanism that is beneficial, since it increases the power savings, without degrading the data dissemination. Using theory from random walks, random environments and diffusion of controlled processes, we model one of these data dissemination schemes and show that the analysis confirms the simulation results for this scheme. (pdf) (ps) Signalling Transport Protocols Gonzalo Camarillo, Henning Schulzrinne, Raimo Kantola 2002-02-18 SCTP is a newly developed transport protocol tailored for signalling transport. Whereas in theory SCTP is supposed to achieve a much better performance than TCP and UDP, at present there are no ex-perimental results showing SCTP s real benefits. This paper analyzes SCTP s strengths and weaknesses and provides simulation results. We implemented SIP on top of UDP, TCP and SCTP in the network simulator and compared the three transport protocols under different network conditions. (pdf) (ps) Mobile Communication with Virtual Network Address Translation Gong Su, Jason Nieh 2002-02-18 Virtual Network Address Translation (VNAT) is a novel architecture that allows transparent migration of end-to-end live network connections associated with various computation units. Such computation units can be either a single process, or a group of processes of an application, or an entire host. VNAT virtualizes network connections perceived by transport protocols so that identification of network connections is decoupled from stationary hosts. Such virtual connections are then remapped into physical connections to be carried on the physical network using network address translation. VNAT requires no modification to existing applications, operating systems, or protocol stacks. Furthermore, it is fully compatible with the existing communication infrastructure; virtual and normal connections can coexist without interfering each other. VNAT functions entirely within end systems and requires no third party proxies. We have implemented a VNAT prototype with the Linux 2.4 kernel and demonstrated its functionality on a wide range of popular real-world network applications. Our performance results show that VNAT has essentially no overhead except when connections are migrated, in which case the overhead of our Linux prototype is less than 7 percent. (pdf) (ps) Extending SDARTS: Extracting Metadata from Web Databases and Interfacing with the Open Archives Initiative Panagiotis G. Ipeirotis, Tom Barry, Luis Gravano 2002-02-01 SDARTS is a protocol and toolkit designed to facilitate metasearching. SDARTS combines two complementary existing protocols, SDLIP and STARTS, to define a uniform interface that collections should support for searching and exporting metasearch-related metadata. SDARTS also includes a toolkit with wrappers that are easily customized to make both local and remote document collections SDARTS-compliant. This paper describes two significant ways in which we have extended the SDARTS toolkit. First, we have added a tool that automatically builds rich content summaries for remote web collections by probing the collections with appropriate queries. These content summaries can then be used by a metasearcher to select over which collections to evaluate a given query. Second, we have enhanced the SDARTS toolkit so that all SDARTS-compliant collections export their metadata under the emerging Open Archives Initiative (OAI) protocol. Conversely, the SDARTS toolkit now also allows all OAI-compliant collections to be made SDARTS-compliant with minimal effort. As a result, we implemented a bridge between SDARTS and OAI, which will facilitate easy interoperability among a potentially large number of collections. The SDARTS toolkit, with all related documentation and source code, is publicly available at http://sdarts.cs.columbia.edu. (pdf) (ps) The Design of High-Throughput Asynchronous Pipelines Montek Singh 2001-12-31 Clocked or synchronous design has traditionally been used for nearly all digital systems. However, it is now facing significant challenges as clock rates reach several GigaHertz, chip sizes increase, and the demand for low power and modular design become paramount. An alternative paradigm is clockless or asynchronous design, which has several potential advantages towards meeting these challenges.\nThis thesis focuses on the design of very high-speed asynchronous systems. A more specific focus of this thesis is on high-throughput asynchronous pipelines, since pipelining is at the heart of most high-performance systems.\nThis thesis contributes four new asynchronous pipeline styles: \"lookahead,\" \"high-capacity,\" \"MOUSETRAP\" and \"dynamic GasP\" pipelines. The styles differ from each other in many aspects, such as protocols, storage capacity, implementation style, and timing assumptions. The new styles are capable of multi-GigaHertz throughputs in today's silicon technology (e.g., 0.13-0.18 micron), yet each style has a simple implementation. High throughputs are obtained through efficient pipelining of systems at a fine granularity, though the pipeline styles are also useful for coarser-grain applications.\nThe basic pipeline styles are extended to address several issues that arise in practice while designing real-world systems. In particular, the styles are generalized to handle a greater variety of architectures (e.g., datapaths with forks and joins), and to robustly interface with arbitrary-speed environments.\nFinally, the approaches of this thesis are validated by designing and fabricating real VLSI subsystems, including: simple FIFO's, pipelined adders, and an experimental digital FIR filter chip. All chips were tested to be fully functional; throughputs of over 2.4 GHz for the FIFO's, and up to 1.3 GHz for the FIR filter, were obtained in an IBM 0.18 micron technology. (pdf) (ps) G.729 Error Recovery for Internet Telephony Jonathan Rosenberg 2001-12-19 This memorandum discusses the use of the ITU G.729 CS-ACELP speech coder on the Internet for telephony applications. In particular, the memo explores issues of error resiliency and recovery. In particular, the paper considers two questions. First, given N consecutive frame erasures (due to packet losses), how long does the decoder take to resynchronize its state with the encoder? What is the strength of the resulting error signal, both in terms of objective and subjective measures? The second issue explores which particular factors contribute to the strength of the error signal: the distortion of the speech due to the incorrect concealment of the erasures, or the subsequent distortion due to the loss of state synchronization, even though correct packets are being received. Both objective and subjective measures are used to characterize the importance of each of these two factors. (pdf) (ps) Where Does Smoothness Count the Most For Fredholm\n        Equations of the Second Kind With Noisy Information? Arthur G. Werschulz 2001-12-19 We study the complexity of Fredholm problems $(I-T_k)u=f$ of the second kind on the $I^d=[0,1]^d$. Previous work on the complexity of this problem has assumed either that we had complete information about the kernel~$k$ or that the kernel~$k$ and the right-hand side~$f$ had the same smoothness; moreover, this information is usually exact. In this paper, we suppose that $f\\in W^{r,p}(I^d)$ and $k\\in W^{s,p}(I^d)$. We also assume that $\\delta$-noisy standard information is available. We find that the $n$th minimal error is $\\Theta(n^{-\\mu}+\\delta)$, where $\\mu = \\min\\{r/d, s/(2d)\\}$, and that a noisy modified finite element method (MFEM) has nearly minimal error. This noisy modified FEM can be efficiently implemented using multigrid techniques. Using these results, we find tight bounds on the $\\varepsilon$-complexity for this problem, said bounds depending on the cost~$c(\\delta)$ of calculating a $\\delta$-noisy information value. As an example, if the cost of a $\\delta$-noisy evaluation is proportional to $\\delta^{-t}$, then the $\\varepsilon$-complexity is roughly $(1/\\varepsilon)^{1/\\mu+t}$. (pdf) (ps) Surface Approximation May Be Easier Than Surface Integration Arthur G. Werschulz, Henryk Wozniakowski 2001-12-19 The approximation and integration problems consist of finding an approximation to a function~$f$ or its integral over some fixed domain~$\\Sigma$. For the classical version of these problems, we have partial information about the functions~$f$ and complete information about the domain~$\\Sigma$; for example, $\\Sigma$ might be a cube or ball in~$\\mathbb R^d$. When this holds, it is generally the case that integration is not harder than approximation; moreover, integration can be much easier than approximation. What happens if we have partial information about~$\\Sigma$? This paper studies the surface approximation and surface integration problems, in which $\\Sigma=\\Sigma_g$ for functions~$g$. More specifically, our functions~$f$ are $r$~times continuously differentiable scalar functions of $l$~variables, and $g$ are $s$~times continuously differentiable injective functions of~$d$ variables with $l$~components. Error for the surface approximation problem is measured in the $L_q$-sense. Our problems are well-defined, provided that $d\\le l$, $r\\ge 0$, and $s\\ge 1$. Information consists of function evaluations of~$f$ and~$g$. We show that the $\\varepsilon$-complexity of surface approximation is proportional to $(1/\\varepsilon)^{1/\\mu}$ with $\\mu=\\mrs/d$. We also show that if $s\\ge 2$, then the $\\varepsilon$-complexity of surface integration is proportional to $(1/\\varepsilon)^{1/\\nu}$ with $$\\nu=\\min\\left\\{ \\frac{r}{d},\\frac{s-\\delta_{s,1}(1-\\delta_{d,l})}{\\min\\{d,l-1\\}}\\right\\}.$$ (This bound holds as well for several subcases of $s=1$; we conjecture that it holds for all $r\\ge0$, $s\\ge1$, and $d\\le l$.) Using these results, we determine when surface approximation is easier than, as easy as, or harder than, surface integration; all three possibilities can happen. In particular, we find that if $s=1$ and $dd/p$. Information consists of(possibly-adaptive) noisy evaluations of $f$, $a$, or~$b$ (or theirderivatives). The absolute error in each noisy evaluation is atmost~$\\delta$. We find that the $n$th minimal radius for this problemis proportional to $n^{-r/d}+\\delta$, and that a noisy finite elementmethod with quadrature (FEMQ), which uses only function values, andnot derivatives, is a minimal error algorithm. This noisy FEMQ can beefficiently implemented using multigrid techniques. Using theseresults, we find tight bounds on the $\\varepsilon$-complexity (minimalcost of calculating an $\\varepsilon$-approximation) for this problem,said bounds depending on the cost~$c(\\delta)$ of calculating a$\\delta$-noisy information value. As an example, if the cost of a$\\delta$-noisy evaluation is $c(\\delta)=\\delta^{-s}$ (for $s>0$), thenthe complexity is proportional to $(1/\\varepsilon)^{d/r+s}$. (ps) Adapting Materialized Views after Redefinitions: Techniques and a Performance Study Ashish Gupta, Inderpal S. Mumick, Jun Rao, Kenneth A. Ross 1997-03-14 We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to ``adapt'' the view in response to changes in the view definition.\nSuch techniques are needed for applications where the user can changequeries dynamically and see the changes in the results fast. Dataarchaeology, data visualization, and dynamic queries are examples ofsuch applications.\nWe consider all possible redefinitions of SQL\\Select-\\From-\\Where-\\Groupby-\\Having, \\Union, and \\Minus\\ views, and show howthese views can be adapted using the old materialization for the caseswhere it is possible to do so. We identify extra informationthat can be kept with a materialization to facilitate redefinition.Multiple simultaneous changesto a view can be handledwithout necessarily materializing intermediate results. Weidentify guidelines for users and database administrators that can beused to facilitate efficient view adaptation.\nWe perform a systematic experimental evaluation of our proposedtechniques. Our evaluation indicates that adaptation is more efficient thanrematerialization in most cases. Certain adaptation techniques can be up to1,000 times better. We also point out the physical layouts that can benefitadaptation. (ps) Federating Process-Centered Environments: the Oz Experience Israel Z. Ben-Shaul, Gail E. Kaiser 1997-03-10 (ps) WWW-based Collaboration Environments with Distributed Tool Services Gail E. Kaiser, Stephen E. Dossick, Wenyu Jiang, Jack Jingshuang Yang, Sonny Xi Ye 1997-03-03 We have developed an architecture and realizationof a framework for hypermedia collaboration environments that supportpurposeful work by orchestrated teams. The hypermedia represents allplausible multimedia artifacts concerned with the collaborativetask(s) at hand that can be placed or generated on-line, fromapplication-specific materials (e.g., source code, chip layouts,blueprints) to formal documentation to digital library resources toinformal email and chat transcripts. The environment capabilitiesinclude both internal (hypertext) and external (link server) linksamong these artifacts, which can be added incrementally as usefulconnections are discovered; project-specific hypermedia search andbrowsing; automated construction of artifacts and hyperlinks accordingto the semantics of the group and individual tasks and the overallprocess workflow; application of tools to the artifacts; andcollaborative work for geographically dispersed teams.\nWe present a general architecture for what we call hypermedia {\\emsubwebs}, and imposition of {\\em groupspace} services operating onshared subwebs, based on World Wide Web technology --- which could beapplied over the Internet and/or within an organizational intranet.We describe our realization in OzWeb, which reuses object-orienteddata management for application-specific subweb organization, andworkflow enactment and cooperative transactions as built-in groupspaceservices, which were originally developed for the Oz process-centeredsoftware development environment framework. Further, we present ageneral architecture for a WWW-based distributed tool launchingservice. This service is implemented by the generic Rivendellcomponent, which could be employed in a stand-alone manner, but hasbeen integrated into OzWeb as an example ``foreign'' (i.e., add-on)groupspace service. (ps) FiST: A File System Component Compiler (Ph.D. Thesis Proposal) Erez Zadok 1997-01-01 (ps) An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning Philip Chan 1996-10-18 Much of the research in inductive learning concentrateson problems with relatively small amounts of data. With the coming age ofubiquitous network computing, it is likely that orders of magnitudemore data in databases will be available for various learning problemsof real world importance. Some learning algorithms assume that theentire data set fits into main memory, which is not feasible formassive amounts of data, especially for applications in data mining.One approach to handling a large data set is to partition the data setinto subsets, run the learning algorithm on each of the subsets, andcombine the results. Moreover, data can be inherently distributedacross multiple sites on the network and merging all the data in onelocation can be expensive or prohibitive.\nIn this thesis we propose, investigate, and evaluate a {\\itmeta-learning} approach to integrating the results of multiplelearning processes. Our approach utilizes machine learning to guidethe integration. We identified two main meta-learning strategies:{\\it combiner} and {\\it arbiter}. Both strategies are independent tothe learning algorithms used in generating the classifiers. Thecombiner strategy attempts to reveal relationships among the learnedclassifiers' prediction patterns. The arbiter strategy tries todetermine the correct prediction when the classifiers have differentopinions. Various schemes under these two strategies have beendeveloped. Empirical results show that our schemes can obtainaccurate classifiers from inaccurate classifiers trained from datasubsets. We also implemented and analyzed the schemes in a paralleland distributed environment to demonstrate their scalability. (ps) Distributed Tool Services via the World Wide Web Steve Dossick, Gail Kaiser, Jack Jingshuang Yang 1996-10-15 We present an architecture for a distributed toolservice which operates over HTTP, the underlying protocol of the WorldWide Web. This allows unmodified Web browsers to request toolexecutions from the server as well as making integration with existingsystems easier. We describe Rivendell, a prototype implementation ofthe architecture. (ps) The complexity of definite elliptic problems with noisy data Arthur G. Werschulz 1996-09-11 We study the complexity of $2m$th order definiteelliptic problems $Lu=f$ (with homogeneous Dirichlet boundaryconditions) over a $d$-dimensional domain~$\\Omega$, error beingmeasured in the $H^m(\\Omega)$-norm. The problem elements~$f$ belongto the unit ball of~$W^{r,p}(\\Omega)$, where $p\\in[2,\\infty]$ and$r>d/p$. Information consists of (possibly-adaptive) noisyevaluations of~$f$ or the coefficients of~$L$. The absolute error ineach noisy evaluation is at most~$\\delta$. We find that the $n$thminimal radius for this problem is proportional to $n^{-r/d}+\\delta$,and that a noisy finite element method with quadrature (FEMQ), whichuses only function values, and not derivatives, is a minimal erroralgorithm. This noisy FEMQ can be efficiently implemented usingmultigrid techniques. Using these results, we find tight bounds onthe $\\varepsilon$-complexity (minimal cost of calculating an$\\varepsilon$-approximation) for this problem, said bounds dependingon the cost~$c(\\delta)$ of calculating a $\\delta$-noisy informationvalue. As an example, if the cost of a $\\delta$-noisy evaluation is$c(\\delta)=\\delta^{-s}$ (for $s>0$), then the complexity isproportional to $(1/\\varepsilon)^{d/r+s}$. (ps) An Architecture for WWW-based Hypercode Environments Gail E. Kaiser, Stephen E. Dossick, Wenyu Jiang, Jack Jingshuang Yang 1996-08-09 A hypercode software engineering environmentrepresents all plausible multimedia artifacts concerned with softwaredevelopment and evolution that can be placed or generated on-line,from source code to formal documentation to digital library resourcesto informal email and chat transcripts. A hypercode environmentsupports both internal (hypertext) and external (link server) linksamong these artifacts, which can be added incrementally as usefulconnections are discovered; project-specific hypermedia search andbrowsing; automated construction of artifacts and hyperlinks accordingthe software process; application of tools to the artifacts accordingto the process workflow; and collaborative work for geographicallydispersed teams. We present a general architecture for what we callhypermedia subwebs, and groupspace services operating on sharedsubwebs, based on World Wide Web technology which could be appliedover the Internet or within an intranet. We describe our realizationin OzWeb. (ps) Reflectance and Texture of Real-World Surfaces: Summary Report Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, Jan J. Koenderink 1996-07-20 In this work, we investigate the visual appearanceof real-world surfaces and the dependence of appearance on scale,viewing direction and illumination direction. At fine scale, surfacevariations cause local intensity variation or image texture. Theappearance of this texture depends on both illumination and viewingdirection and can be characterized by the BTF (bidirectional texturefunction). At sufficiently coarse scale, local image texture is notresolvable and local image intensity is uniform. The dependence ofthis image intensity on illumination and viewing direction isdescribed by the BRDF (bidirectional reflectance distributionfunction). We simultaneously measure the BTF and BRDF of over 60different rough surfaces, each observed with over 200 differentcombinations of viewing and illumination direction. The resulting BTFdatabase is comprised of over 12,000 image textures. To enableconvenient use of the BRDF measurements, we fit the measurements totwo recent models and obtain a BRDF parameter database. Theseparameters can be used directly in image analysis and synthesis of awide variety of surfaces. The BTF, BRDF, and BRDF parameter databaseshave important implications for computer vision and computer graphicsand each is made publicly available. (ps) An Analytical Approach to File Prefetching Hui Lei, Dan Duchamp 1996-06-26 File prefetching is an effective technique forimproving file access performance. In this paper, we present a fileprefetching mechanism that is based on on-line analytic modeling ofinteresting system events and is transparent to higher levels. Themechanism, incorporated into a client's file cache manager, seeks tobuild semantic structures, called access trees, that capture thecorrelations between file accesses. It then heuristically uses thesestructures to represent distinct file usage patterns and exploits themto prefetch files from a file server. We show results of a simulationstudy and of a working implementation. Measurements suggest that ourmethod can predict future file accesses with an accuracy around 90\\%,that it can reduce cache miss rate by up to 47\\% and applicationlatency by up to 40\\%. Our method imposes little overhead, even underantagonistic circumstances. (ps) Fast Joins Using Join Indices Zhe Li, Kenneth Ross 1996-06-26 Two new algorithms, ``Jive-join'' and``Slam-join,'' are proposed for computing the join of two relationsusing a join index. The algorithms are duals: Jive-joinrange-partitions input relation tuple-ids then processes eachpartition, while Slam-join forms ordered runs of input relationtuple-ids and then merges the results. Each algorithm has featuresthat make it preferable to the other depending on the context in whichit is being used. Both algorithms make a single sequential passthrough each input relation, in addition to one pass through the joinindex and two passes through a temporary file whose size is half thatof the join index. Both algorithms perform this efficiently even whenthe relations are much larger than main memory, as long as the numberof blocks in main memory is of the order of the square root of thenumber of blocks in the smaller relation. By storing intermediate andfinal join results in a vertically partitioned fashion, our algorithmsneed to manipulate less data in memory at a given time than otheralgorithms. Almost all the I/O of our algorithms is sequential, thusminimizing the impact of seek and rotational latency. The algorithmsare resistant to data skew and adaptive to memory fluctuations. Theycan be extended to handle joins of multiple relations by usingmultidimensional partitioning while still making only a single passover each input relation. They can also be extended to handle joinsof relations that do not satisfy the memory bound by recursivelyapplying the algorithms. We also show how selection conditions can beincorporated into the algorithms. Using a detailed cost model, thealgorithms are analyzed and compared with competing algorithms. Forlarge input relations, our algorithms perform significantly betterthan Valduriez's algorithm and hash join algorithms. An experimentalstudy is also conducted to validate the analytical results and todemonstrate the performance characteristics of each algorithm inpractice. (ps) Reflectance and Texture of Real-World Surfaces Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, Jan J. Koenderink 1996-06-24 (ps) New methodologies for valuing derivatives Spassimir Paskov 1996-06-10 High-dimensional integrals are usually solved withMonte Carlo algorithms although theory suggests that low discrepancyalgorithms are sometimes superior. We report on numerical testingwhich compares low discrepancy and Monte Carlo algorithms on theevaluation of financial derivatives. The testing is performed on aCollateralized Mortgage Obligation (CMO) which is formulated as thecomputation of ten integrals of dimension up to 360. We tested two low discrepancy algorithms (Sobol and Halton) and tworandomized algorithms (classical Monte Carlo and Monte Carlo combinedwith antithetic variables). We conclude that for this CMO the Sobolalgorithm is always superior to the other algorithms. We believe thatit will be advantageous to use the Sobol algorithm for many othertypes of financial derivatives. Our conclusion regarding the superiority of the Sobol algorithm alsoholds when a rather small number of sample points are used, animportant case in practice. We have built a software system called FINDER for computing highdimensional integrals. Routines for computing Sobol points have beenpublished. However, we incorporated major improvements in FINDER andwe stress that the results reported here were obtained using thissoftware. The software system FINDER runs on a network of heterogeneousworkstations under PVM 3.2 (Parallel Virtual Machine). Sinceworkstations are ubiquitous, this is a cost-effective way to do verylarge computations fast. The measured speedup is at least $.9N$ for$N$ workstations, $N \\leq 25$. The software can also be used tocompute high dimensional integrals on a single workstation. (ps) Faster valuation of financial derivatives Spassimir Paskov 1996-06-10 Monte Carlo simulation is widely used to valuecomplex financial instruments. An alternative to Monte Carlo is touse ``low discrepancy'' methods. Theory suggests that low discrepancymethods might be superior to the Monte Carlo method. We compared theperformance of low discrepancy methods with Monte Carlo on aCollateralized Mortgage Obligation (CMO) with ten tranches. We foundthat a particular low discrepancy method based on Sobol pointsconsistently outperforms Monte Carlo. Although our tests were for aCMO, we believe it will be advantageous to use the Sobol method formany other types of instruments. We have made major improvements inpublished routines for generating Sobol points which we have embeddedin a software system called FINDER. (ps) New Results on Deterministic Pricing of Financial Derivatives A. Papageorgiou, J.F. Traub 1996-06-01 (ps) Closed Terminologies and Temporal Reasoning in Description Logic for Concept and Plan Recognition (phd thesis) Robert Anthony Weida 1996-05-29 Description logics are knowledge representationformalisms in the tradition of frames and semantic networks, but withan emphasis on formal semantics. A terminology contains descriptionsof concepts, such as UNIVERSITY, which are automatically classified ina taxonomy via subsumption inferences. Individuals such as COLUMBIAare described in terms of those concepts. This thesis enhances thescope and utility of description logics by exploiting new completenessassumptions during problem solving and by extending the expressivenessof descriptions. First, we introduce a predictive concept recognition methodology basedon a new closed terminology assumption (CTA). The terminology isdynamically partitioned by modalities (necessary, optional, andimpossible) with respect to individuals as they are specified. In ourinteractive configuration application, a user incrementally specifiesan individual computer system and its components in collaboration witha configuration engine. Choices can be made in any order and at anylevel of abstraction. We distinguish between abstract and concreteconcepts to formally define when an individual's description may beconsidered finished. We also exploit CTA, together with theterminology's subsumption-based organization, to efficiently track thetypes of systems and components consistent with current choices, inferadditional constraints on current choices, and appropriately restrictfuture choices. Thus, we can help focus the efforts of both user andconfiguration engine. This work is implemented in the K-REP system. Second, we show that a new class of complex descriptions can be formedvia constraint networks over standard descriptions. For example, wemodel plans as constraint networks whose nodes represent actions.Arcs represent qualitative and metric temporal constraints, plusco-reference constraints, between actions. By combiningterminological reasoning with constraint satisfaction techniques,subsumption is extended to constraint networks, allowing automaticclassification of a plan library. This work is implemented in theT-REX system, which integrates and builds upon an existing descriptionlogic system (K-REP or CLASSIC) and temporal reasoner (MATS). Finally, we combine the preceding, orthogonal results to conductpredictive recognition of constraint network concepts. As an example,this synthesis enables a new approach to deductive plan recognition,illustrated with travel plans. This work is also realized in T-REX. (ps) An intractability result for multiple integration Henryk Wozniakowski, I. H. Sloan 1996-05-20 (ps) Explicit cost bounds of algorithms for multivariate tensor product problems Henryk Wozniakowski, Greg Wasilkowski 1996-05-20 (ps) Strong tractability of weighted tensor products Henryk Wozniakowski 1996-05-20 (ps) On tractability of path integration Henryk Wozniakowski, Greg Wasilkowski 1996-05-20 (ps) Estimating a largest eigenvector by polynomial algorithms with a random start Henryk Wozniakowski, Z. Leyk 1996-05-20 (ps) Overview of information-based complexity Henryk Wozniakowski 1996-05-20 (ps) Computational complexity of continuous problems Henryk Wozniakowski 1996-05-20 (ps) The exponent of discrepancy is at most 1.4778... Henryk Wozniakowski, Greg Wasilkowski 1996-05-20 (ps) A Metalinguistic Approach to Process Enactment Extensibility Gail E. Kaiser, Israel Z. Ben-Shaul, Steven S. Popovich, Stephen E. Dossick 1996-05-14 We present a model for developing rule-basedprocess servers with extensible syntax and semantics. New processenactment directives can be added to the syntax of the processmodeling language, in which the process designer may specifyspecialized behavior for particular tasks or task segments. Theprocess engine is peppered with callbacks to instance-specific code inorder to implement any new directives and to modify the defaultenactment behavior and the kind of assistance that theprocess-centered environment provides to process participants. Werealized our model in the Amber process server, and describe how weexploited Amber's extensibility to replace Oz's native process enginewith Amber and to integrate the result with a mockup of TeamWare. (ps) Architectures for Federation of Process-Centered Environments Israel Z. Ben-Shaul, Gail E. Kaiser 1996-05-07 We describe two models for federatingprocess-centered environments, homogeneous federation where theinteroperability is among distinct process models enacted by differentcopies of the same system and heterogeneous federation withinteroperability among distinct process enactment systems. Weidentify the requirements and possible architectures for eachmodel. The bulk of the paper presents the specific architecture andinfrastructure for homogeneous federation we realized in the \\ozsystem. We briefly consider how \\oz might be integrated into aheterogeneous federation to serve as one of its interoperating PCEs. (ps) On Reality and Models Joseph F. Traub 1996-05-05 (ps) From Infoware to Infowar Joseph F. Traub 1996-04-16 (ps) OzCare: A Workflow Automation System for Care Plans Wenke Lee, Gail E. Kaiser, Paul D. Clayton, Eric H Sherman 1996-03-29 (ps) Incremental Process Support for Code Reengineering: An Update (Experience Report) Gail E. Kaiser, George T. Heineman, Peter D. Skopp, Jack J. Yang 1996-02-15 {\\em Componentization} is an important, emergingapproach to software modernization whereby a stovepipe system isrestructured into components that can be reused in other systems.More significantly from the system maintenance perspective, selectedcomponents in the original system can be completely replaced, e.g.,the database or user interface. In some cases, a new architecture canbe developed, for example to convert a monolithic system to theclient/server paradigm, and the old components plugged into placealong with some new ones. We update a 1994 publication in thisconference series, where we proposed using process modeling andenactment technology to support both construction of systems fromcomponents and re-engineering of systems to permit componentreplacement. This paper describes our experience following thatapproach through two generations of component-oriented process models. (ps) An Architecture for Integrating OODBs with WWW Jack Jingshuang Yang, Gail E. Kaiser 1996-02-13 (ps) Concurrency-Oriented Optimization for Low-Power Asynchronous Systems Luis A. Plana, Steven M. Nowick 1996-01-20 We introduce new architectural optimizations forasynchronous systems. These optimizations allow application ofvoltage scaling, to reduce power consumption while maintaining systemthroughput. In particular, three new asynchronous sequencer designsare introduced, which increase the concurrent activity of the system.We show that existing datapaths will not work correctly at theincreased level of concurrency. To insure correct operation, modifiedlatch and multiplexer designs are presented, for both dual-rail andsingle-rail implementations. The increased concurrency allows theopportunity for substantial system-wide power savings throughapplication of voltage scaling. (ps) A 3-level Atomicity Model for Decentralized Workflow Management Systems Israel Z. Ben-Shaul, George T. Heineman 1996-01-16 Decentralized workflow management systems (WFMSs) provide an architecture for multiple, heterogeneous WFMSs to interoperate. Atomicity is a standard correctness model for guaranteeing that a set of operations occurs as an atomic unit, or none of them occur at all. Within a single WFMS, atomicity is the concern of its transaction manager. In a decentralized environment, however, the autonomous transaction managers must find ways to cooperate if an atomic unit is split between multiple WFMSs. This paper describes a flexible atomicity model that enables workflow administrators to specify the scope of multi-site atomicity based upon the desired semantics of multi-site tasks in the decentralized WFMS. (ps) Automatic Generation of RBF Networks Shayan Mukherjee, Shree K. Nayar 1995-01-01 Learning can be viewed as mapping from an input space to an output\nspace. Examples of these mappings are used to construct a continuous\nfunction that approximates the given data and generalizes for intermediate\ninstances. Radial basis function (RBF) networks are used to formulate this\napproximating function. A novel method is introduced that\nautomatically constructs a RBF network for a given mapping and error\nbound. This network is shown to be the smallest network within the error bound\nfor the given mapping. The integral wavelet transform is used to determine the \nparameters of the network. Simple one-dimensional examples are used to \ndemonstrate how the network constructed using the transform is superior to one\nconstructed using standard ad hoc optimization techniques. The paper concludes\nwith the automatic generation of a network for a multidimensional problem,\nnamely, object recognition and pose estimation. The results of this \napplication are favorable. (ps) Integrating Groupware Activities into Workflow Management Systems Israel Z. Ben-Shaul, Gail E. Kaiser 1995-01-01 Computer supported cooperative work (CSCW) has been recognized as a crucial \nenabling technology for multi-user computer-based systems, particularly in\ncases where synchronous human-human interaction is required between\ngeographically dispersed users. Workflow is an emerging\ntechnology that supports complex business processes in modern corporations\nby allowing to explicitly define the process, and by supporting its\nexecution in a workflow management system (WFMS). \nSince workflow inherently involves humans carrying out parts of\nthe process, it is only natural to explore how to synergize these two\ntechnologies. We analyze the relationships between groupware and workflow \nmanagement, present our general approach to integrating synchronous\ngroupware tools into a WFMS, and conclude with an example process that\nwas implemented in the WFMS and integrated such tools.  Our main\ncontribution lies in the integration and synchronization of individual\ngroupware activities into modeled workflow processes, as opposed to being\na built-in part of the workflow WFMS. (ps) Coerced Markov Models for Cross-lingual Lexical-Tag Relations Pascale Fung,  Dekai Wu 1995-01-01 We introduce the {\\it Coerced Markov Model\\/} (CMM) to model the\nrelationship between the lexical sequence of a source language and the\ntag sequence of a target language, with the objective of constraining\nsearch in statistical transfer-based machine translation systems.\nCMMs differ from Hidden Markov Models in that state sequence assignments\ncan take on values coerced from external sources.\nGiven a Chinese sentence, a CMM can be used to predict the\ncorresponding English tag sequence, thus constraining the English\nlexical sequence produced by a translation model.\nThe CMM can also be used to score competing translation hypotheses in\nN-best models. Three fundamental problems for CMM designed are discussed. Their\nsolutions lead to the training and testing stages of CMM. (ps) Generalization of Band-Joins and the Merge/Purge Problem Mauricio A. Hernandez 1995-01-01 Many commercial organizations routinely gather large numbers of \ndatabases for various marketing and business analysis functions. \nThe task is to correlate information from different databases by\nidentifying distinct individuals that appear in a number of\ndifferent databases typically in an inconsistent and often\nincorrect fashion.  The problem we study here is the task of\nmerging data from multiple sources in as efficient manner as\npossible, while maximizing the accuracy of the result.  We call\nthis the m merge/purge problem.  The key to successful\nmerging requires  a means of identifying \"equivalent\"  data from\ndiverse sources.  The determination that two pieces of information\nare equivalent, and that they represent some aspect of the same\ndomain entity, depends on sophisticated inference techniques and\nknowledge of the domain.  We introduce the use of a rule program\nthat declaratively specifies an \"equational theory\" for this purpose.\nFurthermore, the number and size of the data sets involved may be\nso large that parallel and distributed computing systems may be\nthe only hope for achieving accurate results in a reasonable amount\nof time with acceptable cost.   In this paper we detail the\n\"sorted neighborhood\" method that is used by some commercial\norganizations to solve merge/purge and present experimental results\nthat demonstrates this approach works well in practice but at\ngreat expense.  An alternative method based upon clustering is\nalso presented with a comparative evaluation to the sorted\nneighborhood method.  We show a means of improving the accuracy\nof the results based upon a \"multi-pass\" approach that succeeds by\ncomputing the Transitive Closure over the results of independent\nruns considering alternative primary key attributes in each pass.\nThe moral is that a multiplicity of \"cheap\" passes produces higher\naccuracy than one \"expensive\" pass over the data. (ps) The gigabit per second Isochronet switch Danilo Florissi, Yechiam Yemini 1995-01-01 ABSTRACT = { \nThis paper  overviews an  electronic design  and implementation  of  a\nscaleable  gigabit  per second  multi-protocol  switch  based  on  the\nIsochronets  high-speed  network  architecture.  Isochronets   do  not\nrequire frame  header processing  in  the switches  and, consequently,\nthey  are  scaleable   with  link   speeds,  support  multiple   frame\nstructures, and  are  suitable for  all-optical  implementations.  The\nelectronic   switch  has  low  cost   and  uses  simple  off-the-shelf\ncomponents. The switch interface is simple and provides novel services\nsuch  as  guaranteed  Quality  of Service  (QoS)  and  propagation  of\nsynchronization   signals   to   upper  protocol   layers,   including\napplications. The switch allows the negotiation of QoS while promoting\nflexible resource sharing.  This  paper also proposes  an  all-optical\nswitch design that can be realized  with current  optical devices. The\nmodular  switch  designs are  scaleable  in  number of nodes  and link\nspeed. Using faster  implementation technology, the electronic  design\ncan reach scores of gigabits per second, while the all-optical  design\ncan potentially operate at terabits per second. (ps) An Overview of the Isochronets architecture for high speed networks Yechiam Yemini, Danilo Florissi 1995-01-01 This paper overviews the novel Route Switching (RS) technique.  In RS,\nnetwork  nodes switch different routing trees over time. Traffic moves\nalong routing trees to the roots  during the tree activation period or\nband. Contrary to circuits, routes do not change frequently,  allowing\nRS  to   pre-established  routes  for  prolonged  network  operations.\nContrary to  packets,  routes can be switched over  time without frame\nheader processing, allowing RS to scale to any network speed and to be\nimplemented using all-optical technology. RS  provide flexible quality\nof service control and multicasting  through  allocation  of  bands to\ntrees. They can be tuned to span a spectrum of performance  behaviors,\noutperforming both Circuit or  Packet  Switching, as evidenced by  the\nperformance  study  in  this  paper.  RS  is the basis  for the  novel\nIsochronets  high-speed network architecture. Isochronets  operate  at\nthe  media-access  layer  and can support  multiple framing  protocols\nsimultaneously. Internetworking  is  reduced  to a  simple media-layer\nbridging.  A  gigabit-per-second  Isochronet switch prototype has been\nimplemented to demonstrate the feasibility of RS. (ps) Integrating, Customizing and Extending Environments with a Message-Based Architecture John E. Arnold, Steven S. Popovich 1995-01-01 Message-based architectures have typically been used for integrating an\nengineer's set of tools as in FIELD and SoftBench. This paper presents our\nexperience using a message-based architecture to integrate complex,\nmulti-user environments. Where this style of control integration has been\neffective for encapsulating independent tools within an environment, we\nshow that these techniques are also useful for integrating environments\nthemselves.\n \nOur experience comes from our integration of two types of process-centered\nsoftware development environments: a groupware application that implements\na Fagan-style code inspection process and a software development process\nenvironment where code inspection is a single step in the overall process.\nWe use a message-based mechanism to federate the two process engines such\nthat the two process formalisms complement rather than compete with each\nother. Moreover, we see that the two process engines can provide some\nsynergy when used in a single, integrated software process environment,\n\nSpecifically, the integrated environment uses the process modeling and\nenactment services of one process engine to customize and extend the code\ninspection process implemented in a different process engine. The\ncustomization and extension of the original collaborative application was\naccomplished without modifying the application. This was possible because\nthe integration mechanism was designed for multi-user, distributed\nevironments and encouraged the use of an environment's services by other\nenvironments. The results of our study indicate that the message-based\narchitecture originally conceived for tool-oriented control integration is\nequally well-suited for environment integration. (ps) What is the complexity of solution-restricted operator equations? Arthur G. Werschulz 1995-01-01 We study the worst case complexity of operator equations $Lu=f$,\nwhere $L\\colon G\\to X$ is a bounded linear injection, $G$ is a Hilbert\nspace, and $X$ is a normed linear space.  Past work on the complexity\nof such problems has generally assumed that the class~$F$ of problem\nelements~$f$ to be the unit ball of~$X$.  However, there are many\nproblems for which this choice of~$F$ yields unsatisfactory results.\nMixed elliptic-hyperbolic problems are one example, the difficulty\nbeing that our technical tools are not strong enoguh to give good\ncomplexity bounds.  Ill-posed problems are another example, because we\nknow that the complexity of computing finite-error approximations is\ninfinite if $F$ is a ball in~$X$.  In this paper, we pursue another\nidea.  Rather than directly restrict the class~$F$ of problem\nelements~$f$, we will consider problems that are solution-restricted,\ni.e., we restrict the class$~U$ of solution elements~$u$.  In\nparticular, we assume that~$U$ is the unit ball of a Hilbert space~$W$\ncontinuously embedded in~$G$.\n\nThe main idea is that our problem can be reduced to the standard {\\it\napproximation problem\\/} of approximating the embedding of $W$\ninto~$G$.  This allows us to characterize optimal information and\nalgorithms for our problem.  Then, we consider specific applications.\nThe first application we consider is any problem for which $G$ and~$W$\nare standard Sobolev Hilbert spaces; we call this the ``standard\nproblem'' since it includes many problems of practical interest.  We\nshow that finite element information and generalized Galerkin methods\nare nearly optimal for standard problems.  We then look at elliptic\nboundary-value problems, Fredholm integral equations of the second\nkind, the Tricomi problem (a mixed hyperbolic-elliptic problem arising\nin the study of transonic flow), the inverse finite Laplace transform,\nand the backwards heat equation.  (Note that with the exception of the\nbackwards heat equation, all of these are standard problems.\nMoreover, the inverse finite Laplace transform and the backwards heat\nequation are ill-posed problems.)  We determine the problem complexity\nand derive nearly optimal algorithms for all these problems. (ps) Adapting Materialized Views after Redefinitions Ashish Gupta,  Inderpal S. Mumick, Kenneth A. Ross 1995-01-01 We consider a variant of the view maintenance problem: How does one\nkeep a materialized view up-to-date when the view definition itself\nchanges?  Can one do better than recomputing the view from the base\nrelations?  Traditional view maintenance tries to maintain the\nmaterialized view in response to modifications to the base relations;\nwe try to ``adapt'' the view in response to changes in the view\ndefinition.\n\nSuch techniques are needed for applications where the user can change\nqueries dynamically and see the changes in the results fast.  Data\narchaeology, data visualization, and dynamic queries are examples of\nsuch applications.\n\nWe consider all possible redefinitions of SQL\nSelect-From-Where-Groupby-Having, Union, and Except views, and show\nhow these views can be adapted using the old materialization for the\ncases where it is possible to do so.  We identify extra information\nthat can be kept with a materialization to facilitate redefinition.\nMultiple simultaneous changes to a view can be handled without\nnecessarily materializing intermediate results.  We identify\nguidelines for users and database administrators that can be used to\nfacilitate efficient view adaptation. (ps) A Tabular Approach to Transition Signaling Stephen H. Unger 1995-01-01 Transition signaling is a popular technique used in the design of unclocked digital systems.  A number of basic modules have been developed by various researchers for use in such systems, and they have presented ingenious circuits for implementing them.  In this paper, a method is presented for describing the behavior of such modules clearly and concisely by means of transition flow tables.  These tables can be used to generate Huffman type primitive flow tables.  All of the formal (and informal) techniques developed for the design of asynchronous sequential logic circuits can then be applied to develop implementations with attention to problems of hazards, metastability, and the like.  These transition tables may also be useful for describing larger subsystems. (ps) Interfacing Oz with PCTE OMS Wenke Lee, Gail Kaiser 1995-01-01 This paper details our experiment interfacing Oz with the Object Management \nSystem (OMS) of PCTE. Oz is a process-centered multi-user software development\nenvironment. PCTE is a specification which defines a language independent \ninterface providing support mechanisms for software engineering environments \n(SEE) populated with CASE tools. Oz is, in theory, a SEE that can be built \n(or extended) using the services provided by PCTE. Oz historically has had a\nnative OMS component whereas the PCTE OMS is an open data repository with an \nAPI for external software tools. Our experiment focused on changing Oz to use \nthe PCTE OMS. This paper describes how several Oz components were changed\nin order to make the Oz server interface with the PCTE OMS. The resulting \nsystem of our experiment is an environment that has process control and \nintegration services provided by Oz, data integration services provided by \nPCTE, and tool integration services provided by both. We discusses in depth \nthe concurrency control problems that arise in such an environment and \ntheir solutions. The PCTE implementation used in our experiment is the \nEmeraude PCTE V 12.5.1 supplied by Transtar Software Incorporation. (ps) A Theory of Pattern Rejection Simon Baker, Shree K. Nayar 1995-01-01 The efficiency of pattern recognition is critical when there are a large \nnumber of classes to be discriminated, or when the recognition algorithm\nmust be applied a large number of times. We propose and analyze a general\ntechnique, namely pattern rejection, that leads to great efficiency\nimprovements in both cases. Rejectors are introduced as algorithms that\nvery quickly eliminate from further consideration,\nmost of the classes or inputs (depending on the setting).\nImportantly, a number of rejectors may be combined\nto form a composite rejector, which performs far more effectively than \nany of its\ncomponent rejectors. Composite rejectors are analyzed, and conditions\nderived which guarantee both efficiency and practicality.\nA general technique is proposed for  the construction of\nrejectors, based on a single assumption about the pattern\nclasses. The generality is shown\nthrough a close relationship with the Karhunen-Lo\\'{e}ve expansion.\nFurther, a comparison with Fisher's discriminant analysis is\nincluded to illustrate the benefits of pattern rejection.\nComposite rejectors were constructed for two\napplications, namely, object recognition\nand local feature detection. In both cases, a substantial\nimprovement in efficiency over existing techniques is demonstrated. (ps) A Paradigm for Decentralized Process Modeling and its Realization in the Oz Environment Israel Z. Ben-Shaul 1995-01-01 This dissertation investigates decentralization of software processes\nand Process Centered Environments (PCEs), and addresses a wide range of\nissues concerned with supporting interoperability and collaboration\namong autonomous and heterogeneous processes, both in their definition\nand in their execution in possibly physically dispersed PCEs.\n\nDecentralization is addressed at three distinct levels of abstraction.\nThe first proposes a generic conceptual model that is both language- and\nPCE-independent. The second level explores the realization of the model\nin a specific PCE, Oz, and its rule-based process modeling\nlanguage. The third level addresses architectural issues in\ninterconnecting autonomous PCEs as a basis for process interoperability.\n\nTwo key concerns guide this research. The first is maximizing local\nautonomy, so as not to force a priori any global constraints on the\ndefinition and execution of local processes, unless explicitly and\nvoluntarily specified by a particular process instance. The second\nconcern is tailorability, dynamicity and fine-grained control over the\ndegree of interoperability.\n\nThe essence of the interoperability model lies in two abstraction\nmechanisms --- Treaty and Summit --- for inter-process\ndefinition and execution, respectively. Treaties enable to specify\nshared sub-processes while retaining the privacy of local\nsub-processes. To promote autonomy, Treaties are established by\nexplicit and active participation of the involved processes. To\npromote fine granularity, Treaties are defined pairwise\nbetween two collaborating processes and formed over a possibly small\nsub-process unit, although multi-site Treaties over large shared\nsub-processes can be constructed, if desired. Finally, Treaties\nare superimposed on top of pre-existing instantiated processes,\nenabling their dynamic and incremental establishment and\nsupporting a decentralized bottom-up approach.\n\nSummits are the execution counterparts of Treaties. They support\n``global'' execution of shared sub-processes involving artifacts\nand/or users from multiple sites, as well as local execution of\nprivate sub-processes. Summits successively alternate between shared\nand private execution modes, where the former is used for synchronous\nexecution of shared activities, and the latter for autonomous\nexecution of any private subtasks emanating from the shared activities\nas defined in the local processes. (ps) Pattern Matching for Translating Domain-Specific Terms from Large Corpora Pascale Fung 1995-01-01 Translating domain-specific terms is one significant component of machine\ntranslation and machine-aided translation systems. These terms are often\nnot found in standard dictionaries. Human translators, not being experts in\nevery technical or regional domain, cannot produce their translations\neffectively. Automatic translation of domain-specific terms is therefore\nhighly desirable.\n \nMost other work on automatic term translation uses statistical information\nof words from parallel corpora. Parallel corpora of clean, translated texts\nare hard to come by whereas there are more noisy, translated texts and many\nmore monolingual texts in various domains. We propose using noisy parallel\ntexts and same-domain texts of a pair of languages to translate terms.\n \nIn our work, we propose using a novel paradigm of pattern matching of\nstatistical signals of word features. These features are robust to the\nsyntactic structure, character sets, language of the text, and to \nthe domain.  We obtain statistical information which is related to the\nlexical properties of a word and its translation in any other language of\nthe same domain. These lexical properties are extracted from the corpora\nand represented in vector form.  We propose using signal processing\ntechniques for matching these features vectors of a word to those of its\ntranslation. Another matching technique we propose is applying\ndiscriminative analysis of the word features.  For each word, the various\nfeatures are combined into a single vector which is then transformed into a\nsmaller dimension eigenvector for matching.\n\nSince most domain specific terms are nouns and noun phrases, we concentrate\non translating English nouns and noun phrases into other languages.  We\nstudy the relationship between English noun phrases and their translations\nin Chinese, Japanese and French in parallel corpora. The result of this\nstudy is used in our system for translation of English noun phrases into\nthese other languages from noisy parallel and non-parallel corpora. (ps) The complexity of the Poisson problem for spaces of bounded mixed derivatives Art Werschulz 1995-01-01 We are interested in the complexity of the Poisson problem with\nhomogeneous Dirichlet boundary conditions on the $d$-dimensional unit\ncube~$\\Omega$.  Error is measured in the energy norm, and only\nstandard information (consisting of function evaluations) is\navailable.  In previous work on this problem, the standard assumption\nhas been that the class~$F$ of problem elements has been the unit ball\nof a Sobolev space of fixed smoothness~$r$, in which case the\n$\\e$-complexity is proportional to $\\e^{-d/r}$.  Given this\nexponential dependence on~$d$, the problem is intractable for such\nclasses~$F$.  In this paper, we seek to overcome this intractability\nby allowing $F$ to be the unit ball of a space $\\circhrho$ of bounded\nmixed derivatives, with $\\rho$ a fixed multi-index with positive\nentries.  We find that the complexity is proportional to\n$c(d)(1/\\e)^{1/\\rhomin}[\\ln(1/\\e)]^b$, and we give bounds\non~$b=b_{\\rho,d}$.  Hence, the problem is tractable in $1/\\e$, with\nexponent at most~$1/\\rhomin$.  The upper bound on the complexity\n(which is close to the lower bound) is attained by a modified finite\nelement method (MFEM) using discrete blending spline spaces; we obtain\nan explicit bound (with no hidden constants) on the cost of using this\nMFEM to compute $\\e$-approximations.  Finally, we show that for any\npositive multi-index ~$\\rho$, the Poisson problem is strongly\ntractable, and that the MFEM using discrete blended piecewise\npolynomial splines of degree~$\\rho$ is a strongly polynomial time\nalgorithm.  In particular, for the case $\\rho={\\bf 1}$, the MFEM using\ndiscrete blended piecewise linear splines produces an\n$\\e$-approximation with cost at most\n$$0.839262\\,(c(d)+2)\\left({1\\over\\e}\\right)^{5.07911}.$$ (ps) Tractable Reasoning in Knowledge Representation Systems Dalal Mukesh 1995-01-01 This document addresses some problems raised by the well-known\nintractability of deductive reasoning in even moderately expressive\nknowledge representation systems.\n\nStarting from boolean constraint propagation (BCP), a previously known\nlinear-time incomplete reasoner for clausal propositional theories, we\ndevelop {\\em fact propagation} (FP) to deal with non-clausal theories, after\nmotivating the need for such an extension.  FP is specified using a\nconfluent rewriting systems, for which we present an algorithm that\nhas quadratic-time complexity in general, but is still linear-time for\nclausal theories.  FP is the only known tractable extension of BCP to\nnon-clausal theories; we prove that it performs strictly more\ninferences than CNF-BCP, a previously-proposed extension of BCP to\nnon-clausal theories.\n\nWe generalize a refutation reasoner based on FP to a family of sound\nand tractable reasoners that are ``increasingly complete'' for\npropositional theories.  These can be used for anytime reasoning,\ni.e., they provide partial answers even if they are stopped\nprematurely, and the ``completeness'' of the answer improves with the\ntime used in computing it. A fixpoint construction based on FP\ngives an alternate characterization of the reasoners in this family,\nand is used to define a transformation of arbitrary theories into\nlogically-equivalent ``vivid'' theories --- ones for which our FP\nalgorithm is complete.\n\nOur final contribution is to the description of tractable classes of\nreasoning problems. Based on FP, we develop a new property, called\nbounded intricacy, which is shared by a variety of tractable classes\nthat were previously presented, for example, in the areas of\npropositional satisfiability, constraint satisfaction, and\nOR-databases. Although proving bounded intricacy for these classes\nrequires domain-specific techniques (which are based on the original\ntractability proofs), bounded intricacy is one more tool available for\nshowing that a family of problems arising in some application is\ntractable. As we demonstrate in the case of constraint satisfaction\nand disjunctive logic programs, bounded intricacy can also be used to\nuncover new tractable classes. (ps) Isochronets: a High-speed Network Switching Architecture (Ph.D. Thesis Danilo Florissi 1995-01-01 Traditional  network  architectures present two main  limitations when\napplied to  High-Speed Networks (HSNs):   they do not scale with  link\nspeeds and they do not adequately support the Quality of Service (QoS)\nneeds  of high-performance applications.   This thesis  introduces the\nIsochronets architecture that overcomes both limitations.\n\nIsochronets  view frame  motions over links  in  analogy to motions on\nroads.  In the latter, traffic lights  can synchronize to create green\nwaves   of uninterrupted    motion.   Isochronets   accomplish similar\nuninterrupted motion by  periodically configuring network switches  to\ncreate  end-to-end routes  in the  network.   Frames flow along  these\nroutes with no required header processing at intermediate switches.\n\nIsochronets offer several  advantages. First, they  are scaleable with\nrespect to transmission speeds. Switches merely  configure routes on a\ntime scale  that is significantly longer than  and independent  of the\naverage frame   transmission time.  Isochronets do  not  require frame\nprocessing  and  thus avoid conversions   from  optical to  electronic\nrepresentations.  They  admit  efficient  optical transmissions  under\nelectronically controlled switches.\n\nSecond, Isochronets ensure  QoS  for high-performance applications  in\nterms  of     latency,   jitter,     loss,    and  other       service\nqualities. Isochronet switches can give   priority to frames  arriving\nfrom selected links.  At one  extreme, they  may   give a source   the\nright-of-way to the destination by assigning  priority to all links in\nits path.  Additionally,  other sources  may still transmit   at lower\npriority. At the  other extreme, they  may give no priority to sources\nand  frames en route to  the same destination contend for intermediate\nlinks. In  between,  Isochronets can accomplish  a myriad  of priority\nallocations with diverse QoS.\n\nThird, Isochronets  can support  multiple protocols without adaptation\nbetween different frame  structures. End nodes view  the network  as a\nmedia access layer that accepts frames of arbitrary structure.\n\nThe main  contributions   of  this thesis  are:  (1)   design   of the\nIsochronets architecture;  (2) design and  implementation of a gigabit\nper  second Isochronet switch    (Isoswitch);  (3) definition of   the\nLoosely-synchronous Transfer Mode  (LTM) and the  Synchronous Protocol\nStack  (SPS)  that  add synchronous and  isochronous  services  to any\nexisting protocol    stack;  and   (4)    performance evaluation    of\nIsochronets. (ps) Enveloping Sophisticated Tools into Process-Centered Environments Giuseppe Valetto, Gail E. Kaiser 1995-01-01 We present a tool integration strategy based on enveloping\npre-existing tools without source code modifications or recompilation,\nand without assuming an extension language, application programming\ninterface, or any other special capabilities on the part of the tool.\nThis Black Box enveloping (or wrapping) idea has existed for a long\ntime, but was previously restricted to relatively simple tools.  We\ndescribe the design and implementation of, and experimentation with, a\nnew Black Box enveloping facility intended for sophisticated tools\n--- with particular concern for the emerging class of groupware\napplications. (ps) The CORD approach to extensible concurrency control George Heineman,  Gail E. Kaiser 1995-01-01 Database management systems (DBMSs) have been increasingly used for\n  advanced application domains, such as software development environments,\n  network management, workflow management systems, computer-aided design\n  and manufacturing, and managed healthcare.  In these domains, the\n  standard correctness model of serializability is often too restrictive.\n  We introduce the notion of a Concurrency Control Language (CCL) that\n  allows a database application designer to specify concurrency control\n  policies to tailor the behavior of a transaction manager.  A well-crafted\n  set of policies defines an extended transaction model.  The necessary\n  semantic information required by the CCL run-time engine is extracted\n  from a task manager, a (logical) module by definition included in\n  all advanced applications.  This module stores task models that encode\n  the semantic information about the transactions submitted to the DBMS.\n  We have designed a rule-based CCL, called CORD, and have implemented a\n  run-time engine that can be hooked to a conventional transaction manager\n  to implement the sophisticated concurrency control required by advanced\n  database applications.  We present an architecture for systems based on\n  CORD and describe how we integrated the CORD engine with the Exodus\n  Storage Manager to implement Altruistic Locking. (ps) Parametric feature detection Shree K. Nayar, Simon Baker, Hiroshi Murase 1995-01-01 A large number of visual features are parametric in nature, including,\nedges, lines, corners, and junctions.\nWe present a general framework for the design and\nimplementation of detectors for parametrized features.\nFor robustness, we argue in favor of elaborate modeling of features\nas they appear in the physical world. In addition,\noptical and sensing artifacts are incorporated to achieve\nrealistic feature models in  image domain.\nEach feature is represented as a densely sampled\nparameterized manifold in a low-dimensional subspace.\nDuring detection, the brightness distribution around each\nimage pixel is projected to the subspace. If the projection\nlies close to the feature manifold, the exact location of\nthe closest manifold point reveals the parameters of the\nfeature. The concepts of parameter reduction \nby normalization, dimension reduction, pattern rejection, and efficient\nsearch are employed to achieve high efficiency.\n\nDetectors have been\nimplemented for five specific features, namely, step edge (5 parameters),\nroof edge (5 parameters), line (6 parameters), corner (5 parameters),\nand circular disc (6 parameters). All five of these detectors\nwere generated using the same technique by simply inputing \ndifferent feature models. Detailed experiments are reported on the\nrobustness of detection and the accuracy of parameter estimation.\nIn the case of the step edge, our results are compared with those\nobtained using popular detectors. We conclude with a brief discussion\non the use of relaxation to refine outputs from multiple feature\ndetectors, and sketch a hardware architecture for a general\nfeature detection machine. (ps) Playback and Jitter Control for Real-Time Video-conferencing Sanjay K. Jha 1995-01-01 The purpose of this report is to examine the problems associated with \ndisplay (playback) of live continuous media under varying conditions for an \ninternetwork of workstations running general purpose operating system such as \nUnix. Under the assumption that the network cannot guarantee the required \nbounds on delay and jitter, there is a need to accommodate the delay jitter in \nthe end systems. Various methods of jitter smoothing at the end systems and \ntheir suitability to audio as well as video transmission have been discussed. \nAn experimental test bed which was  used to investigate these problems is \ndescribed. Some initial empirical results obtained using this test bed\nare also presented. (ps) An Interoperability Model for Process-Centered Software Engineering Environments and its Implementation in Oz Israel Z. Ben-Shaul, Gail E. Kaiser 1995-01-01 A process-centered software engineering environment (PSEE) enables to\nmodel, evolve, and enact the process of software development\nand maintenance. This paper addresses the problem of\nprocess-interoperability among decentralized and autonomous PSEEs by\npresenting the generic International Alliance model, which consists of\ntwo elements, namely Treaty and Summit. The Treaty abstraction allows\npairwise peer-peer definition of multi-site shared sub-processes that\nare integrated inside each of the participating sites, \nwhile retaining the definition- and evolution-autonomy of non-shared\nlocal sub-processes. Summits are the execution abstraction for\nTreaty-defined sub-processes. They enact Treaty sub-processes in\nmultiple sites by successively alternating between shared and private\nexecution modes: the former is used for the synchronous execution of the\nshared activities, and the latter is used for the autonomous execution\nof any private subtasks emanating from the shared activities. We\ndescribe the realization of the models in the Oz multi-site PSEE and\nevaluate the models and system based on experience gained from using Oz\nfor production purposes. We also consider the application of the model\nto Petri net-based and grammar-based PSEEs. (ps) QoSME: QoS Management Environment (Ph.D. Thesis Patricia Gomes Soares Florissi 1995-01-01 Distributed multimedia applications are sensitive  to the Quality of Service\n(QoS) delivered by underlying communication networks.   For example, a video\nconference exchange  can  be   very  sensitive  to  the  effective   network\nthroughput. Network jitter can  greatly  disrupt a  speech stream. The  main\nquestion this  thesis addresses is  how to adapt  multimedia applications to\nthe QoS delivered by the network and vice versa.\n\nSuch adaptation is especially important because  current networks are unable\nto assure  the   QoS required by    applications and the  latter is  usually\nunprepared for  periods of QoS   degradation.  This work introduces the  QoS\nManagement Environment (QoSME) that provides mechanisms for such adaptation.\n\nThe main contributions of this thesis are:\n\\begin{itemize}\n\\item Language level abstractions for QoS management. The Quality Assurance\nLanguage (QuAL) in QoSME enables the specification of how to allocate,\nmonitor, analyze, and adapt to delivered QoS. Applications can express in QuAL\n\ntheir QoS needs and how to handle potential violations.\n\n\\item  Automatic QoS monitoring.  QoSME automatically generates  the\ninstrumentation to monitor QoS when applications use QuAL constructs.  The\nQoSME runtime scrutinizes interactions among applications, transport\nprotocols, and Operating Systems (OS)  and  collects in  QoS Management\nInformation Bases (MIBs) statistics on the QoS delivered.\n\n\\item  Integration of QoS and standard network management. A Simple Network\nManagement Protocol (SNMP) agent embedded in QoSME provides  QoS MIB access to\n\nSNMP managers.  The latter can use this feature  to monitor end-to-end QoS\ndelivery and adapt network resource allocation and operations accordingly.\n\n\\end{itemize}\n\nA partial prototype of QoSME has been released for public access. It runs on\nSunOS 4.3  and  Solaris 2.3   and supports  communication  on ATM adaptation\nlayer, ST-II, UDP/IP, TCP/IP, and Unix internal protocols. (ps) Estimating an Eigenvector by the Power Method with a Random Start Gianna M. Del Corso 1995-01-01 This paper addresses the problem of approximating an eigenvector\nbelonging to the largest eigenvalue of a symmetric positive definite matrix\nby the power method. We assume that the starting\nvector is randomly chosen with uniform distribution over the unit sphere.\n\nThis paper provides lower and upper as well as asymptotic bounds on the\nrandomized error in the ${\\cal L}_p$ sense, $p\\in[1,+\\infty]$.\nWe prove that it is impossible to achieve bounds that are independent\nof the ratio between the two largest eigenvalues.\nThis should be contrasted to the problem of\napproximating the largest eigenvalue for\nwhich Kuczy\\'nski and Wo\\'zniakowski in 1992 proved that\nit is possible to bound the randomized error at the $k$-th step\nwith a quantity that depends only on $k$ and on the size of the matrix.\n\nWe prove that the rate of convergence depends on the ratio of the two\nlargest eigenvalues, on their multiplicities, and on the particular\nnorm. The rate of convergence is at most linear in the ratio of the\ntwo largest eigenvalues. (ps) Topological visual navigation in large environments Il-Pyung Park, John R. Kender 1994-01-01 In this paper, we investigate a new model for robot navigation in large\nunstructured environments. Our model consists of two parts, the map-maker\nand the navigator. Given a source and a goal, the map-maker derives a\nnavigational path based on the topological relationships between landmarks.\nA navigational path is generated as a combination of ``parkway'' and\n``trajectory'' paths, both of which are abstractions of the real world into\ntopological data structures. Traversing within a parkway enables the\nnavigator to follow visible landmarks.  Traversing on a trajectory enables\nthe navigator to move reliably towards a target, based on shapes formed by\nvisible landmarks. Error detection and error recovery routines are also\nencoded into the path segments.  The optimal path is further abstracted\ninto a ``custom map,'' which consists of a list of directional\ninstructions, the vocabulary of which is defined by our environmental\ndescription language.  Based on the custom map generated by the map-maker,\nthe navigating robot looks for events that are characterized by spatial\nproperties of the environment.  The map-maker and navigator are implemented\nusing two cameras, an IBM 7575 robot arm, and a PIPE (Pipelined Image\nProcessing Engine.) (ps) Error Detection and Recovery in Two Dimensional Topological Navigation Il-Pyung Park, John R. Kender 1994-01-01 In this paper we describe error detection and error recovery methods\napplicable to navigation in large scale unstructured environmental\nnavigation. We relax the assumption of error-free following of topological\nlandmarks; the navigator is ``permitted'' to make\nmistakes during its journey. The error detection method involves the\nnavigator observing its immediate surrounding and checking for one of\nseveral types of disparities.\nThe error recovery method is based on a simple fixed set of movements which\nis triggered by the navigator's local observation. Alternately described,\nthis work characterizes those environments in which robust topological\nnavigation is possible, including those landmarks which, literally, ``can't\nbe missed''. These methods have been implemented on our qualitative\nenvironmental navigation system consisting of a camera mounted IBM 7575\nrobot arm. (ps) Qualtative Environmental Navigation: Theory and Practice Il-Pyung Park 1994-01-01 In this thesis we propose and investigate a new model for robot navigation\nin large unstructured environments.  Current models which depend on metric\ninformation contain inherent mechanical and sensory uncertainties.\nInstead we supply the navigator with qualitative information.  Our\nmodel consists of two parts, the map-maker and the navigator.  Given\na source and a goal, the map-maker derives a navigational path based\non the topological relationships between landmarks. A navigational\npath is generated as a combination of ``parkway'' and ``trajectory''\npaths, both of which are abstractions of the real world into\ntopological data structures. Traversing within a parkway enables the\nnavigator to follow visible landmarks. Traversing on a trajectory\nenables the navigator to move reliably into a homogeneous space,\nbased on shapes formed by visible landmarks that are robust to\npositional and orientational errors. Reliability\nmeasures of parkway and trajectory traversals are defined by appropriate\nerror models that account for the sensory errors of the navigator, the\nmotor errors of the navigator, and the population of neighboring\nobjects. Error detection and error recovery methods are also encoded\ninto the generated path. The optimal path is further\nabstracted into a ``custom map,'' which consists of a list of verbal\ndirectional instructions, the vocabulary of which is defined by our\nenvironmental description language.  Based on the custom map generated by\nthe map-maker, the navigating robot looks for events that are characterized\nby spatial properties of the environment.  The map-maker and the\nnavigator are implemented using two cameras, an IBM 7575 robot arm\nand PIPE (Pipelined Image Processing Engine.) Various experiments\nshow the effectiveness of navigation ``in the large'' using the\nproposed methods. (ps) Expanding the Repertoire of Process-based Tool Integration Giuseppe Valetto 1994-01-01 The purpose of this thesis is to design and implement a new protocol for\ntool enveloping, in the context of the Oz Process Centered Environment.\nThis new part of the system would be complementary to the already existing\nBlack Box protocol for Oz and would deal with additional families of tools, \nwhose character would be better serviced by a different approach, providing \nenhanced flexibility and a greater amount of interaction between the human \noperator, the tools and the environment during the execution of\nthe wrapped activities. To achieve this, the concepts of persistent tool \nplatforms, tool sessions and transaction-like activities will be introduced\nas the main innovative features of the protocol. We plan to be able \nto encapsulate and service conveniently classes of tools such as\ninterpretive systems, databases, medium and large size applications that allow\nfor incremental binding of parameters and partial retrieving of results, \nand possibly multi-user tools. Marginal modification and upgrading \nof the Oz general architecture and components will necessarily be performed. (ps) A flexible rule-based process engine A. Z. Tong,  Pr. G Kaiser 1994-01-01 The @marvel process-centered environment (PCE) modeled knowledge of the\nsoftware development process in the form of rules, each representing a\nprocess step with its prerequisites and consequences.  @marvel enforced\nthat prerequisites were satisfied and used backward and forward chaining\nover the rule base to automate tool invocations.  In order to support\nmultiple users, the process engine ensured atomicity of those rule chains\nannotated as transactions.  It has recently become clear, however, that\nthese three modes of process assistance are not sufficient for all projects\nand all users.  Guidance to users regarding the process steps they can\nperform, monitoring of divergences from the prerequisites and consequences\nof a process, delegation of process steps to other users, collaboration\namong multiple users working together on a step (or a process fragment),\nplanning of a process in advance, simulation of processes for training or\nanalysis purposes, and instrumentation of processes for statistics\ngathering may also be desirable, and there are probably other modes that we\nhaven't considered. Yet neither @marvel nor any other existing PCE supports\nmore than a handful of built-in process assistance modes. We present the\ndesign of the new @amber system, which will generalize @marvel and other\nrule-based PCEs to support enforcement, automation, atomicity, guidance,\nmonitoring, delegation, collaboration, planning, simulation,\ninstrumentation and potentially other applications.  In particular, @amber\nwill be fully knowledge-based, tailored by knowledge regarding the\n@i(process assistance policies) to be supported as well as the @i(process\nmodel) (ps) Large Flow Trees: a Lower Bound Computation Tool for Network Optimization Bulent Yener 1994-01-01 This paper presents a new method for computing the lower bounds for multihop\nnetwork design problems which is particularly well suited to lightwave\nnetworks.  The lower bound can be computed in time polynomial in the network\nsize.  Consequently, the results in this work yields a tool which can be used\nin (i) evaluating the quality of heuristic design algorithms, and (ii)\ndetermining a termination criteria during minimization.\n\nMore specifically, given $N$ stations each with at most $d$ transceivers, and\npairwise average traffic values of the stations, the method provides a lower\nbound for the combined problem of finding optimum (i) allocation of\nwavelengths to the stations (configuration), and (ii) routing of the traffic\non this configuration while minimizing {\\em congestion} (i.e. minimizing the\nmaximum flow on any link).\n\nThe lower bound computation is based on first building {\\em flow\ntrees} to find a lower bound on the total flow, and then distributing\nthe total flow over the links to minimize the congestion. (ps) QuAL: Quality Assurance Language Patricia Gomes Soares Florissii 1994-01-01 Distributed  multimedia  applications  are  sensitive  to  the\nQuality   of   Services   (QoS)  provided   by   their  computing  and\ncommunication  environment. For  example, scheduling  of  processing\nactivities or network queueing delays  may cause excessive jitter in a\nspeech stream,  rendering  it difficult  to  understand.  It  is  thus\nimportant to  establish effective technologies  to ensure  delivery of\nQoS required by distributed multimedia applications.\n        This proposal  presents  a new language for the development of\ndistributed   multimedia  applications:   Quality  Assurance  Language\n(QuAL). QuAL abstractions allow the specification of QoS constraints\nexpected from the  underlying computing and communication environment.\nQuAL specifications are compiled into run time components that monitor\nthe actual QoS delivered. Upon QoS  violations, application provided\nexception  handlers  are  signaled to  act  upon  the  faulty  events.\nLanguage   level  abstractions  of  QoS  shelter  programs   from  the\nheterogeneity  of  underlying  infrastructures.  This  simplifies  the\ndevelopment and maintenance of  multimedia applications and promotes\ntheir portability  and reuse. QuAL generates  Management Information\nBases  (MIBs) that  contain QoS statistics per application.  Such MIBs\nmay  be  used to  integrate  application  level  QoS  management  into\nstandard network management frameworks. (ps) Process Support for Incremental Code Re-engineering George T. Heineman,  Gail E. Kaiser 1994-01-01 Reengineering a large code base can be a monumental task, and the situation\nbecomes even worse if the code is concomitantly being modified.  For the\npast two years, we have been using the Marvel process centered environment\n(PCE) for all of our software development and are currently using it to\ndevelop the Oz PCE (Marvel's successor).  Towards this effort, we are\nreengineering Oz's code base to isolate the process engine, transaction\nmanager, and object management system as separate components that can be\nmixed and matched in arbitrary systems.  In this paper, we show how a PCE\ncan guide and assist teams of users in carrying out code reengineering\nwhile allowing them to continue their normal code development.  The key\nfeatures to this approach are its incremental nature and the ability of the\nPCE to automate most of the tasks necessary to maintain the consistency of\nthe code base.\n\nKey words: Process Centered Environments, Componentization (ps) Better Semijoins Using Tuple Bit-Vectors Zhe Li, Kenneth A. Ross 1994-01-01 This paper presents the idea of ``tuple bit-vectors'' for distributed\nquery processing.  Using tuple bit-vectors, a new two-way semijoin\noperator called 2SJ++ that enhances the semijoin with an essentially\n``free'' backward reduction capability is proposed. We explore in\ndetail the benefits and costs of 2SJ++ compared with other semijoin\nvariants, and its effect on distributed query processing performance.\nWe then focus on one particular distributed query processing\nalgorithm, called the ``one-shot'' algorithm.  We modify the one-shot\nalgorithm by using 2SJ++ and demonstrate the improvements achieved in\nnetwork transmission cost compared with the original one-shot\ntechnique.  We use this improvement to demonstrate that equipped with\nthe 2SJ++ technique, one can improve the performance of distributed\nquery processing algorithms significantly without adding much\ncomplexity to the algorithms. (ps) Hamiltonian Decompositions of Regular Topology Networks for Convergence Routing Bulent Yener,  Terry Boult,  Yoram Ofek 1994-01-01 This paper introduces embeddings of multiple \ninto the hypercube and the circulant networks such that each virtual\nring is hamiltonian, and the rings are mutually edge-disjoint.  It is\nshown that multiple virtual rings improve (i) the bound on the length\nof routing, and (ii) the fault tolerance.\n The hamiltonian virtual rings are studied on the hypercube and the\ncirculant graphs.  On the circulant graphs, necessary and sufficient\nconditions for hamiltonian decomposition is established.  On the\nhypercube three algorithms are designed for an hypercube\nwith even dimension: (i) an O(N) time algorithm to find two\nedge-disjoint hamiltonian circuits, (ii) an $O(N \\log N)$ time\nalgorithm to find $\\frac{\\log N}{2}$ hamiltonian circuits with only\nepsilon leq $0.1$ common edges, and (iii) a recursive algorithm for\nthe hamiltonian decomposition of the hypercube with dimension power of\ntwo.\n The routing algorithm on multiple virtual rings is \nwhich combines the actual routing decision with the internal\nflow control state. It is shown analytically, and verified by\nsimulations on the circulants that with the d virtual ring\nembeddings, a bound of O(N/d) is established on the maximum length of\nrouting. (pdf) (ps) Integrating a Transaction manager component with Process WeaverBetter George T. Heineman, Gail E. Kaiser 1994-01-01 This paper details our experience using Process Weaver.  We have been using\nProcess Weaver for two distinct purposes.  The first concerns a set of\nexperiments we are performing to integrate a transaction manager component,\ncalled PERN with Process Weaver.  The second is a prototype system for\nautomatic translation of process modeling formalisms.  We have already\ndeveloped a compiler that translates a process model, created using the\nSEI-developed Statemate-approach, into a Marvel environment.  We are\ncurrently designing a way of integrating this compiler with Process Weaver\nto take advantage of the best features of these three powerful\napplications. (ps) A Comparative Study of Divergence Control Algorithms Akira Kawaguchi, Kui Mok, Calton Pu, Kun-Lung Wu, Philip S. Yu 1994-01-01 This paper evaluates and compares the performance of two-phase locking\ndivergence control (2PLDC) and optimistic divergence control (ODC)\nalgorithms using a comprehensive centralized database simulation\nmodel.  We examine a system with multiclass workloads in which on-line\nupdate transactions and long-duration queries progress based on\nepsilon serializability (ESR).  Our results demonstrate that\nsignificant performance enhancements can be achieved with a non-zero\ntolerable inconsistency ({\\mbox{\\Large$\\epsilon$}}-spec).  With sufficient\n{\\mbox{\\Large$\\epsilon$}}-spec and limited system resources, both algorithms \nachieve comparable performance.  However, with low resource contention,\nODC performs significantly better than 2PLDC.  Moreover, given a small\n{\\mbox{\\Large$\\epsilon$}}-spec, ODC returns more accurate results on the \ncommitted queries then 2PLDC. (ps) A New Client-Server Architecture for Distributed Query Processing Zhe Li, Kenneth A. Ross 1994-01-01 This paper presents the idea of ``tuple bit-vectors'' for\ndistributed query processing.  Using tuple bit-vectors, a new two-way\nsemijoin operator called 2SJ++ that enhances the semijoin with an\nessentially ``free'' backward reduction capability is proposed. We\nexplore in detail the benefits and costs of 2SJ++ compared with other\nsemijoin variants, and its effect on distributed query processing\nperformance.  We then focus on one particular distributed query\nprocessing algorithm, called the ``one-shot'' algorithm.  We modify\nthe one-shot algorithm by using 2SJ++ and demonstrate the improvements\nachieved in network transmission cost compared with the original\none-shot technique.  We use this improvement to demonstrate that\nequipped with the 2SJ++ technique, one can improve the performance of\ndistributed query processing algorithms significantly without adding\nmuch complexity to the algorithms. (ps) Statistical Augmentation of a {C}hinese Machine-Readable Dictionary Pascale Fung, Dekai Wu 1994-01-01 We describe a method of using statistically-collected Chinese\ncharacter groups from a corpus to augment a Chinese dictionary.  The\nmethod is particularly useful for extracting domain-specific and\nregional words not readily available in machine-readable dictionaries.\nOutput was evaluated both using human evaluators and against a\npreviously available dictionary.  We also evaluated performance\nimprovement in automatic Chinese tokenization.  Results show that our\nmethod outputs legitimate words, acronymic constructions, idioms,\nnames and titles, as well as technical compounds, many of which were\nlacking from the original dictionary. (ps) The complexity of multivariate elliptic problems with analytic data Arthur G. Werschulz 1994-01-01 Let $F$ be a class of functions defined on a $d$-dimensional domain.\nOur task is to compute $H^m$-norm $\\e$-approximations to solutions of\n$2m$th-order elliptic boundary-value problems $Lu=f$ for a fixed~$L$\nand for $f\\in F$.  We assume that the only information we can compute\nabout $f\\in F$ is the value of a finite number of continuous linear\nfunctionals of~$f$, each evaluation having cost~$c(d)$.  Previous work\nhas assumed that $F$ was the unit ball of a Sobolev space $H^r$ of fixed\nsmoothness~$r$, and it was found that the complexity of computing an\n$\\e$-approximation was $\\comp(\\e,d)=\\Theta(c(d)(1/\\e)^{d/(r+m)})$.\nSince the exponent of $1/\\e$ depends on~$d$, we see that the problem\nis intractable in~$1/\\e$ for any such $F$ of fixed smoothness~$r$.  In\nthis paper, we ask whether we can break intractability by letting $F$\nbe the unit ball of a space of infinite smoothness.  To be specific,\nwe let $F$ be the unit ball of a Hardy space of analytic functions\ndefined over a complex $d$-dimensional ball of radius greater than\none.  We then show that the problem is tractable in~$1/\\e$.  More\nprecisely, we prove that $\\comp(\\e,d)=\\Theta(c(d)(\\ln 1/\\e)^d)$, where\nthe $\\Theta$-constant depends on~$d$.  Since for any $p>0$, there is a\nfunction $K(\\cdot)$ such that $\\comp(\\e,d)\\le c(d) K(d)(1/\\e)^p$ for\nsufficiently small~$\\e$, we see that the problem is tractable, with\n(minimal) exponent~$0$.  Furthermore, we show how to construct a\nfinite element $p$-method (in the sense of Babu\\u{s}ka) that can\ncompute an $\\e$-approximation with cost $\\Theta(c(d)(\\ln 1/\\e)^d)$.\nHence this finite element method is a nearly optimal complexity\nalgorithm for $d$-dimensional elliptic problems with analytic data. (ps) Protocols for Loosely Synchronous Networks Danilo Florissi ,  Yechiam Yemini 1994-01-01 This   paper   overviews   a   novel   transfer   mode   for   B-ISDN:\nLoosely-synchronous  Transfer Mode  (LTM). LTM  operates  by signaling\nperiphery nodes when destinations become available. No frame structure\nis imposed by LTM, thus avoiding  adaptation layers. Additionally, LTM\ncan  deliver  a  spectrum  of  guaranteed  quality  of  services.  New\nSynchronous Protocol Stacks (SPSs) build on LTM by synchronizing their\nactivities to LTM  signals. Such signals can  be delivered directly to\napplications  that  may  synchronize its operations  to transmissions,\nthus minimizing buffering due to synchronization mismatches. SPSs  can\nuse current  transport protocols  unchanged  and, potentially, enhance\nthem with the real-time capabilities made possible through LTM. (ps) An Architecture for Integrating Concurrency Control into Environment Frameworks George T. Heineman, Gail E. Kaiser 1994-01-01 Research in layered and componentized systems shows the benefit of\n  dividing the reponsibility of services into separate components.  It is\n  still an unresolved issue, however, how a system can be created from a\n  set of existing (independently developed) components.  This issue of\n  integration is of immense concern to software architects since a proper\n  solution would reduce duplicate implementation efforts and promote\n  component reuse.  In this paper we take a step towards this goal within\n  the domain of software development environments (SDEs) by showing how to\n  integrate an external concurrency control component, called PERN, with an\n  environment framework.  We discuss two experiments of integrating PERN\n  with OZ, a decentralized process centered environment, and ProcessWEAVER,\n  a commercial process server.  We introduce an architecture for\n  retrofitting an external concurrency control component into an\n  environment.\n\nKeywords:\n\nComponentization, Transactions, Software Architecture, Collaborative Work (ps) Management of Application Quality of Service Patricia Gomes Soares Florissi ,  Yechiam Yemini 1994-01-01 This paper proposes a new language for the development of distributed\nmultimedia applications: Quality Assurance Language (QuAL). QuAL\nabstractions allow the specification of Quality of Service (QoS)\nconstraints expected from the underlying computing and communication\nenvironment. QuAL specifications are compiled into run time\ncomponents that monitor the actual QoS delivered. Upon QoS violations,\napplication provided exception handlers are signaled to act upon the\nfaulty events. Language level abstractions of QoS shelter programs\nfrom the heterogeneity of underlying infrastructures. This simplifies\nthe development and maintenance of mul- timedia applications and\npromotes their portability and reuse. QuAL generates Management\nInformation Bases (MIBs) that contain QoS statistics per application.\nSuch MIBs may be used to integrate application level QoS management\ninto standard network management frameworks. (ps) Computing High Dimensional Integrals with Applications to Finance Spassimir H. Paskov 1994-01-01 High-dimensional integrals are usually solved with Monte Carlo\nalgorithms although theory suggests that low discrepancy algorithms\nare sometimes superior.  We report on numerical testing which compares\nlow discrepancy and Monte Carlo algorithms on the evaluation of\nfinancial derivatives. The testing is performed on a Collateralized\nMortgage Obligation (CMO) which is formulated as the computation of\nten integrals of dimension up to 360.\n\nWe tested two low discrepancy algorithms (Sobol and Halton) and two\nrandomized algorithms (classical Monte Carlo and Monte Carlo combined\nwith antithetic variables). We conclude that for this CMO the Sobol\nalgorithm is always superior to the other algorithms. We believe that\nit will be advantageous to use the Sobol algorithm for many other\ntypes of financial derivatives.\n\nOur conclusion regarding the superiority of the Sobol algorithm also\nholds when a rather small number of sample points are used, an\nimportant case in practice.\n\nWe built a software system which runs on a network of heterogeneous\nworkstations under PVM 3.2 (Parallel Virtual Machine). Since\nworkstations are ubiquitous, this is a cost-effective way to do very\nlarge computations fast.  The measured speedup is at least .9N for N\nworkstations, $N \\leq 25$. The software can also be used to compute high\ndimensional integrals on a single workstation. (ps) Predictive Dynamic Load Balancing of Parallel and Distributed Rule and Query Processing Hasanat M. Dewan, Salvatore J. Stolfo, Mauricio A. Hernandez, Jae-Jun Hwang 1994-01-01 Expert Databases are environments that support the processing  of\nrule  programs  against  a disk resident database.  They occupy a\nposition intermediate between  active  and  deductive  databases,\nwith  respect  to the level of abstraction of the underlying rule\nlanguage.  The operational semantics of the rule language  influ-\nences the problem solving strategy, while the architecture of the\nprocessing environment determines efficiency and scalability.\n\nIn this paper, we present elements of the PARADISER  architecture\nand its kernel rule language, PARULEL.  The PARADISER environment\nprovides support for parallel and distributed evaluation of  rule\nprograms,  as well as static and dynamic load balancing protocols\nthat predictively balance a computation at runtime.  This  combi-\nnation  of  features results in a scalable database rule and com-\nplex query processing architecture. We  validate  our  claims  by\nanalyzing  the  performance  of the system for two realistic test\ncases. In particular, we show how the performance of  a  parallel\nimplementation of transitive closure is significantly improved by\npredictive dynamic load balancing. (ps) Predictive Dynamic Load Balancing of Parallel Hash-Joins over Heterogeneous Processors in the Presence of Data Skew Hasanat M. Dewan, Mauricio A. Hernandez, Kui W. Mok, Salvatore J. Stolfo 1994-01-01 In this paper, we present new algorithms to balance the  computa-\ntion  of parallel hash joins over heterogeneous processors in the\npresence of data skew and external loads.  Heterogeneity  in  our\nmodel  consists  of disparate computing elements, as well as gen-\neral purpose computing ensembles that  are  subject  to  external\nloading.   Data  skew appears as  significant non-uniformities in\nthe distribution of attribute values of underlying relations that\nare involved in a join.\n\nWe develop cost models and predictive dynamic load balancing pro-\ntocols  to  detect  imbalance  during the computation of a single\nlarge join. Our algorithms can account for imbalance due to  data\nskew  as well as heterogeneity in the computing environment. Sig-\nnificant performance gains are reported for a wide range of  test\ncases on a prototype implementation of the system.\n\nKeywords:\nLoad Balancing, Parallel Join, Hash Join, Data Skew, Parallel Processing,\nHeterogeneity (ps) Expanding the Repertoire of Process-based Tool Integration Giuseppe Valetto 1994-01-01 The purpose of this thesis is to design and implement a new protocol\nfor Black Box tool enveloping, in the context of the Oz Process\nCentered Environment, as an auxiliary mechanism that deals with\nadditional families of tools, whose character prevents a thoroughly\nsatisfactory service by the current encapsulation method.  We mean to\naddress interpretive and query systems, multi-user collaborative and\nnon-collaborative tools, and programs that allow incremental binding\nof parameters after start-up and storing of intermediate and/or\npartial results.  Our goal is to support a greater amount of\ninteraction between multiple human operators, the tools and the\nenvironment, in the context of complex software development and\nmanagement tasks.  During the realization of this project, we\nintroduced several concepts related to integration of Commercial\nOff-The-Shelf tools into Software Development Environments: an\napproach based on multiple enveloping protocols, a categorization of\ntools according to their multi-tasking and multi-user capabilities,\nthe ideas of loose wrapping (as opposed to the usual tight wrapping)\nand of persistent tools (with respect to the duration of a single\ntask), and a functional extension of some intrinsically single-user\napplications to a (limited) form of collaboration. (ps) Enveloping Sophisticated Tools into Computer-Aided Software Engineering Environments Giuseppe Valetto, Gail E. Kaiser 1994-01-01 We present a CASE-tool integration strategy based on enveloping\npre-existing tools without source code modifications, recompilation,\nor assuming an extension language or any other special capabilities on\nthe part of the tool.  This Black Box enveloping (or wrapping) idea\nhas been around for a long time, but was previously restricted to\nrelatively simple tools.  We describe the design and implementation of\na new Black Box enveloping facility intended for sophisticated tools\n--- with particular concern for the emerging class of groupware \napplications. (ps) Stereo in the Presence of Specular Reflection Dinkar N. Bhat,  Shree K. Nayar 1994-01-01 The problem of accurate depth estimation using stereo\nin the presence of specular reflection is addressed. Specular\nreflection is viewpoint dependent and can cause large intensity\ndifferences at corresponding points.\nHence, mismatches could result causing significant depth errors.\nCurrent stereo algorithms largely ignore specular reflection\nwhich is a fundamental reflection phenomenon from surfaces, both\nsmooth and rough. We analyzed the physics of specular reflection\nand the geometry of stereopsis which led us to an interesting\nrelationship between stereo vergence, surface roughness, and the\nlikelihood of a correct match. Given the lower bound on\nsurface roughness, an optimal binocular stereo configuration\ncan be determined which maximizes precision in depth\nestimation despite specular reflection.\nHowever, surface roughness is difficult to estimate in unstructured \nenvironments. Therefore, multiple view configurations independent \nof surface roughness are determined such that at each scene point \nvisible to all sensors, at least one stereo pair provides a correct \ndepth estimate. We have developed a simple algorithm to reconstruct \ndepth from the multiple view images. Experiments with real surfaces confirm the\nviability of our approach. A key feature of this approach is that we do not\nseek to eliminate or avoid specular reflection, but rather minimize its\neffect on stereo matching. (ps) Toward Scalable and Parallel Inductive Learning: A Case Study in Splice Junction Prediction Philip K. Chan, Salvatore J. Stolfo 1994-01-01 Much of the research in inductive learning concentrates on problems\nwith relatively small amounts of training data.  With the steady\nprogress of the Human Genome Project, it is likely that orders of\nmagnitude more data in sequence databases will be available in the\nnear future for various learning problems of biological importance.\nThus, techniques that provide the means of {\\it scaling} machine\nlearning algorithms requires considerable attention.\n\n{\\it Meta-learning} is proposed as a general technique to integrate a\nnumber of distinct learning processes that aims to provide a means of\nscaling to large problems.  This paper details several meta-learning\nstrategies for integrating independently learned classifiers on\nsubsets of training data by the same learner in a parallel and\ndistributed computing environment.  Our strategies are particularly\nsuited for massive amounts of data that main-memory-based learning\nalgorithms cannot handle efficiently.  The strategies are also\nindependent of the particular learning algorithm used and the\nunderlying parallel and distributed platform.  Preliminary experiments\nusing different learning algorithms in a simulated parallel\nenvironment demonstrate encouraging results: parallel learning by\nmeta-learning can achieve comparable prediction accuracy in less space\nand time than serial learning. (ps) Essential-Hazard-Free State Minimization of Incompletely Specified Asynchronous Sequential Machines Fu-Chiung J. Cheng,  Luis Plana, Stephen H. Unger 1994-01-01 This paper describes a novel algorithm for essential-hazard-free state\nminimization of incompletely specified asynchronous sequential machines.\nSome novel techniques for  elimination and avoidance of\npotential transient and steady state essential hazards \nunder unbounded delay assumption are proposed and\nexploited in our algorithm.  This paper illustrates that apparent steady state\nessential hazards can be removed from a flow table if at least two of\nthe rows which contribute to the hazard can be merged. It also shows\nthat the existing state merging algorithms introduce steady state and\ntransient essential hazards that can be avoided.  The algorithm has\nbeen implemented and applied to over a dozen asynchronous sequential\nmachines. Results are compared with results of\nnon-essential-hazard-free method SIS.  Most of the tested cases can be\nreduced to essential hazard free flow tables. (ps) REVISION-BASED GENERATION OF NATURAL LANGUAGE SUMMARIES PROVIDING HISTORICAL BACKGROUND: Corpus-based Analysis, Design, Implementation and Evaluation Jacques Robin 1994-01-01 Automatically summarizing vast amounts of on-line quantitative data with a \nshort natural language paragraph has a wide range of real-world applications. \nHowever, this specific task raises a number of difficult issues that are quite \ndistinct from the generic task of language generation: conciseness, complex \nsentences, floating concepts, historical background, paraphrasing power and \nimplicit content.\n\nIn this thesis, I address these specific issues by proposing a new generation \nmodel in which a first pass builds a draft containing only the essential new \nfacts to report and a second pass incrementally revises this draft to \nopportunistically add as many background facts as can fit within the space \nlimit. This model requires a new type of linguistic knowledge: revision \noperations, which specifyies the various ways a draft can be transformed in \norder to concisely accommodate a new piece of information. I present an \nin-depth corpus analysis of human-written sports summaries that resulted\nin an extensive set of such revision operations. I also present the \nimplementation,based on functional unification grammars, of the system STREAK, \nwhich relies on these operations to incrementally generate complex sentences \nsummarizing basketball games. This thesis also contains two quantitative \nevaluations. The first shows that the new revision-based generation model \nis far more robust than the one-shot model of previous generators. The second \nevaluation demonstrates that the revision operations acquired during the corpus\nanalysis and implemented in STREAK are, for the most part, portable to at least\none other quantitative domain (the stock market).\n\nSTREAK is the first report generator that systematically places the facts which\nit summarizes in their historical perspective. It is more concise than previous\nsystems thanks to its ability to generate more complex sentences and to \nopportunistically convey facts by adding a few words to carefully chosen draft \nconstituents. The revision operations on which STREAK is based constitute the \nfirst set of corpus-based linguistic knowledge geared towards incremental \ngeneration. The evaluation presented in this thesis is also the first attempt \nto quantitatively assess the robustness of a new generation model and the \nportability of a new type of linguistic knowledge. (ps) PGMAKE: A Portable Distributed Make System Andrew Lih, Erez Zadok 1994-01-01 (ps) Discovery and Hot Replacement of Replicated Read-Only File Systems, with Application to Mobile Computing (M.S. Thesis Erez Zadok 1994-01-01 (ps) MeldC: A Reflective Object-Oriented Coordination Language Gail  E.  Kaiser, Wenwey Hseush, James Lee, Felix Wu, Ester Woo, Eric Hilsdale 1993-01-01 We   present  a  coordination   language,   MeldC,  for   open  system\nprogramming.  MeldC is a C-based, concurrent, object-oriented language\nbuilt on a reflective architecture.  The key of the reflective feature\nis that the metaclass supports  {\\it  shadow}  objects that  implement\n{\\it secondary behaviors} of objects.  The  behavior of  an object can\nbe extended by dynamically composing multiple secondary behaviors with\nthe primary behavior of the object which  is defined by its class. The\nmechanism  is referred to as  {\\it dynamic composition}.  Our focus is\nto  study the language architecture with which programmers are able to\nconstruct -- without modifying the language internals --  new language\nfeatures in a high-level  and efficient  way. We demonstrate that the\nMeldC  reflective  architecture  is  the  right  approach to  building\ndistributed and persistent systems. (ps) Extending Attribute Grammars to Support Programming-in-the-Large Josephine Micallef, Gail E. Kaiser 1993-01-01 Attribute grammars add specification of static semantic properties to\ncontext-free grammars, which in turn describe the syntactic structure\nof program units.  However, context-free grammars cannot express\nprogramming-in-the-large features common in modern programming\nlanguages, including unordered collections of units, included units\nand sharing of included units.  We present extensions to context-free\ngrammars, and corresponding extensions to attribute grammars, suitable\nfor defining such features.  We explain how batch and incremental\nattribute evaluation algorithms can be adapted to support these\nextensions, resulting in a uniform approach to intra-unit and\ninter-unit static semantic analysis and translation of multi-unit\nprograms. (ps) On the Cost of Transitive Closures in Relational Databases Zhe Li,  Ken Ross 1993-01-01 We consider the question of taking transitive closures on top of pure\nrelational systems (Sybase and Ingres in this case). We developed\nthree kinds of transitive closure programs, one using a stored\nprocedure to simulate a built-in transitive closure operator, one\nusing the C language embedded with SQL statements to simulate the\niterated execution of the transitive closure operation, and one using\nFloyd's matrix algorithm to compute the transitive closure of an input\ngraph.  By comparing and analyzing the respective performances of\ntheir different versions in terms of elapsed time spent on taking the\ntransitive closure, we identify some of the bottlenecks that arise\nwhen defining the transitive closure operator on top of existing\nrelational systems. The main purpose of the work is to estimate the\ncosts of taking transitive closures on top of relational systems,\nisolate the different cost factors (such as logging, network\ntransmission cost, etc.), and identify some necessary enhancements to\nexisting relational systems in order to support transitive closure\noperation efficiently.  We argue that relational databases should be\naugmented with efficient transitive closure operators if such queries\nare made frequently. (ps) crep: a regular expression-matching textual corpus tool Darrin Duford 1993-01-01 crep is a UNIX tool which searches either a tagged or free textual\ncorpus file and outputs each sentence that matches the specified\nregular expression provided by the user as a parameter.  The\nexpression consists of user-defined regular expressions and\npart-of-speech tags.  The purpose of crep is to make the searches\nfaster and easier than by either a) searching through corpora by hand;\nor b) constructing a lexical scanner for each specific search.  \ncrep achieves this facilitation by offering the user a simple\nexpression syntax, from which it automatically constructs an \nappropriate scanner.  The user therefore has the ability to execute a \nwhole search in one command, invoking several modules explicity or\nimplicitly, including a sentence delimiter, a part-of-speech tagger, \nan expression matcher, and various output filters. (ps) Cooperative Transactions for Multi-User Environments Gail E. Kaiser 1993-01-01 We survey extended transaction models proposed to support long\nduration, interactive and/or cooperative activities in the context of\nmulti-user software development and CAD/CAM environments.  Many of\nthese are variants of the checkout model, which addresses the long\nduration and interactive nature of the activities supported by\nenvironments but still isolates environment users so that they cannot\n(or at least are not supposed to) collaborate while their activities\nare in progress.  However, a few cooperative transaction models have\nbeen proposed to facilitate collaboration, usually while maintaining\nsome guarantees of consistency. (ps) There Exists a Problem Whose Computational Complexity is Any Given Function of the Information Complexity Ming Chu 1993-01-01 We present an information-based complexity\nproblem for which the computational complexity can be any given\nincreasing function of the information complexity, and the information\ncomplexity can be any non-decreasing function of $\\varepsilon^{-1}$,\nwhere $\\varepsilon$ is the error parameter. (pdf) Logical Embeddings for Minimum Congestion Routing in Lightwave Networks B\\\"ulent Yener, Terrance E. Boult\" 1993-01-01 The problem considered in this paper is motivated by the independence\nbetween logical and physical topology in Wavelength Division\nMultiplexing (WDM) based local and metropolitan lightwave networks.\n\n  This paper suggests logical embeddings of digraphs into multihop lightwave\nnetworks to maximize the throughput under nonuniform traffic\nconditions.\n\n Defining {\\it congestion} as the maximum flow carried on any link, two\nperturbation heuristics are presented to find a {\\it good logical}\nembedding on which the routing problem is solved with minimum congestion.\n\n\n  A constructive proof for a lower bound of the problem is given, and\nobtaining an optimal solution for integral routing is shown to be\nNP-Complete.\n\n\n   The performance of the heuristics is empirically analyzed on various\ntraffic models.  Simulation results show that our heuristics perform, on\nthe average, $20\\%$ from a computed lower bound. Since this lower bound is\nnot quite tight, we suspect that the actual performance is better. In\naddition, we show that $5\\% - 20\\%$ performance improvements can be\nobtained over the previous work. (ps) Oz: A Decentralized Process Centered Environment (Ph.D. Thesis Proposal Israel Z. Ben-Shaul 1993-01-01 This is a proposal for a model and an architecture for \ndecentralized process centered environments, supporting collaboration and\nconcerted efforts among geographically-dispersed teams -- each team with\nits own autonomous process -- with emphasis on flexible control over the\ndegree of collaboration and autonomy provided. The focus is on\ndecentralized process {\\it modeling} and on decentralized process {\\it\nenaction. (ps) The complexity of two-point boundary-value problems with piecewise analytic data Arthur G. Werschulz 1993-01-01 Previous work on the $\\e$-complexity of elliptic boundary-value problems\n$Lu=f$ assumed that the class~$F$ of problem elements~$f$ was the unit\nball of a Sobolev space.  In a recent paper, we considered the case\nof a model two-point boundary-value problem,\nwith $F$ being a class of analytic functions.  In this paper, we ask\nwhat happens if $F$ is a class of piecewise analytic functions.  We\nfind that the complexity depends strongly on how much a priori\ninformation we have about the breakpoints.  If the location of the\nbreakpoints is known, then the $\\e$-complexity is proportional to\n$\\ln(\\e^{-1})$, and there is a finite element $p$-method (in the sense\nof Babu\\u{s}ka) whose cost is optimal to within a constant factor.\nIf we know neither the location nor the number of breakpoints, then\nthe problem is unsolvable for $\\e<\\sqrt{2}$.  If we know only that\nthere are $b\\ge 2$ breakpoints, but we don't know their location, then\nthe $\\e$-complexity is proportional to $b\\e^{-1}$, and a finite element\n$h$-method is nearly optimal.  In short, knowing the location of the\nbreakpoints is as good as knowing that the problem elements are\nanalytic, whereas only knowing the number of breakpoints is no better\nthan knowing that the problem elements have a bounded derivative in\nthe $L_2$ sense. (pdf) A Repository for a CARE Environment Toni A. B\\\"unter\" 1993-01-01 Repositories in CASE hold information about the development process and the\nstructure of developing software. The migration or reuse of CASE\nrepositories for CARE (Computer Aided Re-Engineering) is not adequate for \nthe reengineering process. The main reasons for its inadequacy are the \nemptyness of such repositories, and the nature of the process itself. \nIn the following report we will define a CARE architecture, from the \nreengineering point of view, and derive a structure of a repository \nappropriate to the reengineering process. (ps) A Non-Deterministic Approach to Restructuring Flow Graphs Toni A. B\\\"unter\" 1993-01-01 The history of programming is filled with works about the properties\nof program flow graphs. There are many approaches to defining the\nquality of such graphs, and to improving a given flow graph by\nrestructuring the underlying source code.  We present here a new,\ntwofold approach to restructuring the control flow of arbitrary source\ncode. The first part of the method is a classical deterministic\nalgorithm; the second part is non-deterministic and involves user\ninteraction. The method is based on node splitting, enabling it to\nsatisfy the definition of the extended Nassi-Shneiderman diagrams. (ps) Isochronets: a High-Speed Network Switching Architecture Danilo Florissi 1993-01-01 Traditional  switching   techniques  need  hundred-  or  thousand-MIPS\nprocessing power within switches to  support Gbit/s transmission rates\navailable  today.  These  techniques anchor  their  decision-making on\ncontrol information within transmitted frames and  thus  must  resolve\nroutes at the speed in which frames are being pumped into switches.\n\nIsochronets can potentially  switch at any transmission rate by making\nswitching decisions independent of frame contents.  Isochronets divide\nnetwork  bandwidth among  routing  trees,  a  technique  called  Route\nDivision  Multiple  Access (RDMA).   Frames  access  network resources\nthrough  the  appropriate  routing tree  to  the  destination.   Frame\nstructures  are  irrelevant  for  switching  decisions.  Consequently,\nIsochronets can support multiple framing protocols without  adaptation\nlayers and are strong candidates for all-optical implementations.  All\nnetwork-layer functions  are reduced to an admission control mechanism\ndesigned  to provide quality of service (QOS) guarantees for  multiple\nclasses of traffic.\n\nThe main results of this work are:\n\n(1)  A new network architecture suitable for high-speed transmissions;\n\n(2)  An  implementation  of  Isochronets  using  cheap   off-the-shelf\n     components;\n\n(3)  A comparison of RDMA  with more traditional switching techniques,\n     such as Packet Switching and Circuit Switching;\n\n(4)  New protocols necessary for Isochronet operations; and\n\n(5)  Use  of Isochronet techniques at higher  layers  of  the protocol\n     stack (in particular, we show how Isochronet techniques may solve\n     routing problems in ATM networks). (ps) A Configuration Process for a Distributed Software Development Environment Israel Z. Ben-Shaul, Gail E. Kaiser 1993-01-01 This paper describes work-in-progress on a configuration facility for\na multi-site software development environment.  The environment\nsupports collaboration among geographically-dispersed teams of\nsoftware developers.  Addition and deletion of local subenvironment\nsites to a global environment is performed interactively inside any\none of the existing local subenvironments, with the same user\ninterface normally employed for invoking software development tools.\nThis registration process is defined and executed using the same\nnotation and mechanisms, respectively, as for the software development\nprocess.  Each remote site is represented by a root object in the\ndistributed objectbase containing the software under development; each\nlocal subobjectbase can be displayed and queried at any site, but only\nits root is physically copied at every site.  Everything described in\nthis paper has been implemented and is working, but since we are in\nthe midst of experimentation, we do not expect that the ``final''\nsystem will be exactly as described here. (ps) Graphical Editing by Example David Kurlander 1993-01-01 Constructing illustrations by computer can be both tedious and\ndifficult.  This doctoral thesis introduces five example-based\ntechniques to facilitate the process. These techniques are\nindependently useful, but also interrelate in interesting ways:\n\n* Graphical Search and Replace, the analogue to textual search and\nreplace in text editors, is useful for making repetitive changes\nthroughout graphical documents.\n\n* Constraint-Based Search and Replace, an extension to graphical\nsearch and replace, allows users to define their own illustration\nbeautification rules and constraint inferencing rules by\ndemonstration.\n\n* Constraint Inferencing from Multiple Snapshots facilitates\nconstraint specification by automatically computing constraints that\nhold in multiple configurations of an illustration.\n\n* Editable Graphical Histories, a visual representation of commands in\na graphical user interface, are useful for reviewing, undoing, and\nredoing sets of operations.\n\n* Graphical Macros By Example, based on this history representation,\nallow users to scroll through previously executed commands and\nencapsulate useful sequences into macros. These macros can be\ngeneralized into procedures, with arguments and flow of control using\ngraphical and constraint-based search and replace.\n\nIndividually and in combination, these techniques reduce repetition in\ngraphical editing tasks, visually and by example, using the\napplication's own interface. These techniques have been implemented in\nChimera, an editor built to serve as a testbed for this research. (ps) DFLOPS:A Dataflow Machine for Production Systems Fu-Chiung Cheng, Mei-Yi Wu 1993-01-01 Many production system machines have been proposed to speed up the \nexecution of production system programs. Most of them are implemented \nin kinds of conventional control flow model of execution which is \nlimited by \"von Neumann bottleneck.\" In this paper we propose a new \nmultiprocessor dataflow machine, called DFLOPS, for parallel processing\nof production systems. The rules programs are compiled into dataflow \ngraphs and then map into DFLOPS processing elements. Our parallel \nexecution model exploits not only matching rules in parallel but also  \nfiring rules in parallel. The design and implementation of  DFLOPS is \npresented in detail. The distinguishing characteristics of this proposed \nmachine lies in its  simplicity, fully-pipelined processing and  fine \ngrain parallelism. MISD, SIMD and/or MIMD modes of execution can be \nexploited in this machine according to the properties of applications.\nThe initial results reveal that the performance of production systems is\ngreatly improved. (ps) Disconnected Operation in a Multi-User Software Development Environment Peter D. Skopp, Gail E. Kaiser 1993-01-01 Software Development Environments have traditionally relied upon a\ncentral project database and file repository, accessible to a\nprogrammer's workstation via a local area network connection.  The\nintroduction of powerful mobile computers has demonstrated the need\nfor a new model, which allows for machines with transient network\nconnectivity to assist programmers in product development.  We propose\na process-based checkout model by which process and product files that\nmay be needed during a planned period of dis-connectivity are\npre-fetched with minimal user effort.  Rather than selecting each file\nby hand, which is tedious and error-prone, the user only informs the\nenvironment of the portion of the software development process\nintended to be executed while disconnected.  The environment is then\nresponsible for pre-fetching the necessary files.  We hope that this\napproach will enable programmers to continue working on a project\nwithout network access. (ps) Terminological Constraint Network Reasoning and its Application to Plan Recognition (Thesis Proposal Robert Weida 1993-01-01 Terminological systems in the tradition of KL-ONE are widely used in AI to\nrepresent and reason with concept descriptions.  They compute subsumption\nrelations between concepts and automatically classify concepts into a\ntaxonomy having well-founded semantics.  Each concept in the taxonomy\ndescribes a set of possible instances which are a superset of those\ndescribed by its descendants.  One limitation of current systems is their\ninability to handle complex compositions of concepts, such as constraint\nnetworks where each node is described by an associated concept.  For\nexample, plans are often represented (in part) as collections of actions\nrelated by a rich variety of temporal and other constraints.  The T-REX\nsystem integrates terminological reasoning with constraint network\nreasoning to classify such plans, producing a ``terminological'' plan\nlibrary.  T-REX also introduces a new theory of plan recognition as a\ndeductive process which dynamically partitions the plan library by\nmodalities, e.g., necessary, possible and impossible, while observations\nare made.  Plan recognition is guided by the plan library's terminological\nnature.  Varying assumptions about the accuracy and monotonicity of the\nobservations are addressed.  Although this work focuses on temporal\nconstraint networks used to represent plans, terminological systems can be\nextended to encompass constraint networks in other domains as well. (ps) Hazards, Critical Races, and Metastability Stephen H. Unger 1993-01-01 The various modes of failure of asynchronous sequential logic circuits\ndue to timing problems are considered.  These are hazards, critical\nraces and metastable states.  It is shown that there is a mechanism\ncommon to all forms of hazards and to metastable states.  A similar\nmechanism, with added complications, is shown to characterize critical\nraces.  Means for defeating various types of hazards and critical\nraces through the use of one-sided delay constraints are introduced.\nA method is described for determining from a flow table situations in\nwhich metastable states may be entered.  A circuit technique for\ndefeating metastability problems in self timed systems is presented.\nIt is shown that the use of simulation for verifying the correctness\nof a circuit with given bounds on the branch delays cannot be relied\nupon to expose all timing problems. An example is presented that\nrefutes the conjecture that replacing pure delays with inertial delays\ncan only eliminate glitches. (ps) Optimization of the Binding Mechanism of the Characteristic Function in Marvel Bunter,  Toni A. 1993-01-01 The applied binding mechanism in {\\sc Marvel} 3.1 for the\ncharacteristic function checks all instances of a given class against the\nbinding formula, regardless of  the actual structure of the formula\nand its predicates. This can cause unnecessary computation overhead\nwhile executing a rule. This report displays a more advanced mechanism\nconsidering relational information between objects, the structure of\nthe binding formula and optimizing rewriting of the binding formula. (ps) An Approach for Distributed Query Processing in Marvel: Concepts and Implementation Bunter,  Toni A. 1993-01-01 This work displays an approach for the query processing of\n{\\sc Marvel} rules upon a distributed {\\sc Marvel} objectbase. \nRules and rest rules run simultaneously on different \nsubenvironments, synchronized by a coordinating subenvironment.\nInstead of transmitting objects, the showed method transmits\nimages. The concept of lazy calling is introduced. (ps) New Lower Bounds on the Cost of Binary Search Trees De Prisco,  R., De Santis,  A. 1993-01-01 In this paper we provide new lower bounds on the cost\nof binary search trees. The bounds are expressed in terms\nof the entropy of the probability distribution,\nthe number of elements and the probability\nthat a search is successfully.\nMost of our lower bounds are derived by means of a new technique\nwhich exploits the relation between trees and codes.\nOur lower bounds compare favorably with known limitations.\n\nWe also provide an achievable upper bound on the Kraft sum\ngeneralized to the internal nodes of a tree. This\nimproves on a previous result. (ps) Catastrophic Faults in Reconfigurable Linear Arrays of Processors De Prisco,  R., De Santis,  A. 1993-01-01 In regular architectures of identical processing elements,\na widely used technique to improve the reconfigurability\nof the system consists of providing redundant processing\nelements and mechanisms of reconfiguration. \n\nIn this paper we consider linear arrays of processing elements,\nwith unidirectional bypass links of length $g$.\nWe count the number of particular sets of faulty\nprocessing elements. We show that\nthe number of catastrophic faults of $g$ elements\nis equal to the $(g-1)$-th Catalan number.\nWe also provide algorithms to \nrank and unrank all catastrophic sets of $g$ faults.\nFinally, we describe a linear time algorithm that generates all \nsuch sets of faults. (ps) Minimal Path Length of Trees with Known Fringe Roberto De Prisco,  Giuseppe Parlati,  Giu 1993-01-01 In this paper we study the path length of binary trees with known\nnumber of leaves and known fringe, that is the difference\nbetween the longest and the shortest root-leaf path.\nWe compute the path length of the minimal tree with given\nnumber of leaves $N$ and fringe $\\Delta$ for the case $\\Delta\\ge N/2$.\nThis complements a known tight lower bound that holds in the case\n$\\Delta\\le N/2$.  Our method also yields a linear time algorithm for\nconstructing the minimal tree valid for any $N$ and $\\Delta$. (ps) Solution of Ulam's Problem on Binary Search with Four Lies Vincenzo Auletta,  Alberto Negro,  Giusepp 1993-01-01 S.M. Ulam, in his autobiography (1976), suggested\nan interesting two--person search game which can be formalized as follows:\na Responder chooses an element $x$ in $\\{ 1,2,\\ldots,1000000 \\}$\nunknown to a Questioner. The Questioner has to find\nit by asking queries of the form ``$x \\in Q ?$\", where $Q$ is an\narbitrary subset of $\\{ 1,2,\\ldots, 1000000 \\}$. The Responder provides\n``yes\" or ``no\" answers, some of which may be {\\it erroneous }.\n\nIn this paper we determine the minimal number of yes-no queries needed to find\nthe unknown integer $x$ between $1$ and $1000000$ if at most four of the\nanswers may be erroneous. (ps) Process Centered Software Development on Mobile Hosts Peter D. Skopp 1993-01-01 Software Development Environments have traditionally relied upon a\ncentral project database and file repository, accessible to a\nprogrammer's workstation via a high speed local area network\nconnection.  The introduction of powerful mobile computers has\ndemonstrated the need for a new model, which allows for variable\nbandwidth machines as well as transient network connectivity to assist\nprogrammers in product development.  A new client-server model is\nintroduced which minimizes network traffic when bandwidth is limited.\nTo support disconnected operation, I propose a em process-based\ncheckout model by which process information and product files that may\nbe needed during a planned period of dis-connectivity are pre-fetched\nwith minimal user effort.  Rather than selecting each file by hand,\nwhich is tedious and error-prone, the user only informs the\nenvironment of the portion of the software development process\nintended to be executed while disconnected.  The environment is then\nresponsible for pre-fetching the necessary files.  It is hoped that\nthese research efforts will enable programmers to continue working on\na project without continuous high speed network access. (ps) Automatic Translation of Process Modeling Formalisms George T. Heineman 1993-01-01 This paper demonstrates that the enaction of a software process can\nbe separated from the formalism in which the process is modeled.\nTo show this, we built a compiler capable of automatically\ntranslating a process model specified using Statemate into a MARVEL\nenvironment which then supports the enaction of the process. (pdf) (ps) Adaptive Remote Paging for Mobile Computers Bill N. Schilit, Dan Duchamp 1991-01-01 There is a strong trend toward the production of small ``notebook''\r\ncomputers.  The small size of portable computers places inherent\r\nlimits on their storage capacity, making remote paging desirable or\r\nnecessary.  Further, mobile computers can ``walk away from'' their\r\nservers, increasing load on network routing resources and/or breaking\r\nnetwork connections altogether.  Therefore, it is desirable to allow\r\nclient-server matchups to be made dynamically and to vary over time,\r\nso that a client might always be connected to nearby servers.\r\nAccordingly, we have built a self-organizing paging service that\r\nadapts to changes in locale and that stores pages in remote memory if\r\npossible.  We show empirically that there is no performance penalty\r\nfor using our paging facility instead of a local disk.  This suggests\r\nthat portable computers need neither a hard disk nor an excessive\r\namount of RAM, provided that they will operate in environments in\r\nwhich remote storage is plentiful.  These are important facts because\r\nboth a hard disk and large amounts of RAM are undesirable\r\ncharacteristics for very small portable computers. (pdf) (ps) {IP}-based Protocols for Mobile Internetworking John Ioannidis, Dan Duchamp, Gerald Q. Maguire Jr. 1991-01-01 We consider the problem of providing network access to hosts whose\nphysical location changes with time. Such hosts cannot depend on\ntraditional forms of network connectivity and routing because their\nlocation, and hence the route to reach them, cannot be deduced from\ntheir IP address. We present protocols that seamlessly integrate\nmobile hosts into the current IP networking infrastructure. They are\nprimarily targeted at supporting a campus environment with mobile\ncomputers, but also extend gracefully to accomodate hosts moving\nbetween different networks. The key feature is the dependence on\nancillary machines to track the location of the mobile hosts.  Our\nprotocols are designed to react quickly to changing topologies, to\nscale well, and not to place an overwhelming burden on the network. (ps) Inferring Constraints from Multiple Snapshots David Kurlander, Steven Feiner 1991-01-01 Many graphics tasks, such as the manipulation of graphical objects,\nand the construction of user-interface widgets, can be facilitated by\ngeometric constraints. However, the difficulty of specify-ing\nconstraints by traditional methods forms a barrier to their widespread\nuse. In order to make constraints easier to declare, we have developed\na method of specifying constraints implicitly, through multiple\nexamples. Snapshots are taken of an initial scene configuration, and\none or more additional snapshots are taken after the scene has been\nedited into other valid configurations. The constraints that are\nsatisfied in all the snapshots are then applied to the scene objects.\nWe discuss an efficient algorithm for inferring constraints from\nmultiple snapshots. The algorithm has been incorporated into the\nChimera editor, and several examples of its use are discussed. (pdf) (ps) Extending A Tool Integration Language Mark A. Gisi, Gail E. Kaiser 1991-01-01 The {\\sc Marvel} environment supports rule-based automation of\nsoftware processes. {\\sc Marvel} invokes external tools to carry out\nsteps in a software process. One of the major objectives of this\nresearch is to invoke external tools to carry out steps in a software\nprocess without modifying the tools. This is achieved by encapsulating\ntools in {\\it envelopes}, designed to abstract the details of a tool\nfrom the {\\sc Marvel} kernel, thereby providing a ``black box''\ninterface.  Initially we used the {\\sc Unix} shell script language to\nwrite envelopes.  However, due to several limitations of the shell\nlanguage the black box abstraction could not be fully supported.  We\ndescribe these limitations and discuss how we extended the shell\nlanguage to obtain a new envelope language that fully supports the\nblack box abstraction. (ps) Fully Dynamic Algorithms for 2-Edge Connectivity Zvi Galil, Giuseppe F. Italiano 1991-01-01 We study the problem of maintaining the 2-edge-connected components of\na graph undergoing repeated dynamic modifications, such as edge\ninsertions and edge deletions.  We show how to test at any time\nwhether two vertices belong to the same 2-edge-connected component,\nand how to insert and delete an edge in $O(m^{2/3})$ time in the worst\ncase, where $m$ is the current number of edges in the graph. This\nanswers a question posed by Westbrook and Tarjan [WT89].  For\nplanar graphs, we present algorithms that support all these operations\nin $O(\\sqrt{n\\log\\log n}\\,)$ worst-case time each, where $n$ is the\ntotal number of vertices in the graph. (ps) Maintaining the 3-Edge-Connected Components of a Graph On-Line Zvi Galil, Giuseppe F. Italiano 1991-01-01 We study the problem of maintaining the 3-edge-connected components of\na graph undergoing repeated dynamic modifications, such as edge and\nvertex insertions. We show how to answer whether two vertices belong\nto the same 3-edge-connected component of a connected graph that is\nundergoing only edge insertions. Any sequence of $q$ query and updates\non an $n$-vertex graph can be performed in $O((n+q)\\alpha(q,n))$ time. (ps) Maintaining Biconnected Components of Dynamic Planar Graphs Zvi Galil, Giuseppe F. Italiano 1991-01-01 We present algorithms for maintaining the biconnected components of a\nplanar graph undergoing repeated dynamic modifications, such as\ninsertions and deletions of edges and vertices. We show how to test at\nany time whether two vertices belong to the same biconnected\ncomponent, and how to insert and delete an edge in $O(n^{2/3})$ time\nin the worst case, where $n$ is the number of vertices in the graph.\nThe data structure supports also insertions of new vertices and\ndeletions of disconnected vertices in the same time bounds. This is\nthe first sublinear-time algorithm known for this problem. (ps) Dynamic data structures for graphs Giuseppe Francesco Italiano 1991-01-01 We consider dynamic algorithms and dynamic data structures for various\ngraph problems, such as edge connectivity, vertex connectivity and\nshortest paths. We first show how to maintain efficiently a solution\nto the shortest-path problem during edge insertions and edge-cost\ndecreases.  We then present the first sublinear-time algorithms known\nfor maintaining the 2-edge-connected components of a general graph and\nthe 2-vertex-connected components of a planar graph during edge and\nvertex insertions and deletions. Finally, we introduce a fast\nalgorithm for maintaining the 3-edge-connected components of a graph\nduring edge insertions only.  The solutions of all the above problems\nshare many common features and combine a variety of graph properties,\nnovel algorithmic techniques and new data structures. (ps) Control in Functional Unification Grammars for Text Generation Michael Elhadad, Jacques Robin 1991-01-01 Standard Functional Unification Grammars (FUGs) provide a structurally\nguided top-down control regime for text generation that is not\nappropriate for handling non-structural and dynamic constraints. We\nintroduce two control tools that we have implemented for FUGs to\naddress these limitations: {\\em bk-class}, a tool to limit search by\nusing a form of dependency-directed backtracking and {\\em external}, a\nco-routine mechanism allowing a FUG to cooperate with dynamic\nconstraint sources.  We show how these tools complement the top-down\nregime of FUGs to enhance lexical choice.\" (ps) MpD:  A Multiprocessor Debugger (M.S. Thesis Krish Ponamgi 1991-01-01 MpD is a multiprocessor C debugger designed for multithreaded\napplications running under the Mach operating system.  MpD is\nbuilt on top of gdb, an existing sequential debugger.  The MpD\nlayer utilizes the modeling languages Data Path Expressions \ndeveloped by Hseush and Kaiser to provide a rich set of commands\nto trace sequential and parallel execution of a program.  Associated\nwith each DPE are actions that allow access to useful trace variables\nand I/O facilities.  DPEs are useful for describing sequential and\nconcurrent patterns of events to be verified during execution.  The \npatterns include conditions such as synchronizations, race conditions,\nand wrongly classified sequential/concurrent behavior.  We show in this\nthesis Data Path Expressions are a viable language for multiprocessor\ndebuggers. (pdf) Rule Chaining in {\\sc Marvel}: Dynamic Binding of Parameters George T.~Heineman, Gail E.~Kaiser, Naser S.~Barghouti, Israel Z.~Ben-Shaul 1991-01-01 {\\sc Marvel} is a rule-based development environment (RBDE) that assists in\nthe development of software projects.  {\\sc Marvel} encapsulates each\nsoftware development activity in a rule that specifies the condition for\ninvoking the activity and its effects on the components of the project\nunder development.  These components are abstracted as objects and stored\nin a persistent object database.  Each rule applies to a specific class of\nobjects, which is specified as the parameter of the rule.  Firing a rule\nentails binding its formal parameter to a specific object. If the rule\nchanges the object in such a way that the conditions of other rules become\nsatisfied, these other rules are automatically fired.  A problem arises in\nthis forward chaining model when the classes of the objects manipulated by\nthe rules are different.  {\\sc Marvel} has to determine which object to\nbind to the parameter of each rule in the chain, based on the object\nmanipulated by the original rule that initiated the chain.  We describe a\nheuristic approach for solving this problem in the current {\\sc Marvel}\nimplementation and introduce an algorithmic approach that does better. (pdf) (ps) Incremental Attribute Evaluation for Multi-User Semantics-Based Editors Josephine Micallef 1991-01-01 This thesis addresses two fundamental problems associated with\nperforming incremental attribute evaluation in multi-user editors\nbased on the attribute grammar formalism: (1) multiple asynchronous\nmodifications of the attributed derivation tree, and (2) segmentation\nof the tree into separate modular units.  Solutions to these problems\nmake it possible to construct semantics-based editors for use by teams\nof programmers developing or maintaining large software systems.\nMulti-user semantics-based editors improve software productivity by\nreducing communication costs and snafus.\n\nThe objectives of an incremental attribute evaluation algorithm for\nmultiple asynchronous changes are that (a) all attributes of the\nderivation tree have correct values when evaluation terminates, and\n(b) the cost of evaluating attributes necessary to reestablish a\ncorrectly attributed derivation tree is minimized.  We present a family\nof algorithms that differ in how they balance the tradeoff between\nalgorithm efficiency and expressiveness of the attribute grammar.\nThis is important because multi-user editors seem a practical basis\nfor many areas of computer-supported cooperative work, not just\nprogramming.  Different application areas may have distinct\ndefinitions of efficiency, and may impose different requirements on\nthe expressiveness of the attribute grammar.  The characteristics of\nthe application domain can then be used to select the most efficient\nstrategy for each particular editor.\n\nTo address the second problem, we define an extension of classical\nattribute grammars that allows the specification of interface\nconsistency checking for programs composed of many modules. Classical\nattribute grammars can specify the static semantics of monolithic programs or\nmodules, but not inter-module semantics; the latter was done in the\npast using @i[ad hoc] techniques.  Extended attribute grammars support \nprogramming-in-the-large constructs found in real programming\nlanguages, including textual inclusion, multiple kinds of modular\nunits and nested modular units.  We discuss attribute evaluation in the\ncontext of programming-in-the-large, particularly the separation of\nconcerns between the local evaluator for each modular unit and the\nglobal evaluator that propagates attribute flows across module\nboundaries. The result is a uniform approach to formal specification\nof both intra-module and inter-module static semantic properties, with\nthe ability to use attribute evaluation algorithms to carry out a\ncomplete static semantic analysis of a multi-module program. (pdf) Execution Autonomy in Distributed Transaction Processing Calton Pu, Avraham Leff 1991-01-01 We study the feasibility of execution autonomy in systems with\nasynchronous transaction processing based on epsilon-serializability\n(ESR).  The abstract correctness criteria defined by ESR are\nimplemented by techniques such as asynchronous divergence control and\nasynchronous consistency restoration.  Concrete application examples\nin a distributed environment, such as banking, are described in order\nto illustrate the advantages of using ESR to support execution\nautonomy. (ps) An Architectural Framework for Object Management Systems Steven S. Popovich 1991-01-01 Much research has been done in the last decade in the closely related\nareas of object-oriented programming languages and databases.  Both\nareas now seem to be working toward a common end, that of an @i(object\nmanagement system), or OMS.  An OMS is constructed similarly to an\nOODB but provides a general purpose concurrent object oriented\nprogramming language as well, sharing the object base with the OODB\nquery facilities.  In this paper, we will define several different\ntypes of object systems (object servers, persistent OOPL's, OODB's and\nOMS's) in terms of their interfaces and capabilities.  We will examine\nthe distinguishing features and general architecture of systems of\neach type in the light of a general model of OMS architecture. (ps) Implementing Activity Structures Process Modeling On Top Of The Marvel Environment Kernel Gail E. Kaiser, Israel Ben-Shaul, Steven S. Popovich 1991-01-01 Our goal was to implement the activity structures model defined by\nSoftware Design \\& Analysis on top of the {\\sc Marvel} environment kernel.\nThis involved further design of the activity structures process\ndefinition language and enaction model as well as translation and\nrun-time support in terms of facilities provided by {\\sc Marvel}.  The\nresult is an elegant declarative control language for multi-user\nsoftware processes, with data and activities defined as classes and\nrules in the previously existing {\\sc Marvel} Strategy Language.\nSemantics-based concurrency control is provided by a combination of\nthe {\\sc Marvel} kernel's lock and transaction managers and the send/receive\nsynchronization primitives of the activity structures model. (ps) Service Interface and Replica Management Algorithm for Mobile File System Clients Carl D. Tait, Dan Duchamp 1991-01-01 Portable computers are now common, a fact that raises the possibility\nthat file service clients might move on a regular basis.  This new\ndevelopment requires re-thinking some features of distributed file\nsystem design.  We argue that existing approaches to file replica\nmanagement would not cope well with the likely behavior of mobile\nclients, and we present our solution: a lazy ``server-based'' update\noperation.  This operation facilitates fast, scalable, and highly\nfault-tolerant implementations of both read and write operations in\nthe usual case.  To cope with the weak semantics of the update\noperation, we propose a new file system service interface that allows\napplications to opt for ``UNIX semantics'' by use of a slower, less\nfault-tolerant read operation. (ps) An Architecture for Dynamic Reconfiguration in a Distributed Object-Based Programming Language Brent Hailpern, Gail E. Kaiser 1991-01-01 Distributed applications ideally allow reconfiguration while the\napplication is running, but changes are usually limited to adding new\nclient and server processes and changing the bindings among such\nprocesses.  In some application domains, such as real-time financial\nservices, it is necessary to support finer grained reconfiguration at\nthe level of entities smaller than processes, but desirable to reduce\nthe overhead associated with ad hoc dynamic storage allocation.  We\npresent a scheme for special cases of fine-grained dynamic\nreconfiguration sufficient for our application domain and show how it\ncan be used for practical changes.  We introduce new language concepts\nto apply this scheme in the context of an object-based programming\nlanguage that supports shared data in a distributed environment. (ps) Real-Time portfolio management and automatic extensions Tushar M. Patel 1991-01-01 Voluminous amounts of rapidly changing data in financial markets\ncreate a challenging problem for portfolio managers attempting to\nexploit such changes to achieve their investment objectives.  The\nSPLENDORS real time portfolio management system, built using the\nPROFIT language, is such a system.\n\nChanges in security prices are monitored in a real-time fashion and\nindividual portfolios can be optimized by taking advantage of these\nchanges within the constraints imposed by the investment philosophy of\nthe investor.\n\nSPLENDORS allows great flexibility in creating portfolio managers for\nthe sophisticated C programmer and considerable ease of programming\nfor the PROFIT programmer. Additionally, the non-programming financial\nanalyst may also create portfolio managers from parts of a predefined\nlibrary without any knowledge of programming at all.  Extensions to\nSPLENDORS made by the end user are available immediately without\nbringing the system down for re-compilation. (ps) Navigating the MeldC: The MeldC User's Manual Howard Gershen, Erik Hilsdale 1991-01-01 MeldC is an attempt to create a computer language that is a watershed\nof developments in the fields of object-oriented programming, parallel\nprogramming, and distributed applications. This easy-to-understand\nmanual provides readers with the details of how to write programs in\nMeldC, in addition to plain-English explanations of some of the theory\n(e.g., object/class hierarchies, race condition problems, and\nremote-file access problems) behind MeldC's features. Illustrated. Commented\nsample programs and BNF included. (ps) Marvel 3.0 Administrator's manual Programming System Laboratory 1991-01-01 This manual is intended for system/project administrators of Marvel 3.0.\nIt covers various aspects of managing a Marvel environment, including\ninstallation of the system, administrator built-in commands, and mainly,\nhow to design and write a Marvel environment using Marvel Strategy Language \n(MSL) and Shell Envelope Language (SEL). (pdf) (ps) Marvel 3.0 User's manual Programming System Laboratory 1991-01-01 This manual is intended for Marvel end-users, who want to learn how to use\nMarvel in a specific environment set-up by an administrator. It includes\na tutorial, and explanations on the user-interface, messages, and the various\nbuilt-in commands. (pdf) (ps) Taxonomic Plan Reasoning Premkumar T. Devanbu, Diane J. Litman 1991-01-01 CLASP (ClAssification of Scenarios and Plans) is a knowledge representation\nsystem that extends the notion of subsumption from frame-based languages to\nplans.  The CLASP representation language provides description forming\noperators that specify temporal and conditional relationships between actions\nrepresented in CLASSIC (a current subsumption-based knowledge representation\nlanguage).  CLASP supports subsumption inferences between plan concepts and\nother plan concepts, as well as between plan concepts and plan instances. \nThese inferences support the automatic creation of a plan taxonomy. \nSubsumption in CLASP builds on term subsumption in CLASSIC and illustrates\nhow term subsumption can be exploited to serve special needs.  In particular,\nthe CLASP algorithms for plan subsumption integrate work in automata theory\nwith work in term subsumption.  We are using CLASP to store and retrieve\ninformation about feature specifications and test scripts in the context of a\nlarge software development project. (pdf) (ps) FUF User Manual - Version 5.0 Michael Elhadad 1991-01-01 This report is the user manual for FUF version 5.0, a natural language\ngenerator program that uses the technique of unification grammars. The\nprogram is composed of two main modules: a unifier and a linearizer. The\nunifier takes as input a semantic description of the text to be generated\nand a unification grammar, and produces as output a rich syntactic\ndescription of the text. The linearizer interprets this syntactic\ndescription and produces an English sentence. \n\nThis manual includes a detailed presentation of the technique of\nunification grammars and a reference manual for the current implementation\n(FUF 5.0).  Version 5.0 now includes novel techniques in the unification\nallowing the specification of types and the expression of complete\ninformation.  It also allows for procedural unification and supports\nsophisticated forms of control. (ps) Empirical Studies on the Disambiguation of Cue Phrases Julia Hirschberg, Diane Litman 1991-01-01 Cue phrases are linguistic expressions such as @i(now) and @i(well) that function as\nexplicit indicators of the structure of a discourse.  For example, @i(now) may\nsignal the beginning of a subtopic or a return to a previous topic, while\n@i(well) may mark subsequent material as a response to prior material, or as an\nexplanatory comment.  However, while cue phrases @b(may) convey discourse\nstructure, each also has one or more alternate uses.  While @i(incidentally) may\nbe used @b(SENTENTIALLY) as an adverbial, for example, the @b(DISCOURSE) use\ninitiates a digression.  The question of how speakers and hearers distinguish\nbetween discourse and sentential uses of cue phrases is rarely addressed in\ndiscourse studies.\n\nThis paper reports results of several empirical studies of discourse and\nsentential uses of cue phrases, in which both text-based and prosodic\nfeatures were examined for disambiguating power.  Based on these studies, we\npropose that discourse versus sentential usage may be distinguished by\nintonational features, specifically, @b(PITCH) @b(ACCENT) and @b(INTONATIONAL)\n@b(PHRASING).  We identify a prosodic model that characterizes these\ndistinctions.  We associate this model with features identifiable from text\nanalysis, including orthography and part-of-speech, to permit the application\nof our findings to the generation of appropriate intonational features for\ndiscourse and sentential uses of cue phrases in synthetic speech. (pdf) Parallel Dynamic Programming Z. Galil, K. Park 1991-01-01 We study the parallel computation of dynamic programming.\nThere are five important dynamic programming problems which\nhave wide application, and that have been studied extensively\nin sequential computation.  Two problems among the five have\nfast parallel algorithms; almost no work has been done for\nparallelizing the other three.  We give fast parallel algorithms\nfor four of these problems, including the three.\nWe use two well-known methods as general paradigms for developing\nparallel algorithms.  Combined with various techniques, they\nlead to a number of new results.  At the heart of our new results\nis a processor reduction technique, which enables us to solve\nall four problems with a fewer number of operations than that\nneeded for computing a matrix closure. (ps) Machine Learning in Molecular Biology Sequence Analysis Philip K. Chan 1991-01-01 To investigate how human characteristics are inherited, molecular\nbiologists have been analyzing chemical sequences from DNA, RNA, and\nproteins.  To facilitate this process, sequence analysis knowledge has\nbeen encoded in computer programs.  However, translating human\nknowledge to programs is known to be problematic.  Machine Learning\ntechniques allow these systems to be generated automatically.  This\narticle discusses the application of learning techniques to various\nanalysis tasks.  It is shown that the learned systems constructed to\ndate are often more accurate than human-designed systems.  Moreover,\nlearning can form plausible new hypotheses, which potentially lead to\ndiscovering new knowledge. (ps) A Formal Characterization of Epsilon Serializability Krithi Ramamritham, Calton Pu 1991-01-01 Epsilon Serializability (ESR) is a generalization of classic\nserializability (SR).  ESR allows some limited amount of inconsistency\nin transaction processing (TP), through an interface called\nepsilon-transactions (ETs).  For example, some query ETs may view\ninconsistent data due to non-SR interleaving with concurrent updates.\nIn this paper, we restrict our attention to the situation where\nquery-only ETs run concurrently with {\\em consistent} update\ntransactions that are SR without the ETs.\n\nThis paper presents a {\\em formal characterization} of ESR and ETs.\nUsing the ACTA framework, the first part of this characterization\nformally expresses the inter-transaction conflicts that are recognized\nby ESR and, through that, defines ESR, analogous to the manner in\nwhich conflict-based serializability is defined.  The second part of\nthe paper is devoted to deriving expressions for: (1) the {\\em\ninconsistency} in the values of data -- arising from ongoing updates,\n(2) the inconsistency of the results of a query -- arising from the\ninconsistency of the data read in order to process the query, and (3)\nthe inconsistency exported by an update ET -- arising from ongoing\nqueries reading uncommitted data produced by the update ET.  These\nexpressions are used to determine the preconditions that ET operations\nhave to satisfy in order to maintain the limits on the inconsistency\nin the data read by query ETs, the inconsistency exported by update\nETs, and the inconsistency in the results of queries.  This\ndetermination suggests possible mechanisms that can be used to realize\nESR. (ps) Knowledge Representation and Reasoning with Definitional Taxonomies Robert A. Weida 1991-01-01 We provide a detailed overview of knowledge representation issues in\ngeneral and terminological knowledge representation in particular.\nTerminological knowledge representation, which originated with KL-ONE, \nis an object-centered approach in the tradition of semantic networks and\nframes.  Terminological systems share three distinguishing characteristics:\n(1) They are intended to support the definition of conceptual terms\ncomprising a \"terminology\" and to facilitate reasoning about such terms.\nAs such, they are explicitly distinguished from assertional systems which\nmake statements of fact based on some terminology.  (2) Their concepts are\narranged in a taxonomy so that the attributes of a concept apply to its\ndescendants without exception.  Thus, the proper location of any concept\nwithin the taxonomy can be uniquely determined from the concept's\ndefinition by an automatic process known as classification.  (3) They\nrestrict the expressiveness of their language to achieve relatively\nefficient performance.\n\nWe first survey important general issues in the field of knowledge\nrepresentation, consider the semantics of concepts and their\ninterrelationship, and examine the intertwined notions of taxonomy and\ninheritance.  After discussing classification, we present a number of\nimplemented terminological systems in detail, along with several hybrid\nsystems which couple terminological and assertional reasoning components.\nWe conclude by assessing the current state of the art in terminological\nknowledge representation. (ps) TLex User Manual Steven M. Kearns 1990-01-01 The TLex Language, a regular expression-based language\nfor finding and extracting information from text, is described.  TLex\nfeatures a pattern language that includes intersection, complement,\nand context sensitive operators.  In addition, TLex automatically\ncreates a parse tree from a successful match and offers convenient\nfunctions for manipulating it. (ps) Modeling Safe and Regular Concurrent Processes Wenwey Hseush, Timothy S. Balraj, Gail E. Kaiser 1990-01-01 The authors have previously described the use of data path expressions\nand predecessor  automata  in  debugging concurrent systems.  In  this\npaper we examine  the relationship of these  models to two traditional\nmodels of concurrent processes:  pomset languages and k-safe Petri net\nsystems.  We  explore the  regularity  and   safety of  the concurrent\nlanguages described by each of the four models. Our main result is the\nequivalence   of  regular safe  pomset    languages and  the languages\ndescribed by safe data path expressions, safe predecessor automata and\nk-safe Petri net systems. (ps) Lexical Choice in Natural Language Generation Jacques Robin 1990-01-01 In this paper we survey the issue of lexical choice in natural language\ngeneration. We first define lexical choice as the choice of {\\em open-class}\nlexical items (whether phrasal patterns or individual words) appropriate to\nexpress the content units of the utterance to generate in a given situation\nof enunciation. We then distinguish between five major classes of\nconstraints on lexical choice: grammatical, inter-lexical, encyclopedic,\ninterpersonal and discursive. For each class we review how they have been\nrepresented and used in existing generators. We conclude by pointing out\nthe common limitations of these generators with respect to lexical choice\nas well as some directions for future research in the field. (ps) Testing Object-Oriented Programs by Mutually Suspicious Parties Travis Lee Winfrey 1990-01-01 Testing object-oriented programs has been studied primarily in terms\nof paradigms that apply to all programs, i.e., white-box and black-box\ntesting.  We describe a new testing method for object-oriented\nprograms that specifically exploits encapsulation properties.\nIndividual object classes or even individual instances of objects may\nbe instrumented for testing.  At the heart of the method is the\nsystematic renaming and duplication of object classes. (ps) Porting AIX onto the Student Electronic Notebook John Ioannidis, Gerald Q. Maguire Jr., Israel Ben-Shaul, Marios Levedopoulos, Micky Liu 1990-01-01 We describe the Student Electronic Notebook and the process of porting\nIBM's AIX 1.1 to run on it. We believe that portable\nworkstation-class machines connected by wireless networks and\ndependent on a computational and informational infrastructure raise a\nnumber of important issues in operating systems and distributed\ncomputation (e.g., the partitioning of tasks between workstations and\ninfrastructure), and therefore the development of such machines and\ntheir software is important. We conclude by summarizing our activites,\nitemizing the lessons we learned and identifying the key criteria for\nthe design of the successor machines. (ps) The Coherent Trivial File Transfer Protocol John Ioannidis, Gerald Q. Maguire Jr. 1990-01-01 Coherent TFTP is a protocol that takes advantage of the broadcast\nnature of CSMA networks to speed up simultaneous one-to-many file\ntransfers (e.g., when booting diskless workstations). The CTFTP server\nlisens and services request for entire files or portions thereof.\nCTFTP clients first determine whether the file they are interested in\nis already being transferred, in which case they ``eavesdrop'' and\nload as much of it as they can, or they initiate a new transfer. The\nclients timeout when the server stops transmitting, and if they are\nstill missing parts of the file they request them with a\nblock-transfer request. CTFTP is a back-end protocol; a front end is\nneeded to handle naming and security issues. (ps) A Methodology for Measuring Applications over the Internet Calton Pu, Frederick M. Korz, Robert C. Lehman 1990-01-01 Measuring the performance of applications running over live wide area\nnetworks such as the Internet is difficult.  Many of the important\nvariables (path, network load, etc) cannot be controlled.  Monitoring\nthe entire Internet to follow individual messages is impractical.  We\ndeveloped the Layered Refinement methodology to meet these challenges.\nOur method uses generic software tools to collect data simultaneously\nat important software layers.  Data are analyzed and refined to reduce\nvariance, and correlated to give us a good understanding of each\nlayer.  Sample data spanning seven days illustrate the method, which\nnarrows the confidence interval by an order of magnitude to show the\nend-to-end performance of applications that run over Internet. (ps) Automated Sensor Planning and Modeling for Robotic Vision Tasks Kostantinos Tarabanis 1990-01-01 In this paper we present new results on the automatic determination of\nthe loci of camera poses and optical settings that satisfy the machine\nvision task requirements of visibility, field-of-view, depth-of-field\nand resolution for given features of interest.  It is important to\ndetermine the entire admissible domain of sensor locations,\norientations, and settings for each task constraint, so that these\ncomponent results can be combined in order to find globally admissible\nsensor parameter values.  This work is part of more extensive research\nthat we are pursuing, as part of our \"MVP\" (Machine Vision Planner)\nsystem, on the problem of sensor planning for satisfaction of several\nge neric machine vision requirements.  When designing a vision system\nthat will satisfy the requirements of the machine vision task at hand,\nit is necessary to properly select the specifications of the image\nsensor (e.g.  pixel size), as well as decide the image sensor\nplacement and settings. We present techniques to determine the latter\ntwo using a synthesis approach to the problem.  This approach provides\nsignificant advantages over the generate-and-test techniques currently\nemployed, in which, sensor configurations are generated and then\ntested for satisfaction of the task criteria.  Compared to the\nprevious papers we published on \"MVP\", the new results presented in\nthis paper include techniques and implementation for determining the\nconstraint satisfaction loci when all degrees of freedom of camera\nplacement are taken into account; obtaining globally feasible sensor\nconfigurations by posing the constraint satisfaction problem in an\noptimization setting; analyzing and formulating a new constraint, that\nof depth-of-field, for the determination of sensor placement and\nsetting that satisfy this constraint; providing a more efficient\napproach to the visibility constraint; plus presenting the new test\nresults of the above mentioned techniques.  Camera placement\nexperiments are shown that demonstrate the method in an actual robotic\nsetup for a general three-dimensional viewing configuration.  A camera\nmounted on a robotic arm is placed and set according to the results of\nthe new technique and camera views are taken to verify that the\nfeatures of interest are visible, within the camera field-of-view and\ndepth-of-field, and resolvable to the given specification.  Results of\nthis research will help automate the vision system design process,\nassist in programming the vision system itself and lead to intelligent\nautomated robot imaging systems. (ps) Scaling Up Rule-Based Development Environments Naser S. Barghouti, Gail E. Kaiser 1990-01-01 Rule-based development environments (RBDEs) model the software\ndevelopment process in terms of rules that encapsulate development\nactivities, and execute forward and backward chaining on the rules to\nprovide assistance in carrying out the development process. We\ninvestigate the scaling up of RBDEs along two dimensions.  The first\ndimension covers the nature of the assistance provided by the RBDE,\nand the second spans the functionality.  There is a spectrum of\nassistance models implemented by RBDEs.  We consider three models:\npure automation, strict consistency preservation, and a maximalist\nassistance model that integrates consistency and automation.  The\nchoice of assistance model impacts the functionality of the RBDE, more\nprecisely the solutions to three problems that arise in scaling up\nRBDEs: (1) multiple views; (2) evolution; and (3) concurrency control.\nWe explore the solutions to the three problems with respect to each of\nthe three assistance models.  Throughout the paper, we use the Marvel\nRBDE as an example. (ps) An Information Retrieval Approach for Automatically Constructing Software Libraries Yoelle S. Maarek, Daniel M. Berry, Gail E. Kaiser 1990-01-01 Although software reuse presents clear advantages for programmer\nproductivity and code reliability, it is not practiced enough.  One of\nthe reasons for the only moderate success of reuse is the lack of\nsoftware libraries that facilitate the actual locating and\nunderstanding of reusable components.  This paper describes a technology\nfor automatically assembling large software libraries that\npromote software reuse by helping the user locate the components\nclosest to her/his needs.\n\nSoftware libraries are automatically assembled from a set of\nunorganized components by using information retrieval techniques.\nThe construction of the library is done in two steps.  First,\nattributes are automatically extracted from natural language\ndocumentation by using a new indexing scheme based on the notions of\nlexical affinities and quantity of information.  Then, a hierarchy for\nbrowsing is automatically generated using a clustering technique that\ndraws only on the information provided by the attributes.  Thanks to\nthe free-text indexing scheme, tools following this  approach can\naccept free-style natural  language queries.\n\nThis technology has been implemented in the {\\sc Guru} system, which has\nbeen applied to construct an organized library of {\\sc Aix} utilities.\n An experiment was  conducted  in order to evaluate the retrieval\neffectiveness of\n{\\sc Guru} as compared to {\\sc InfoExplorer} a hypertext library\nsystem for {\\sc Aix} 3 on the IBM RISC System/6000 series. We followed the\nusual evaluation procedure used in information retrieval, based upon\nrecall and precision measures, and determined that our system\nperforms 15\\% better on a random test set,  while being much less\nexpensive to build than {\\sc InfoExplorer (pdf) Detection and Exploitation of File Working Sets Carl Tait, Dan Duchamp 1990-01-01 The work habits of most individuals yield file access patterns that\nare quite pronounced and can be regarded as defining working sets of\nfiles used for particular applications.  This paper describes a\nclient-side cache management technique for detecting these patterns\nand then exploiting them to successfully prefetch files from servers.\nTrace-driven simulations show the technique substantially increases\nthe hit rate of a client file cache in an environment in which a\nclient workstation is dedicated to a single user.  Successful file\nprefetching carries three major advantages: (1) applications run\nfaster, (2) there is less ``burst'' load placed on the network, and\n(3) properly-loaded client caches can better survive network outages.\nOur technique requires little extra code, and DASH because it is\nsimply an augmentation of the standard LRU client cache management\nalgorithm DASH is easily incorporated into existing software. (ps) An Object-Based Approach to Implementing Distributed Concurrency Control Steven S. Popovich, Gail E. Kaiser, S.F. Wu 1990-01-01 We have added distributed concurrency control to the MELD object\nsystem by representing in-progress transactions as simulated objects.\n{\\em Transaction objects} exploit MELD's normal message passing\nfacilities to support the concurrency control mechanism.  We have\ncompleted the implementation of an optimistic mechanism using\ntransaction objects and have designed a two-phase locking mechanism\nbased on the same paradigm.  We discuss the tradeoffs made and lessons\nlearned, dealing both with transactions {\\em on} objects and with\ntransactions {\\em as} objects. (ps) Applications of Epsilon-Serializability in Federated Databases Calton Pu, Avraham Leff 1990-01-01 We apply the concept of {\\em epsilon-serializability} (ESR) to data\nreplication.  ESR converges to global serializability when the\nreplicated system reachs a quiescent state, i.e., all the inter-site\nmessages arrive their destination.  ESR integrates many previous\nproposals for increasing availability for replicated data and bring\nnew insights. (pdf) (ps) Epsilon-Serializability Calton Pu, Avraham Leff 1990-01-01 We introduce and formalize the concept of espsilon-serializability\n(ESR).  ESR converges to serializability (SR) when the database reachs\na quiescent state, i.e., all the update messages are processed.\nDivergence control methods make ESR practical by restricting the\ntemporary inconsistency in the database.  ESR is useful since it\nallows more concurrency than SR and can be integrated with SR. (ps) PIP-1: A Personal Information Portal with wireless access to an information infrastructure John Ioannidis, Gerald Q. Maguire Jr. 1990-01-01 We discuss our ideas for the Personal Information Portal, its\nhardware and software platform, our visions for its use and its impact\non the student and the professional community. (ps) A distributed algorithm for adaptive replication of data Ouri Wolfson 1990-01-01 We present a distributed algorithm for replication of a data-item in a\nset of processors interconnected by a tree network.  The algorithm is\nadaptive in the sense that the replication scheme of the item (i.e.\nthe set of processors, each of which stores a replica of the\ndata-item), changes as the read-write pattern of the processors in the\nnetwork changes.  The algorithm is optimal in the sense that when the\nreplication scheme stabilizes, the total number of messages required\nfor the reads and writes is minimal.  The algorithm is extremely\nsimple. (pdf) Incremental Evaluation of Rules and its Relationship to Parallelism Ouri Wolfson, Hasanat Dewan, Salvatore Stolfo, Yechiam Yemini 1990-01-01 Rule interpreters usually start with an initial database and perform\nthe inference procedure in cycles, ending with a final database.\nIn a real time environment it is possible to receive updates to the\ninitial database after the inference procedure has started or even after it \nhas ended.  We present an algorithm for incremental maintenance of the \ninference database in the presence of such updates.\nInterestingly, the same algorithm is useful for parallel and distributed\nrule processing. When the processors evaluating a program operate\nasynchronously, then they may have different {\\em views} of the database.\nThe incremental maintenance procedure we present can be used to\nsynchronize these views. (pdf) Active databases for communication network management Ouri Wolfson, Soumitra Sengupta, Yechiam Yemini 1990-01-01 This paper has two purposes.  First is to propose new database\nlanguage features for systems used in real time management.  These\nfeatures enable the specification of change-traces, events and\ncorrelation among events, and they do so in a declarative set-oriented\nfashion.  Second is to introduce network management as an important\nand interesting application of active distributed databases. (pdf) Getting and Keeping the Center of Attention Rebecca J. Passonneau 1990-01-01 The present work investigates the contrastive discourse functions of a\ndefinite and a demonstrative pronoun in similar contexts of use.  It\nthus provides an opportunity to examine the separate contributions to\nattentional state [15] of two linguistic features---definiteness and\ndemonstrativity---independently of pronominalization per se.  The two\npronouns, \"it\" and \"that\", have clearly contrastive contexts of use,\nexplained here in terms of distinct pragmatic functions.  Certain uses\nof \"it\" are claimed to perform a distinctive cohesive function, namely\nto establish a \"local center\" (which modifies rather than replaces\nthe notion of a center).  The crucial distinction between a local\ncenter and the Cb of the centering framework (cf. [38] [13] [14] [21])\nis that there is only a single potential local center rather than an\nordered set of Cfs.  The local center is argued to constitute a\nreference point in the model of the speech situation in a manner\nanalogous to 1st and 2nd person pronouns.  In contrast, a deictic\nfunction is posited for apparently anaphoric uses of \"that\" whereby\nthe attentional status of a discourse entity is changed, or a new\ndiscourse entity is constructed based on non-referential constituents\nof the linguistic structure.  Since it is impossible to observe\nattentional processes directly, I present an empirical method for\ninvestigating discourse coherence relations. I analyze statistically\nsignificant distributional models in terms of three types of\ntransitions in the cognitive states of conversational participants:\nexpected transitions, unexpected transitions, and transitions with no\nrelevant effect. (ps) Complexity of Mortgage Pool Allocation Salvatore J. Stolfo ,  Xiangdong Yu 1990-01-01 A new class of NP-hard problems are proved and this result is applied\nto the complexity analysis of the mortgage pool allocation problem in\nAmerican banks.  A boundary on pool sizes is shown to sharply\ndistinguish hard instances from easy ones in this kind of\ncombinational optimazition problems. (ps) Dynamic Programming with Convexity, Concavity, and Sparsity Zvi Galil, Kunsoo Park 1990-01-01 Dynamic programming is a general problem-solving technique that has\nbeen widely used in various fields such as control theory, operations\nresearch, biology, and computer science.  In many applications dynamic\nprogramming problems satisfy additional conditions of convexity,\nconcavity, and sparsity.  This paper presents a classification of\ndynamic programming problems and surveys efficient algorithms based on\nthe three conditions. (ps) Inversion of Toeplitz Matrices with only Two Standard Equations George Labahn, Tamir Shalom 1990-01-01 It is shown that the invertibility of a Toeplitz matrix can be\ndetermined through the solvability of two standard equations.  The\ninverse matrix is represented by two of its columns, (which are the\nsolutions of the two standard equations) and the entries of the\noriginal Toeplitz matrix. (ps) Incremental Algorithms for Minimal Length Paths Giorgio Ausiello, Giuseppe F. Italiano, Alberto Marchetti Spaccamela, Umberto Nanni 1990-01-01 We consider the problem of maintaining on-line a solution to the All\nPairs Shortest Paths Problem in a directed graph $G=(V,E)$ where edges\nmay be dynamically inserted or have their cost decreased.  For the\ncase of integer edge costs in a given range $[1\\ldots C]$, we\nintroduce a new data structure which is able to answer queries\nconcerning the length of the shortest path between any two vertices in\nconstant time, and to trace out the shortest path between any two\nvertices in time linear in the number of edges reported.  The total\ntime required to maintain the data structure under a sequence of at\nmost $O(n^2)$ edge insertions and at most $O(Cn^2)$ edge cost\ndecreases is $O(Cn^3\\log (nC))$ in the worst case, where $n$ is the\ntotal number of vertices in $G$.  For the case of unit edge costs, the\ntotal time required to maintain the data structure under a sequence of\nat most $O(n^2)$ insertions of edges becomes $O(n^3\\log n)$ in the\nworst case.  Both data structures can be adapted to solve the problem\nof maintaining on-line maximal length paths in directed acyclic\ngraphs. All our algorithms improve on previously known algorithms and\nare only a logarithmic factor away from the best possible bounds. (ps) onstraint-based Text Generation: Using local constraints and argumentation to generate a turn in conversation Michael Elhadad 1990-01-01 This report  describes work on the problem  of generating a turn  within an\nongoing  conversation.     Three main points   are advanced:  (1)  Previous\ndiscourse determines the form and content of a new turn; this work proposes\ntechniques to take the influence  of  previous  discourse into account when\ngenerating text. (2) The connection  between previous discourse and  a  new\nturn can be described  as the interaction  between five  local constraints;\nlocal  constraints are   relations between one   discourse segment  of  the\nprevious discourse and the new turn.  This multi-dimensional description of\ndiscursive  relations  allows a   flexible definition of complex rhetorical\nrelations.  (3) Argumentation is one  class of local constraints  which has\nimportant  effects  on the form   of the language  produced   and interacts\nclosely with the other types of local constraints used in this work.\n\nThe problem of generating a turn is then posed as  the problem of finding a\nset of local  constraints  compatible with each  other,  with the  previous\ndiscourse,  and with the information  to  convey.   The  application of the\nlocal constraints  determines  the value of pragmatic  features of  the new\nturn.  A  sophisticated grammar   can then translate these  decisions  into\nappropriate lexical and syntactic decisions.\n\nThe approach is applied to the implementation  of an explanation module for\nthe {\\sc ADVISOR} expert system.  The  implementation is based on an extended\nform  of functional unification; this  extended formalism  is a good choice\nfor handling the problem of constraint interaction. (ps) An Object-Oriented Approach to Content Planning for Text Generation Ursula Wolz 1990-01-01 This paper describes Genie, an object-oriented architecture that\ngenerates text with the intent of extending user expertise in\ninteractive environments. Instead of generating text based solely on\neither discourse goals, intentions, or the domain, we found a\nneed to combine techniques from each.  We have developed an object-\noriented architecture in which the concepts about which we talk\n(domain entities), the goals that may be accomplished with them\n(intentions), and the rhetorical acts through which we express them\n(discourse goals) are represented as objects with localized knowledge\nand methods. A three stage process of content selection, content\nstructuring and content filtering is presented. Content selection and\nstructuring allow us to produce text that is within the context of the\ntask at hand for the user.  Content filtering allows us to revise and\nrestructure the utterance to achieve clarity and conciseness. (ps) Decomposability and Its Role in Parallel Logic-Program Evaluation Ouri Wolfson, Avi Silberschatz 1990-01-01 This paper is concerned with the issue of parallel evaluation of logic programs.  We define the concept of ``program decomposability'', which means that the load of evaluation can be partitioned among a number of\nprocessors, without a need for communication among them.  This in turn\nresults in a very significant speed-up of the evaluation process.\nSome programs are decomposable, whereas others are not.  We completely\nsyntactically characterize three classes of single rule programs with\nrespect to decomposability: nonrecursive, simple linear, and simple\nchain programs.  We also establish two sufficient conditions for\ndecomposability. (pdf) Network Management with Consistently Managed Objects Shyhtsun F. Wu, Gail E. Kaiser 1990-01-01 A {\\it consistency constraint} exists between two objects in a network {\\it management information base (MIB)} if a change in value of one object will cause a change in another.  Based on the definition of {\\it common\nknowledge} by Halpern, Fagin, and Moses, such constraints can\nbe classified by the time needed to maintain them and the probability\nof correctly maintaining them. In this paper, we study the practical\nissues of how to build an MIB with a set of\nobjects and consistency constraints. We will go through an experimental MIB\nexample showing that distributed network entities can be consistently\nmanaged in an elegant way, even when the network environment is very dynamic. (ps) Redundancy Management in a Symmetric Main Memory Database Calton Pu, Avraham Leff, Frederick Korz, Shu-Wie Chen 1990-01-01 We describe the architecture of a symmetric distributed main memory database.  The high performance networks in many large distributed systems enable a machine to reach the main memory of other nodes faster than local disks.  Thus we introduce {\\em remote memory} as an additional layer in the memory hierarchy between local memory anddisks.  Exploiting the remote memory (every node's cache) improves performance and increases availability.  We have created a simulation program that shows this significant advantage over a wide range of\ncache sizes and compares the effects of several object replacement policies in the symmetric distributed main memory database. (ps) WHY A SINGLE PARALLELIZATION STRATEGY IS NOT ENOUGH IN KNOWLEDGE BASES Simona Rabinovici Cohen, Ouri Wolfson 1990-01-01 We address the problem of parallelizing the evaluation of logic programs in data intensive applications.  We argue that the appropriate parallelization strategy for logic-program evaluation depends on the program being evaluated. Therefore, this paper is concerned with the issues of program-classification, and parallelization-strategies.  We propose several parallelization strategies based on the concept of data-reduction -- the original logic-program is evaluated by several processors working in parallel, each using only a subset of the database.  The strategies differ on the evaluation cost, the overhead of communication and synchronization among processors, and the programs to which they are applicable.  In\nparticular, we start our study with pure-parallelization, i.e., parallelization without overhead. An interesting class-structure of logic programs is demonstrated, when considering amenability to pure-parallelization.  The relationship to the NC complexity class is demonstrated.  Then we propose strategies that do incur an overhead,but are optimal in a sense that will be precisely defined. \nThis paper makes the initial steps towards a theory of parallel logic-programming. (pdf) A Comparison of Cache Performance in Server-Based and Symmetric Database Architectures Avraham Leff, Calton Pu, Frederick Korz 1990-01-01 We study the cache performance in a symmetric distributed main-memory\ndatabase.  The high performance networks in many large distributed\nsystems enable a machine to reach the main memory of other nodes more\nquickly than the time to access local disks.  We therefore introduce\n{\\em remote memory} as an additional layer in the memory hierarchy\nbetween local memory and disks.  In order to appreciate the tradeoffs\nof memory and cpu in the symmetric architecture, we compare system\nperformance in alternative architectures.  Simulations show that, by\nexploiting remote memory (in each node's cache), performance improves\nover a wide range of cache sizes as compared to a distributed\nclient/server architecture.  We also compare the symmetric model to a\ncentralized-server model and parameterize the performance tradeoffs. (ps) An Object-Oriented Framework for Modeling Cooperation in Multi-Agent Rule-Based Development Environments N. S. Barghouti, G. E. Kaiser 1990-01-01 Rule-Based Development Environments (RBDEs) exploit expert systems via\nrule-based modeling of the software development process.  RBDEs store\nthe software artifacts in a project database, and define each software\ndevelopment activity that manipulates these artifacts as a rule.\nOpportunistic forward and backward chaining on the rules automates\nsome of chores that developers would have otherwise done manually and\nensures consistency in the database.  Existing RBDEs do not scale up\nto real software development projects involving large teams of\ndevelopers, in part because the current expert system paradigm does\nnot handle multiple agents sharing the working memory of the rule\nsystem. In this article, we investigate the scaling\nup of RBDEs through the use of an object-oriented framework that\nintegrates the structural and behavioral aspects of data modeling and\nrule-based process modeling.  We use this framework to support both\nconsistency maintenance and cooperation in multi-agent RBDEs.\n}} (ps) A Formalization and Implementation of Topological Visual Navigation in Two Dimensions John R. Kender, Il-Pyung Park, David Yang 1990-01-01 In this paper we formalize and implement a model of topological\nvisual navigation in two-dimensional spaces.  Unlike much of\ntraditional quantitative visual navigation, the emphasis throughout\nis on the methods and the efficiency of qualitative visual\ndescriptions of objects and environments, and on the methods and the\nefficiency of direction-giving by means of visual landmarks.  We\nformalize three domains--the world itself, the map-maker's view of\nit, and the navigator's experience of it--and the concepts of custom\nmaps and landmarks.  We specify, for a simplified navigator (the\n``level helicopter'') the several ways in which visual landmarks\ncan be chosen, depending on which of several costs (sensor, distance,\nor communication) should be minimized.  We show that paths minimizing\none measure can make others arbitrarily complex; the algorithm for\nselecting the path is based on a form of Dijkstra's algorithm, and\ntherefore automatically generates intelligent navigator overshooting and\nbacktracking.  We implement, using an arm-held camera, such a\nnavigator, and detail its basic seek-and-adjust behaviors as it\nfollows visual highways (or departs from them) to reach a goal.\nSeeking is based on topology, and adjusting is based on symmetry; there are\nessentially no quantitative measures.  We describe under what\ncircumstances its environment is visually difficult and perceptively\nshadowed, and describe how errors in path-following impact landmark\nselection.  Since visual landmark selection and direction-giving are\nin general NP-complete, and rely on the nearly intractable concept of\ncharacteristic views, we suggest some heuristics; one is that the\nlandmark object ``itself'', rather than its views, may be its most\ncompact encoding.  We conclude with speculations about the feasibility\nof intelligent navigation in very large self-occluding visual worlds. (ps) Parallel Least-Squares Solution of General and Toeplitz-like Linear Systems Victor Pan 1990-01-01 We use $O(\\log^2n)$ parallel arithmetic steps and $n^2$ processors to\ncompute the least-squares solution ${\\bf x} = A^+{\\bf b}$ to a linear\nsystem of equations, $A {\\bf x} = {\\bf b}$, given a $g\\times h$ matrix\n$A$ and a vector ${\\bf b}$, both filled with integers or with rational\nnumbers, provided that $g+h\\le 2n$ and that $A$ is given with its\ndisplacement generator of length $r = O(1)$ and thus has displacement\nrank $O(1)$.  For a vector ${\\bf b}$ and for a general $p\\times q$\nmatrix $A$ (with $p + q \\le n$), we compute $A^+$ and $A^+ {\\bf b}$ by\nusing $O(\\log^2 n)$ parallel arithmetic steps and $n^{2.851}$\nprocessors, and we may also compute $A^+{\\bf b}$ by using\nO$(n^{2.376})$ arithmetic operations. (ps) On the evaluation of the eigenvalues of a banded Toeplitz block matrix Dario Bini, Victor Pan 1990-01-01 Let $A=(a_{ij})$ be an $n\\times n$ Toeplitz matrix with bandwidth\n$k+1,\\ k=r+s$, that is, $a_{ij}=a_{j-i},\\ i,j=1,\\ldots ,n,\\ a_i=0\\\n\\hbox{ if} \\ i>s\\ \\hbox{ and if}\\ i<-r$. We compute $p(\\lambda)=\\det\n(A-\\lambda I)$, as well as $p(\\lambda)/p'(\\lambda)$, where\n$p'(\\lambda)$ is the first derivative of $p(\\lambda)$, by using\n$O(k\\log k\\log n)$ arithmetic operations. Moreover, if $a_i$ are\n$m\\times m$ matrices, so that $A$ is a banded Toeplitz block matrix,\nthen we compute $p(\\lambda)$, as well as $p(\\lambda)/p'(\\lambda)$, by\nusing $O(m^3k(\\log^2k+\\log n)+ m^2k\\log k\\log n)$ arithmetic\noperations. The algorithms can be extended to the computation of\n$\\det(A-\\lambda B)$ and of its first derivative, where both $A$ and\n$B$ are banded Toeplitz matrices. The algorithms may be used as a\nbasis for iterative solution of the eigenvalue problem for the matrix\n$A$ and of the generalized eigenvalue problem for $A$ and $B$. (ps) Computing matrix eigenvalues and polynomial zeros  where the output is real Dario Bini, Victor Pan 1990-01-01 Surprisingly simple corollaries from the Courant-Fischer minimax\ncharacterization theorem enable us to devise a very effective\nalgorithm for the evaluation of a set $ S$ interleaving the set $E$ of\nthe eigenvalues of a real symmetric tridiagonal matrix $T_n$ (as well\nas a point that splits $E$ into two subsets of comparable\ncardinality). This is similar to the more complicated algorithms of\n[BOT] and [BFKT] that compute such interleaving sets and splitting\npoints for the set of the zeros of a polynomial $p(\\lambda)$ having\nonly real zeros. As a result, the record upper estimates for the\nparallel and sequential complexity of approximating to all the zeros\nof such a polynomial are obtained in an alternate way and, moreover,\nare extended to the eigenvalue computation for $T_n$. Furthermore, we\ndramatically decreased the previous record upper estimates for the\nparallel complexity of the latter problem, to the level within\npolylogarithmic factors from the straightforward lower bounds. (ps) On a Recursive Triangular Factorization of Matrices Victor Pan 1990-01-01 We estimate the parallel and sequential complexity of computing a recursive triangular factorization of an $n\\times n$ matrix, which may, in particular, replace Choleski factorization as a means of computing the QR-factorization of a matrix and may also have some other applications.  The estimates for the storage space, sequential time and the number of processors required for the computation and storage of such a recursive factorization is by almost the factor of $n$ less than the similar estimates for the Choleski factorization in the case of Toeplitz-like and Hankel-like matrices. (ps) A Framework for Immibrating Existing Software into New Software Development Environements Michael H. Sokolsky, Gail E. Kaiser 1990-01-01 We have investigated the problem of immigrating software\nartifacts from one software development environment (SDE) to another\nfor the purpose of upgrading to new SDEs as technology improves, while\ncontinuing development or maintenance of existing software systems.\nWe first taxonomize the larger problem of data migration, to\nestablish the scope of immigration.  We then classify SDEs in terms of\nthe ease of immigrating software artifacts out of the data repository\nof the source SDE without knowledge of its internal representation.  A\nframework is presented for constructing automatic immigration tools as\nutilities provided by destination SDEs. We describe a specific\nimmigration tool, called Marvelizer, that we have implemented as\npart of the Marvel SDE and discuss our experience using the tool. (ps) Complexity of Computations with Matrices and Polynomials Victor Pan 1990-01-01 We review the complexity of polynomial and matrix computations, as well as their various correlations to each other and some major techniques for the design of algebraic and numerical algorithms. (ps) Parametrization of Newton's Iteration for Computations\nwith Structured Matrices and Applications Victor Pan 1990-01-01 We apply a new parametrized version of Newton's\niteration in order to compute (over any field $F$ of constants) the\nsolution or a least-squares solution to a linear system $Bx = v$ with\nan $n x n$ Toeplitz or Topelitz-like matrix $B$, as well as the\ndeterminant of $B$ and the coefficients of its characteristic\npolynomial, $det(\\lambda I-B)$, dramatically improving the processor\nefficiency of the know fast parallel algorithms.  Our algorithms,\ntogether with some previously know and some resect results, as well as\nwith our new techniques for computing polynomial GCDs and LCMs, imply\nrespective improvement of known estimates for parallel arithmetic\ncomplexity of several fundamental computations with polynomials and\nwith both structured and general matrices. (ps) Understanding Data Refinement Using Procedural Refinement Steven M. Kearns 1990-01-01 Data refinement is converting a program that uses one\nset of variables to an equally correct program that uses another set\nof variables, usually of different types.  There have been a number of\nseemingly different mathematical definitions of data refinement.  We\npresent a unifying view of data refinement as a special case of\nprocedural refinement, which is simpler to understand.  All the\ndata refinement theories in the literature are shown to be instances\nof two formulas, but we show that there are actually an infinite number\nof theories.  In addition, we introduce the concepts of nonlocal data\nrefinement, data refinement using semi-invariants, and procedural\noptimization using data refinement. (ps) Real-time visual servoing Peter Allen, Billibon Yoshimi, Aleksandar Timcenko 1990-01-01 This paper describes a new\nreal-time tracking algorithm in conjunction with a predictive filter \nto allow real-time visual servoing of a robotic arm.  The system\nconsists of two calibrated cameras that provide images to a real-time,\npipelined-parallel optic-flow algorithm that can robustly compute\noptic-flow and calculate the 3-D position of a moving object at\napproximately 5 Hz rates.  These 3-D positions serve as input to a\npredictive kinematic control algorithm that uses an $alpha~ - ~beta~ - ~gamma$\nfilter to update the position of a robotic arm tracking the moving\nobject. Experimental results are presented for the\ntracking of a moving model train in a variety of different trajectories. (ps) IMAGE WARPING WITH SPATIAL LOOKUP TABLES George Wolberg 1989-01-01 (pdf) A REGULARIZATION METHOD FOR THE SOLUTION OF THE SHAPE FROM SHADOWS PROBLEM Michael G. Hatzitheodorou, Tomasz Jackowski, Anargyros Papageorgiou 1989-01-01 (pdf) A DYNAMIC DATA STRUCTURE FOR GENERAL SERIES PARALLEL DIGRAPHS G. F. Italiano, A. Marchetti Spaccamela, U. Nanni 1989-01-01 (pdf) DICTIONARIES FOR LANGUAGE GENERATION ACCOUNT FOR CO-OCCURRENCE KNOWLEDGE Franck Smadja 1989-01-01 (pdf) A PROCEDURE FOR THE SELECTION OF CONNECTIVES IN TEXT GENERATION: HOW DEEP IS THE SURFACE? M. Elhadad, K. R. McKeown 1989-01-01 (pdf) EXTENDED FUNCTIONAL UNIFICATION PROGRAMMARS M. Elhadad 1989-01-01 (pdf) A VISUAL LANGUAGE FOR BROWSING, UNDOING AND REDOING GRAPHICAL INTERFACE COMMANDS David Kurlander, Steven Feiner 1989-01-01 (ps) ENERGY-BASED SEGMENTATION OF VERY SPARSE RANGE SURFACES Terrance E. Boult, Mark Lerner 1989-01-01 This paper describes a new segmentation technique for very sparse surfaces which is based on minimizing the energy of the surfaces in the scene.  While it could be used in almost any system as part of surface reconstruction/model recovery, the algorithm is designed to be usable when the depth information is scattered and very sparse, as is generally the case with depth generated by stereo algorithms.  We show results from a sequential algorithm and discuss a working prototype that executes on the massively parallel Connection Machine. The idea of segmentation by energy minimization is not new, however past techniques have relied on discrete regularization or Markov random fields to model the surfaces to build smooth surfaces and detect depth edges.  In addition, both of the aforementioned techniques are ineffective at energy minimization for very sparse data.  The technique presented herein models thesurfaces with reproducing kernel-based splines which can be shown to solve a regularized surface reconstruction problem.  From the functional form of these splines we derive computable upper and lower bounds on the energy of a surface over a given finite region.  The computation of the spline, and the corresponding surface representation are quite efficient for very sparse data. An interesting property of the algorithm is that it makes no attempt to determine segmentation boundaries; the algorithm can be viewed as a classification scheme which segments the data into collections of points which are ``from'' the same surface. (pdf) COST FUNCTION ERROR IN ASYNCHRONOUS PARALLEL SIMULATED ANNEALING ALGORITHMS M. D. Durand 1989-01-01 Reducing synchronization constraints in parallel simulated annealing\nalgorithms can improve performance.  However, this introduces error in\nthe global cost function.  Previous work in parallel simulated\nannealing suggests that if the amount of error in the cost function is\ncontrolled, good quality results can still be obtained.  In this\npaper, we present a model of error in asynchronous parallel simulated\nannealing algorithms to partition graphs and use it to predict the\nbehavior of three different synchronization strategies.  These three\napproaches were implemented on a 20-processor Encore, a shared memory,\nMIMD multiprocessor, and tested on a variety of graphs.\n\nAs predicted, the strategy which allows controlled error yields\nsolutions comparable to those of the serial algorithm with greatly\nimproved running time.  Speedups from 5 to 11 (depending on the\ndensity of the graphs) using 16 processors were obtained.  In\ncontrast, the more synchronized version exhibited unacceptably high\nrunning times, whereas the version characterized by uncontrolled error\nyielded significantly poorer results.  This confirms behavior seen in\nparallel simulated annealing algorithms to perform placement in VLSI\ncircuit layout systems. (pdf) DATA MIGRATION IN AN OBJECT-ORIENTED SOFTWARE DEVELOPMENT ENVIRONMENT Michael Sokolsky 1989-01-01 (pdf) MODULAR SYNCHRONIZATION IN MULTIVERSION DATABASES: VERSION CONTROL AND CONCURRENCY CONTROL Soumitra Sengupta, Divyakant Agrawal 1989-01-01 (pdf) TESTING RELIABLE DISTRIBUTED APPLICATIONS THROUGH SIMULATED EVENTS Travis L. Winfrey, Gail E. Kaiser 1989-01-01 (pdf) {MARVEL} IMPLEMENTATION GUIDE Michael Sokolsky 1989-01-01 (pdf) THE {LLC} PARALLEL LANGUAGE AND ITS IMPLEMENTATION ON {DADO2} Russell C. Mills 1989-01-01 (pdf) A CRITIQUE OF THE {LLC} PARALLEL LANGUAGE AND SOME SOLUTIONS Russell C. Mills 1989-01-01 (pdf) {DADO2} {LLC} USERS' MANUAL Russell C. Mills 1989-01-01 (pdf) {LLC} REFERENCE MANUAL Russell C. Mills 1989-01-01 (pdf) A TUTORIAL INTRODUCTION TO {LLC} Russell C. Mills 1989-01-01 (pdf) ON-LINE MAINTENANCE OF MINIMAL DIRECTED HYPERGRAPHS G. F. Italiano, U. Hanni 1989-01-01 (pdf) EXPLORING ``MULTIPLE WORLDS'' IN PARALLEL Jonathan M. Smith, Gerald Q. Maguire Jr. 1989-01-01 (pdf) SYSTEM SUPPORT FOR ``MULTIPLE WORLDS'' Jonathan M. Smith, Gerald Q. Maguire Jr. 1989-01-01 (pdf) TOWARDS A FRAMEWORK FOR COMPARING OBJECT-ORIENTED SYSTEMS Shyhtsun F. Wu 1989-01-01 (pdf) OBJECT-ORIENTED PROGRAMMING LANGUAGE FACILITIES FOR CONCURRENCY CONTROL Gail E. Kaiser 1989-01-01 (pdf) MELDING MULTIPLE GRANULARITIES OF PARALLELISM Gail E. Kaiser, Wenwey Hseush, Steven S. Popovich, Shyhtsun Felix Wu 1989-01-01 (pdf) CONCURRENT EXECUTION OF MUTUALLY EXCLUSIVE ALTERNATIVES ({Ph.D}. DISSERTATION Jonathan M. Smith 1989-01-01 (pdf) ACQUISITION AND INTERPRETATIONS OF 3-D SENSOR DATA FROM TOUCH Peter K. Allen, Paul Michelman 1989-01-01 (pdf) INCREMENTAL ATTRIBUTE EVALUATION WITH APPLICATIONS TO MULTI-USER LANGUAGE-BASED ENVIRONMENTS Josephine Micallef 1989-01-01 (pdf) A FLEXIBLE TRANSACTION MODEL FOR SOFTWARE ENGINEERING Gail E. Kaiser 1989-01-01 (pdf) EXPERIENCE WITH PROCESS MODELING IN THE {MARVEL} SOFTWARE DEVELOPMENT ENVIRONMENT KERNEL Gail E. Kaiser 1989-01-01 (pdf) MICROCODING THE LEXICON WITH CO-OCCURRENCE KNOWLEDGE LANGUAGE GENERATION, Thesis Proposal, {June} 89 Frank Smadja 1989-01-01 (pdf) INTERACTIVE MULTIMEDIA EXPLANATION FOR EQUIPMENT MAINTENANCE AND REPAIR Steven Feiner, Kathleen R. McKeown 1989-01-01 (pdf) INDUCTIVE LEARNING WITH {BCT} Philip K. Chan 1989-01-01 BCT (Binary Classification Tree) is a system that learns from examples and\r\nrepresents learned concepts as a binary polythetic decision tree.  Polythetic\r\ntrees differ from monothetic decision trees in that a logical combination\r\nof multiple (versus a single) attribute values may label each tree\r\narc.  Statistical evaluations are used to recursively partition the concept\r\nspace in two and expand the tree.  As with standard decision trees,\r\nleaves denote classifications.  Classes are predicted for unseen instances \r\nby traversing appropriate branches in the tree to the leaves.  Empirical\r\nresults demonstrated that BCT is generally more accurate or comparable to \r\ntwo earlier systems. (pdf) (ps) HAPTIC PERCEPTION WITH A ROBOT HAND:  REQUIREMENTS AND REALIZATION Paul Michelman, Peter K. Allen 1989-01-01 (pdf) VALUED REDUNDANCY C. Pu, A. Leff, S.W. Chen, F. Korz, J. Wha 1989-01-01 (pdf) ON THE COMPLEXITY OF CONSTRAINED DISTANCE TRANSFORMS AND DIGITAL DISTANCE MAP UPDATES IN TWO DIMENSIONS Terrence E. Boult 1989-01-01 (pdf) AN OPTIMIZATION TO THE TWO-PHASE COMMITMENT PROTOCOL Daniel J. Duchamp 1989-01-01 (pdf) ANALYSIS OF TRANSACTION MANAGEMENT PERFORMANCE Daniel J. Duchamp 1989-01-01 (pdf) A NON-BLOCKING COMMITMENT PROTOCOL Daniel J. Duchamp 1989-01-01 (pdf) EXPERIENCE WITH A MULTI-THREADED EVENT-DRIVEN PROGRAM Daniel J. Duchamp 1989-01-01 (pdf) AN ABORT PROTOCOL FOR NESTED, DISTRIBUTED TRANSACTIONS Daniel J. Duchamp 1989-01-01 (pdf) MODELING CONCURRENCY IN PARALLEL DEBUGGING Wenwey Hseush, Gail E. Kaiser 1989-01-01 (pdf) THE {MELD} PROGRAMMING LANGUAGE USER MANUAL Bill N. Schilit, Wen-Wey Hseush, Shyhtsun Felix Wu, Steven S. Popovich 1989-01-01 (ps) THE MULTICAST POLICY AND ITS RELATIONSHIP TO REPLICATED DATA PLACEMENT Amir Milo, Ouri Wolfson 1989-01-01 In this paper we consider the communication complexity of maintaining\nthe replicas of a logical data-item, in a database distributed over a\ncomputer network.  We propose a new method, called the minimum\nspanning tree write, by which a processor in the network should\nmulticast a write of a logical data-item, to all the processors that\nstore replicas of the item. Then we show that the minimum spanning\ntree write is optimal from the communication cost point of view.  We\nalso demonstrate that the method by which a write is multicast to all\nthe replicas of a data-item, affects the optimal replication scheme of\nthe item, i.e., at which processors in the network the replicas should\nbe located.  Therefore, next we consider the problem of determining an\noptimal replication scheme for a data item, assuming that each\nprocessor employs the minimum spanning tree write at run-time.  The\nproblem for general networks is shown NP-Complete, but we provide\nefficient algorithms to obtain an optimal allocation scheme for three\ncommon types of network topologies.  They are completely-connected,\ntree, and ring networks.  For these topologies, efficient algorithms\nare also provided for the case in which reliability considerations\ndictate a minimum number of replicas. (pdf) A NEW PARADIGM FOR PARALLEL AND DISTRIBUTED RULE-PROCESSING Ouri Wolfson, Aya Ozeri 1989-01-01 This paper is concerned with the parallel evaluation of\ndatalog rule programs, mainly by processors that are interconnected by\na communication network.  We introduce a paradigm, called\ndata-reduction, for the parallel evaluation of a general datalog\nprogram. Several parallelization strategies discussed previously in\n[CW, GST, W, WS] are special cases of this paradigm.  The paradigm\nparallelizes the evaluation by partitioning among the processors the\ninstantiations of the rules.  After presenting the paradigm, we\ndiscuss the following issues, that we see fundamental for\nparallelization strategies derived from the paradigm: properties of\nthe strategies that enable a reduction in the communication overhead,\ndecomposability, load balancing, and application to programs with\nnegation.  We prove that decomposability, a concept introduced\npreviously in [WS, CW], is undecidable. (pdf) AVERAGE CASE COMPLEXITY OF MULTIVARIATE INTEGRATION H. Wozniakowski 1989-01-01 (pdf) ESTIMATING THE LARGEST EIGENVALUE BY THE POWER AND LANCZOS ALGORITHMS WITH A RANDOM START J. Kuczynski, H. Wozniakowski 1989-01-01 (pdf) FINDING NEW RULES FOR INCOMPLETE THEORIES: INDUCTION WITH EXPLICIT BIASES IN VARYING CONTEXTS Andrea Danyluk 1989-01-01 (pdf) A SURVEY OF MACHINE LEARNING SYSTEMS INTEGRATING EXPLANATION-BASED AND SIMILARITY-BASED METHODS Andrea Danyluk 1989-01-01 (pdf) AN IMPROVED ALGORITHM FOR APPROXIMATE STRING MATCHING Zvi Galil, Kunsoo Park 1989-01-01 (pdf) A LINEAR-TIME ALGORITHM FOR CONCAVE ONE-DIMENSIONAL DYNAMIC PROGRAMMING Zvi Galil, Kunsoo Park 1989-01-01 (pdf) AN OVERVIEW OF THE {Synthesis} OPERATING SYSTEM Calton Pu, Henry Massalin 1989-01-01 (pdf) SPARSE DYNAMIC PROGRAMMING {I}: LINEAR COST FUNCTIONS D. Eppstein, Z. Galil, R. Giancarlo, G. F. Italiano 1989-01-01 (pdf) SPARSE DYNAMIC PROGRAMMING {II}: CONVEX AND CONCAVE COST FUNCTIONS D. Eppstein, Z. Galil, R. Giancarlo, G. F. Italiano 1989-01-01 (pdf) DATA STRUCTURES AND ALGORITHMS FOR DISJOINT SET UNION PROBLEMS Z. Galil, G. F. Italiano 1989-01-01 (pdf) TELEOPERATOR CONTROL OF THE {Utah}/{MIT} DEXTROUS HAND-- A FUNCTIONAL APPROACH Amy Morishima, Thomas H. Speeter 1989-01-01 (ps) MAPPING HAPTIC EXPLORATORY PROCEDURES TO MULTIPLE SHAPE REPRESENTATIONS Peter K. Allen 1989-01-01 (pdf) THE APPLICATION OF MICROECONOMICS TO THE DESIGN OF RESOURCE ALLOCATION AND CONTROL ALGORITHMS - {Ph.D}. Dissertation Donald Francis Ferguson 1989-01-01 (pdf) {PSi}: A SILICON COMPILER FOR VERY FAST PROTOCOL PROCESSING H. Abu-Amara, T. Balraj, T. Barzilai, Y. Yemini 1989-01-01 (pdf) {NEST}:  A NETWORK SIMULATION \\& PROTOTYPING TESTBED A. Dupuy, J. Schwartz, Y. Yemini, D. Bacon 1989-01-01 (pdf) LOGIC LEVEL AND FAULT SIMULATION ON THE {RP3} PARALLEL PROCESSOR Steven Unger 1989-01-01 (pdf) ROBOT ACTIVE TOUCH EXPLORATION: CONSTRAINTS AND STRATEGIES Kenneth S. Roberts 1989-01-01 (pdf) COORDINATING A ROBOT ARM AND MULTI-FINGER HAND Kenneth S. Roberts 1989-01-01 (pdf) ON THE AVERAGE GENUS OF A GRAPH Jonathan L. Gross, E. Ward Klein, Robert G. Rieper 1989-01-01 (pdf) STRATIFIED GRAPHS Jonathan L. Gross, Thomas W. Tucker 1989-01-01 (pdf) GRAPH IMBEDDINGS AND OVERLAP MATRICES -- Preliminary Report Jainer Chen, Jonathan L. Gross 1989-01-01 (pdf) FULL TEXT INDEXING BASED ON LEXICAL RELATIONS -- AN APPLICATION:  SOFTWARE LIBRARIES Yoelle S. Maarek, Frank A. Smadja 1989-01-01 (pdf) LEXICAL CO-OCCURRENCE:  THE MISSING LINK Frank A. Smadja 1989-01-01 (pdf) A CONTRASTIVE EVALUATION OF FUNCTIONAL UNIFICATION GRAMMAR FOR SURFACE LANGUAGE GENERATION:  A CASE STUDY IN CHOICE OF CONNECTIVES Kathleen R. McKeown, Michael Elhadad 1989-01-01 (pdf) RADIO PROJECT USER MODEL (Version 1.0 Matthew Kamerman 1989-01-01 (pdf) {FUF}: THE UNIVERSAL UNIFIER -- USER MANUAL, VERSION 2.0 Michael Elhadad 1989-01-01 (pdf) LEARNING THE PROBABILITY DISTRIBUTION FOR PROBABILISTIC EXPERT SYSTEMS Michelle Baker, Kenneth Roberts 1989-01-01 (pdf) AN ALGORITHM TO RECOVER GENERALIZED CYLINDERS FROM A SINGLE INTENSITY VIEW Ari D. Gross, Terrance E. Boult 1989-01-01 (pdf) AN OPTIMAL $O(\\log \\log n$ TIME PARALLEL STRING MATCHING ALGORITHM Dany Breslauer 1989-01-01 (pdf) IMAGE UNDERSTANDING AND ROBOTICS RESEARCH AT {Columbia University} John R. Kender, Peter K. Allen, Terrance E. Boult 1989-01-01 (pdf) SHAPE FROM TEXTURES: A PARADIGM FOR FUSING MIDDLE LEVEL VISION CUES -- {Ph.D}. Dissertation Mark Laurence Moerdler 1989-01-01 (pdf) USER-DEFINED PREDICATES IN {OPS5}: A NEEDED LANGUAGE EXTENSION FOR FINANCIAL EXPERT SYSTEMS A. J. Pasik, D. P. Miranker, S. J. Stolfo, T. Kresnicka 1989-01-01 (pdf) TECHNIQUES FOR BUILDING HIGHLY AVAILABLE DISTRIBUTED FILE SYSTEMS Carl D. Tait 1989-01-01 This paper analyzes recent research in the field\r\nof distributed file systems, with a particular emphasis on the problem\r\nof high availability.  Several of the techniques involved in building\r\nsuch a system are discussed individually: naming, replication,\r\nmultiple versions, caching, stashing, and logging.  These techniques\r\nrange from extensions of ideas used in centralized file systems,\r\nthrough new notions already in use, to radical ideas that have not yet\r\nbeen implemented.  A number of working and proposed systems are\r\ndescribed in conjunction with the analysis of each technique.  The\r\npaper concludes that a low degree of replication, a liberal use of\r\nclient and server caching, and optimistic behavior in the face of\r\nnetwork partition are all necessary to ensure high availability. (pdf) (ps) {MARVEL} USERS MANUAL, version 2.5 Michael Sokolsky, Mara Cohen 1989-01-01 (pdf) THE COMMUNICATION COMPLEXITY OF ATOMIC COMMITMENT AND OF GOSSIPING Ouri Wolfson 1989-01-01 We consider the problem of atomic commitment of a transaction\nin a distributed database.  This is a variant of the famous gossiping\nproblem (see [HHL] for a survey).  Given a set of communication costs\nbetween pairs of participant sites, we establish that the necessary\ncommunication cost for any atomic commitment algorithm is twice the\ncost of a certain minimum spanning tree.  We also establish the\nnecessary communication time for any atomic commitment algorithm,\ngiven a set of communication delays between pairs of participant\nsites, and the time at which each participant completes its\nsubtransaction.  Then we determine that both lower bounds are also\nupper bounds in the following sense.  There is an efficient (i.e.\npolynomial-time) algorithm that, in the absence of failures, has a\nminimum communication cost.  There is another efficient algorithm\nthat, in the absence of failures, has a minimum communication time.\nHowever, unless P=NP, there is no efficient algorithm which has a\nminimum communication complexity, namely, for which the product of\ncommunication cost and communication time is minimum.  Then we present\na simple, linear time, distributed algorithm, called TREE-COMMIT,\nwhose communication complexity is not worse than $p$ times the minimum\ncomplexity, where $p$ is the number of participants.  Finally, we\ndemonstrate that TREE-COMMIT is superior to the existing variations of\nthe two-phase commit protocol. (pdf) Surface Orientation from Texture Autocorrelation Lisa Gottesfeld Brown 1989-01-01 We report on a refinement of our technique for determining the\norientation of a textured surface from the two-point autocorrelation\nfunction of its image.  We replace our previous assumptions of\nisotropic texture by knowledge of the autocorrelation moment matrix of\nthe texture when viewed head on.  The orientation of a textured\nsurface is then deduced from the effects of foreshortening on these\nautocorrelation moments.  This technique is applied to natural images\nof planar textured surfaces and gives significantly improved results\nwhen applied to anisotropic textures which under the assumption of\nisotropy mimic the effects of projective foreshortening.  The\npotential practicality of this method for higher level image\nunderstanding systems is discussed. (pdf) CAMERA PLACEMENT PLANNING AVOIDING OCCLUSION:  TEST RESULTS USING A ROBOTIC HAND/EYE SYSTEM Kostantinos Tarabanis, Roger Y. Tsai 1989-01-01 Camera placement experiments are presented that demonstrate the\neffectiveness of a viewpoint planning algorithm that avoids occlusion\nof a visual target.  A CCD camera mounted on a robot in a hand-eye\nconfiguration is placed at planned unobstructed viewpoints to observe\na target on a real object.  The validity of the method is tested by\nplacing the camera inside the viewing region, that is constructed\nusing the proposed new sensor placement planning algorithm, and\nobserving whether the target is truly visible. The accuracy of the\nboundary of the constructed viewing region is tested by placing the\ncamera at the critical locations of the viewing region boundary and\nconfirming that the target is barely visible. The corresponding scenes\nfrom the candidate viewpoints are shown demonstrating that occlusions\nare properly avoided. (pdf) MODEL-BASED PLANNING OF SENSOR PLACEMENT AND OPTICAL SETTINGS Kostantinos Tarabanis, Roger Y. Tsai 1989-01-01 We present a model-based vision system that automatically plans the\nplacement and optical settings of vision sensors in order to meet\ncertain generic task requirements common to most industrial machine\nvision applications. From the planned viewpoints, features of interest\non an object will satisfy particular constraints in the image. In this\nwork, the vision sensor is a CCD camera equipped with a programmable\nlens (i.e. zoom lens) and the image constraints considered are:\nvisibility, resolution and field of view. The proposed approach uses a\ngeometric model of the object as well as a model of the sensor, in\norder to reason about the task and the environment. The sensor\nplanning system then computes the regions in space as well as the\noptical settings that satisfy each of the constraints separately.\nThese results are finally combined to generate acceptable viewing\nlocations and optical settings satisfying all constraints\nsimultaneously.  Camera planning experiments are described in which a\nrobot-arm positions the camera at a computed location and the planned\noptical settings are set automatically. The corresponding scenes from\nthe candidate viewpoints are shown demonstrating that the constraints\nare indeed satisfied. Other constraints, such as depth of focus, as\nwell as other vision sensors can also be considered resulting in a\nfully integrated sensor planning system. (pdf) EXTENDING THE {\\em Mercury} SYSTEM TO SUPPORT TEAMS OF {Ada} PROGRAMMERS Josephine Micallef, Gail E. Kaiser 1989-01-01 (pdf) SYNCHRONIZATION OF MULTIPLE AGENTS IN RULE-BASED DEVELOPMENT ENVIRONMENTS -- Thesis Proposal Naser S. Barghouti 1989-01-01 The Rule-Based Development Environment (RBDE)  is a recently-developed\napproach for providing intelligent assistance to developers working on\na large-scale  software project.  RBDEs  model the development process\nin terms of rules,  and then enact this  model by automatically firing\nrules at  the  appropriate  time.  The RBDE  approach has been used to\ndevelop single-user environments, but support  for multiple developers\ncooperating on the same project is still not  available because of the\nlack  of mechanisms  that can synchronize    the  efforts of  multiple\ndevelopers, who concurrently  select commands, causing  the  firing of\nmultiple rules  (either directly  or  via chaining)  that concurrently\naccess shared data.  Conflicts  between different rules and concurrent\naccess  to shared data may cause  the  violation of consistency in the\nproject  database,  and  thus    necessitate the  synchronization   of\nconcurrent  activities.   The conjecture of  this  proposal is that an\nRBDE can provide the  required synchronization  if it is provided with\nknowledge about what it means for the data of a specific project to be\nin  a consistent state,  and about the semantics  of   operations that\ndevelopers perform on the data.  The research that this paper proposes\nwill formulate  a framework for  specifying consistency of data  in an\nRBDE,  and formulate  a mechanism  for  synchronizing the  actions  of\nconcurrent rules fired on behalf of multiple developers cooperating on\na common or different tasks. (pdf) PARALLEL BOTTOM-UP EVALUATION OF {Datalog} PROGRAMS BY LOAD SHARING Ouri Wolfson 1989-01-01 We propose a method of syntactically parallelizing\nbottom-up-evaluation of logic programs.  The method is distinguished\nby the fact that it is .i pure i.e., does not require interprocessor\ncommunication, or synchronization overhead.  The method cannot be used\nto parallelize every logic program, but we syntactically characterize\nseveral classes of logic programs that are $sharable$, i.e. programs\nto which the method can be applied (e.g., the class of linear single\nrule programs).  We also provide a characterization of a class of\n$nonsharable$ programs, and demonstrate that sharability is a\nfundamental semantic property of the program, that is independent of\nthe parallelization method proposed in this paper. (pdf) IMAGE-FLOW COMPUTATION: ESTIMATION-THEORETIC FRAMEWORK, UNIFICATION AND INTEGRATION, THESIS PROPOSAL Ajit Singh 1989-01-01 Visual-motion is a major source of three-dimensional information.  It\nis commonly recovered from time-varying imagery in the form of a\ntwo-dimensional image-flow field.  This thesis is about computation of\nimage-flow.\n\nA new framework is proposed that classifies the image-flow information\navailable in time-varying imagery into two categories - conservation\ninformation and neighborhood information.  Each type of information is\nrecovered in the form of an estimate accompanied by a\ncovariance-matrix.  Image-flow is then computed by fusing the two\nestimates using estimation-theoretic techniques. This framework is\nshown to allow estimation of certain types of discontinuous\nflow-fields without any a-priori knowledge about the location of\ndiscontinuities.  An algorithm based on this framework is described.\nResults of applying this algorithm to a variety of image-sequences are\nalso discussed.\n\nThe new framework is shown to be applicable identically to each one of\nthe three major approaches for recovering conservation information,\ni.e., gradient-based approach, correlation-based approach and\nspatiotemporal energy-based approach. The formulation of neighborhood\ninformation used in this framework is also shown to reduce to some of\nthe existing smoothing-based formulations under various simplifying\nassumptions. In essence, the framework described in this thesis\nunifies various existing approaches for image-flow computation. Such\nunification is shown to be useful in analyzing various existing\nframeworks as well as in generating new frameworks.\n\nThe new framework is also shown to serve as a platform to integrate\nthe three approaches mentioned above. It is observed that the\nmeasurements obtained by the three approaches have different\nerror-characteristics. This situation is regarded analogous to the\nmulti-sensor fusion problem, where the algorithms based on the three\napproaches behave as multiple sensors measuring image-flow.  An\nintegrated framework is described that applies the principles of\nstatistical estimation theory to fuse the measurements obtained from\ndifferent approaches. The resulting estimate of image-flow has the\nminimum expected-error. Some algorithms based on this framework are\ndescribed. Several proposals for extension of this thesis are also\nincluded. (pdf) AN OBJECT MODEL FOR SHARED DATA Gail E. Kaiser, Brent Hailpern 1989-01-01 (pdf) {\\em AI} TECHNIQUES IN SOFTWARE ENGINEERING Gail E. Kaiser 1989-01-01 (pdf) AN INTEGRATED APPROACH TO STEREO MATCHING, SURFACE RECONSTRUCTION AND DEPTH SEGMENTATION USING CONSISTENT SMOOTHNESS ASSUMPTIONS L. H. Chen, Terrence E. Boult 1988-01-01 (pdf) REGULARIZATION:  PROBLEMS AND PROMISES Terrence E. Boult 1988-01-01 (pdf) PERFORMANCE ANALYSIS AND IMPROVEMENT IN UNIX FILE SYSTEM TREE TRAVERSAL Jonathan M. Smith 1988-01-01 (pdf) A SURVEY OF PROCESS MIGRATION MECHANISMS Jonathan M. Smith 1988-01-01 (pdf) A SURVEY OF SOFTWARE FAULT TOLLERENCE TECHNIQUES Jonathan M. Smith 1988-01-01 (pdf) (ps) PARALLEL ALGORITHMIC TECHNIQUES FOR COMBINATORIAL COMPUTATION David Eppstein, Zvi Galil 1988-01-01 (pdf) SPEEDING UP DYNAMIC PROGRAMMING WITH APPLICATION TO THE COMPUTATION OF RNA STRUCTURE David Eppstein, Zvi Galil, Raffaele Giancarlo 1988-01-01 (pdf) MINIMUM-KNOWLEDGE INTERACTIVE PROOFS FOR DECISION PROBLEMS Zvi Galil, Stuart Haber, Moti Yung 1988-01-01 (pdf) IMAGE WARPING AMONG ARBITRARY PLANAR SHAPES George Wolberg 1988-01-01 (pdf) IMPROC:  AN INTERACTIVE IMAGE PROCESSING SOFTWARE PACKAGE George Wolberg 1988-01-01 (pdf) ON THE RECOVERY OF SUPERELLIPSOIDS Terrence E. Boult, Ari Gross 1988-01-01 (pdf) IMAGE UNDERSTANDING AND ROBOTICS RESEARCH AT COLUMBIA UNIVERSITY John R. Kender, Peter K. Allen, Terrence E. Boult, Hussein A. Ibrahim 1988-01-01 (pdf) RAY TRACING USING POLARIZATION PARAMETERS Lawrence B. Wolff, David J. Kurlander 1988-01-01 (pdf) BUT WHAT DO YOU NEED TO PRODUCE A BUT? Michael Elhadad, Kathleen R. McKeown 1988-01-01 (pdf) THE APPLICATION OF APPROXIMATION AND COMPLEXITY THEORY METHODS TO THE SOLUTION OF COMPUTER VISION PROBLEMS Michael Hatzitheodorou 1988-01-01 (pdf) GLOBAL READING OF ENTIRE DATABASES Calton Pu, Christine H. Hong, Jae M. Wha 1988-01-01 (pdf) METHODS AND APPROACHES IN REAL TIME HIERARCHICAL MOTION DETECTION Ajit Singh, Peter K. Allen 1988-01-01 (pdf) AN ALGORITHMIC TAXONOMY OF PRODUCTION SYSTEM MACHINES Russell C. Mills 1988-01-01 (pdf) USER'S MANUAL FOR PYRAMID EMULATION ON THE CONNECTION MACHINE Lisa Gottesfeld Brown, Qifan Ju, Cynthia Norman 1988-01-01 (pdf) SYNCHRONIZATION, COMMUNICATION AND I/O FACTORS IN DATABASE MACHINE PERFORMANCE Andy Lowry 1988-01-01 (pdf) STRUCTURE OF COMPLEXITY CLASSES: SEPARATIONS, COLLAPSES, AND COMPLETENESS Lane A. Hemachandra 1988-01-01 (pdf) ON THE STRUCTURE OF SOLUTIONS OF COMPUTABLE REAL FUNCTIONS Juris Hartmanis, Lane Hemachandra 1988-01-01 (pdf) HIGHLIGHTING USER RELATED ADVICE Kathleen R. McKeown, Robert A. Weida 1988-01-01 (pdf) INFERRING USER-ORIENTED ADVICE IN 'ADVISOR' Robert A. Weida, Kathleen R. McKeown 1988-01-01 (pdf) APPROXIMATE THEORY FORMATION:  AN EXPLANATION-BASED APPROACH Thomas Ellman 1988-01-01 (pdf) THE DERIVATION OF TWO-DIMENSIONAL SHAPE FROM SHADOWS Michael Hatzitheodorou 1988-01-01 (pdf) INFUSE TEST MANAGEMENT Gail E. Kaiser, Dewayne E. Perry 1988-01-01 (pdf) THE WORLD ACCORDING TO GARP Gail E. Kaiser, Roy Campbell, Steven Goering, Susan Hinrichs,  Brenda Jackels, Joe Loyall 1988-01-01 (pdf) Incremental Dynamic Semantics for Language-Based Programming Environments Gail E. Kaiser 1988-01-01 (pdf) (ps) SPLIT-TRANSACTIONS FOR OPEN-ENDED ACTIVITIES Calton Pu, Gail E. Kaiser, Norman Hutchinson 1988-01-01 (pdf) SUPPORT FOR RELIABLE DISTRIBUTED COMPUTING Gail E. Kaiser, Wenwey Hseush 1988-01-01 (pdf) A RETROSPECTIVE ON DOSE: AN INTERPRETIVE APPROACH TO STRUCTURE EDITOR GENERATION Gail E. Kaiser, Peter H. Feiler, Fahimeh Jalili, Johann H. Schlichter 1988-01-01 (pdf) A PRACTICAL COURSE IN SOFTWARE DESIGN Jonathan M. Smith 1988-01-01 (pdf) SOLVING THE DEPTH INTERPOLATION PROBLEM ON A PARALLEL ARCHITECTURE WITH EFFICIENT NUMERICAL METHODS -- Ph.D. Dissertation Dong Jae Choi 1988-01-01 (pdf) AN EXPERIMENT WITH THE STUDENT ADVISOR SYSTEM Jong G. Lim\", 1988-01-01 (pdf) ALGORITHM-BASED FAULT TOLERANCE IN MASSIVELY PARALLEL SYSTEMS Mark D. Lerner 1988-01-01 (pdf) APPROXIMATE STRING MATCHING ON THE DADO2 PARALLEL COMPUTER Toshikatsu Mori, Salvatore J. Stolfo 1988-01-01 (pdf) A METHODOLOGY FOR PROGRAMMING PRODUCTION SYSTEMS AND ITS IMPLICATIONS ON PARALLELISM Alexander J. Pasik 1988-01-01 (pdf) HAPTIC OBJECT RECOGNITION USING A MULTI-FINGERED DEXTROUS HAND Peter K. Allen, Kenneth S. Roberts 1988-01-01 (pdf) THE RB LANGUAGE Jonathan M. Smith, Gerald Q. Maguire Jr. 1988-01-01 (ps) IMPLEMENTING REMOTE FORK ( WITH CHECKPOINT/RESTART Jonathan M. Smith, John Ioannidis 1988-01-01 (ps) RAPID LOCATION OF MOUNT POINTS Jonathan M. Smith 1988-01-01 (pdf) (ps) TOOL EXTENSION IN AN ALOE EDITOR Takahisa Ishizuka 1988-01-01 (pdf) IMAGE FORMATS: FIVE YEARS AFTER THE AAPM STANDARD FOR DIGITAL IMAGE INTERCHANGE Gerald Q. Maguire Jr., Marilyn E. Noz 1988-01-01 (pdf) ON THE POWER OF PROBABILISTIC POLYNOMIAL TIME: P(NP[LOG] IS CONTAINED IN PP Lane A. Hemachandra, Gerd Wechsung 1988-01-01 (pdf) NEST USER'S GUIDE Alexander Dupuy, Jed Schwartz 1988-01-01 (pdf) NEST LIBRARY REFERENCE MANUAL Alexander Dupuy, Jed Schwartz 1988-01-01 (pdf) NEST SYSTEM OVERVIEW Alexander Dupuy, Jed Schwartz 1988-01-01 (pdf) NEST USER INTERFACE MANUAL Alexander Dupuy, Jed Schwartz 1988-01-01 (pdf) INCREMENTAL EVALUATION OF ORDERED ATTRIBUTE GRAMMARS FOR ASYNCHRONOUS SUBTREE REPLACEMENTS Josephine Micallef 1988-01-01 (pdf) SEQUENCE COMPARISON WITH MIXED CONVEX AND CONCAVE COSTS David Eppstein 1988-01-01 (pdf) A DISTRIBUTED SIGNAL PROCESSING FACILITY FOR SPEECH RESEARCH Nathaniel Polish 1988-01-01 (pdf) TRANSPARENT CONCURRENT EXECUTION OF MUTUALLY EXCLUSIVE ALTERNATIVES Jonathan M. Smith, Gerald Q. Maguire,  Jr. 1988-01-01 (pdf) FAST FOURIER TRANSFORMS: A REVIEW George Wolberg 1988-01-01 (pdf) CUBIC SPLINE INTERPOLATION: A REVIEW George Wolberg 1988-01-01 (pdf) GEOMETRIC TRANSFORMATION TECHNIQUES FOR DIGITAL IMAGES: A SURVEY George Wolberg 1988-01-01 (pdf) AN AUTOMATED CONSULTANT FOR INTERACTIVE ENVIRONMENTS U. Wolz, G. E. Kaiser 1988-01-01 (pdf) AUTOMATED TUTORING IN INTERACTIVE ENVIRONMENTS: A TASK CENTERED APPROACH U. Wolz, K. R. McKeown, G. E. Kaiser 1988-01-01 (pdf) AUTOMATED CONSULTING FOR EXTENDING USER EXPERTISE IN INTERACTIVE ENVIRONMENTS: A TASK CENTERED APPROACH Ursula Wolz 1988-01-01 (pdf) SEGMENTING SPECULAR HIGHLIGHTS FROM OBJECT SURFACES Lawrence B. Wolff 1988-01-01 (pdf) SHAPE UNDERSTANDING FROM LABERTIAN PHOTOMETRIC FLOW FIELDS Lawrence B. Wolff 1988-01-01 (pdf) COMPUTATIONAL ASPECTS OF LANGUAGE ACQUISITION FROM WORLD TO WORD Frank Smadja 1988-01-01 (pdf) MECHANICAL GENERATION OF HEURISTICS FOR INTRACTABLE THEORIES Thomas Ellman 1988-01-01 (pdf) Intelligent Assistance for Software Development and Maintenance Gail E. Kaiser,  Peter H. Feiler, Steven S. Popovich 1988-01-01 (ps) EXTENDED TRANSACTION MODELS FOR SOFTWARE DEVELOPMENT ENVIRONMENTS Gail E. Kaiser 1988-01-01 (pdf) AN INTEGRATED SYSTEM FOR DEXTROUS MANIPULATION Peter K. Allen, Paul Michelman, Kenneth S. Roberts 1988-01-01 (pdf) A MODEL OF COMPUTATION FOR TRANSACTION PROCESSING Avraham Leff, Calton Pu 1988-01-01 (pdf) COMPARISON OF SURFACE LANGUAGE GENERATORS: A CASE STUDY IN CHOICE OF CONNECTIVES K. R. McKeown, M. Elhadad 1988-01-01 (pdf) THE FUF FUNCTIONAL UNIFIER: USER'S MANUAL M. Elhadad 1988-01-01 (pdf) FINDING A BETTER WAY: CHOOSING AND EXPLAINING ALTERNATIVE PLANS U. Wolz 1988-01-01 (pdf) TUTORING THAT RESPONDS TO USER QUESTIONS AND PROVIDES ENRICHMENT U. Wolz 1988-01-01 (pdf) LANDMARKS, MAPS, AND SELF-LOCATION IN THE PRESENCE OF ERROR Avraham Leff 1988-01-01 (pdf) CONCURRENT ALGEBRAS FOR VLSI DESIGN T. S. Balraj 1988-01-01 (pdf) AN OBJECT-ORIENTED MODEL FOR NETWORK MANAGEMENT Soumitra Sengupta, Alexander Dupuy, Jed Schwartz, Yechiam Yemini 1988-01-01 (pdf) THE SYNTHESIS KERNEL Calton Pu, Henry Massalin, John Ioannidis 1987-01-01 (pdf) MONITORING CONCEPTUAL DECISIONS TO GENERATE COHERENT TEXT Michal Blumenstyck, Kathleen R. McKeown 1987-01-01 (pdf) A SURVEY OF AUTOMATED CONSULTING IN INTERACTIVE PROGRAMMING ENVIRONMENTS Ursula Wolz 1987-01-01 (pdf) FINDING A MAXIMUM-GENUS GRAPH IMBEDDING Merrick L. Furst, Jonathan L. Gross, Lyle A. McGeoch 1987-01-01 (pdf) THE THEORY OF MINIMUM-KNOWLEDGE PROTOCOLS Stuart Haber 1987-01-01 (pdf) EXPLANATION-BASED METHODS FOR SIMPLIFYING INTRACTABLE THEORIES: Thesis Proposal Thomas Ellman 1987-01-01 (pdf) EXPLANATION-BASED LEARNING: A SURVEY OF PROGRAMS AND PERSPECTIVES Thomas Ellman 1987-01-01 (pdf) PROCESS MIGRATION:  EFFECTS ON SCIENTIFIC COMPUTATION Gerald Q. Maguire, Jr., Jonathan A. Smith 1987-01-01 (pdf) RB:  PROGRAMMER SPECIFICATION OF REDUNDANCY Jonathan M. Smith, Gerald Q. Maguire,  Jr. 1987-01-01 (pdf) CHOICES IN THE DESIGN OF A LANGUAGE PRE-PROCESSOR FOR SPECIFYING REDUNDANCY Jonathan M. Smith, Gerald Q. Maguire,  Jr. 1987-01-01 (pdf) ON THE POWER OF PARITY Jin-yi Cai, Lane A. Hemachandra 1987-01-01 (pdf) DIAL: DIAGRAMMATIC ANIMATION LANGUAGE TUTORIAL AND REFERENCE MANUAL Steven Feiner 1987-01-01 (pdf) RELIABLE NETWORK COMMUNICATIONS Gail E. Kaiser, Yael J. Cycowicz, Wenwey Hseush, Josephine Micallef 1987-01-01 (pdf) SOFTWARE DEVELOPMENT ENVIRONMENTS FOR VERY LARGE SOFTWARE SYSTEMS Gail E. Kaiser, Yoelle S. Maarek, Dewayne E. Perry, Robert W. Schwanke 1987-01-01 (pdf) MERCURY: DISTRIBUTED INCREMENTAL ATTRIBUTE GRAMMAR EVALUATION Gail E. Kaiser, Josephine Micallef, Simon M. Kaplan 1987-01-01 (pdf) MELD: A MULTI-PARADIGM LANGUAGE WITH OBJECTS, DATAFLOW AND MODULES Gail E. Kaiser, David Garlan 1987-01-01 (pdf) PARALLEL COMPUTERS, NUMBER THEORY PROBLEMS, AND EXPERIMENTAL RESULTS Mark D. Lerner 1987-01-01 (pdf) AN INCONSISTENCY MANAGEMENT SYSTEM: MASTER'S THESIS Harris M. Morgenstern 1987-01-01 (pdf) ENCAPSULATION, REUSABILITY AND EXTENSIBILITY IN OBJECT-ORIENTED PROGRAMMING LANGUAGES Josephine Micallef 1987-01-01 (pdf) DERIVING SHAPE FROM SHADOWS:  A HILBERT SPACE SETTING Michael Hatzitheodorou 1987-01-01 (pdf) LINKS BETWEEN SITUATION AND LANGUAGE USE; TOWARDS A MORE COHERENT INTERACTION Michael Elhadad 1987-01-01 (pdf) UPDATING DISTANCE MAPS WHEN OBJECTS MOVE Terrence E. Boult 1987-01-01 (pdf) RECOVERING SUPERQUADRICS FROM 3-D INFORMATION Terrence E. Boult, Ari Gross 1987-01-01 (pdf) AN EXPERIMENTAL SYSTEM FOR THE INTEGRATION OF INFORMATION FROM STEREO AND MULTIPLE SHAPE-FROM-TEXTURE ALGORITHMS Terrence E. Boult, Mark L. Moerdler 1987-01-01 (pdf) GENUS DISTRIBUTIONS FOR BOUQUETS OF CIRCLES Jonathan L. Gross, David P. Robbins, Thomas W. Tucker 1987-01-01 (pdf) GENUS DISTRIBUTIONS FOR TWO CLASSES OF GRAPHS Merrick L. Furst, Jonathan L. Gross, Richard Statman 1987-01-01 (pdf) RESET SEQUENCES FOR FINITE AUTOMATA WITH APPLICATION TO DESIGN OF PARTS ORIENTERS David Eppstein 1987-01-01 (pdf) A SYNTACTIC OMNI-FONT CHARACTER RECOGNITION SYSTEM George Wolberg 1987-01-01 (pdf) DATA STRUCTURES AND ALGORITHMS FOR APPROXIMATE STRING MATCHING Zvi Galil, Raffaele Giancarlo 1987-01-01 (pdf) SPEEDING UP DYNAMIC PROGRAMMING WITH APPLICATIONS TO MOLECULAR BIOLOGY Zvi Galil, Raffaele Giancarlo 1987-01-01 (pdf) ON NONTRIVIAL SEPARATORS FOR k-PAGE GRAPHS AND SIMULATIONS BY NONDETERMINISTIC ONE-TAPE TURING MACHINES Zvi Galil, Ravi Kannan, Endre Szemeredi 1987-01-01 (pdf) AN O(n2(m+nlog nlog n MIN-COST FLOW ALGORITHM Zvi Galil, Eva Tardos 1987-01-01 (pdf) ON 3-PUSHDOWN GRAPHS WITH LARGE SEPARATORS Zvi Galil, Ravi Kannan, Endre Szemeredi 1987-01-01 (pdf) TWO LOWER BOUNDS IN ASYNCHRONOUS DISTRIBUTED COMPUTATION Pavol Duris, Zvi Galil 1987-01-01 (pdf) A SURVEY OF INTERFACES TO DATA BASE AND EXPERT SYSTEMS Galina Datskovsky Moerdler 1987-01-01 (pdf) BUILDING A NATURAL LANGUAGE INTERFACE TO EXPERT SYSTEMS Galina Datskovsky Moerdler 1987-01-01 (pdf) A COLLECTION OF 4 TRANSCRIPTS OF LONG CONVERSATIONS THROUGH COMPUTERS - INCLUDING THE EXPERIMENTS PROTOCOLS Michael Elhadad 1987-01-01 (pdf) THE USE OF EXPLICIT USER MODELS IN TEXT GENERATION: TAILORING TO A USER'S LEVEL OF EXPERTISE Cecile Laurence Paris 1987-01-01 (pdf) MIXED DISTANCE MEASURES FOR OPTIMIZING CONCATENATIVE VOCABULARIES FOR SPEACH SYNTHESIS: A THESIS PROPOSAL Nathaniel Polish 1987-01-01 (pdf) A SURVEY OF PARALLEL PROGRAMMING CONSTRUCTS Michael van Biema 1987-01-01 (pdf) IMPROVING PRODUCTION SYSTEM PERFORMANCE ON PARALLEL ARCHTECTURES BY CREATING CONSTRAINED COPIES OF RULES Alexander J. Pasik, Salvatore J. Stolfo 1987-01-01 (pdf) CIRCUIT MINIMIZATION TECHNIQUES APPLIED TO KNOWLEDGE ENGINEERING Alexander J. Pasik 1987-01-01 (pdf) THE EXPECTED-OUTCOME MODEL OF TWO-PLAYER GAMES (Ph.D. Dissertation Bruce Abramson 1987-01-01 (pdf) SIMD TREE ALGORITHMS FOR IMAGE CORRELATION Hussein A. H. Ibrahim, John R. Kender, David Elliot Shaw 1986-01-01 (pdf) DISK RESPONSE TIME MEASUREMENTS Thomas D. Johnson, Jonathan M. Smith, Eric S. Wilson 1986-01-01 (pdf) CHANGE MANAGEMENT SUPPORT FOR LARGE SOFTWARE SYSTEMS Gail Kaiser, Dewayne E. Perry, Robert W. Schwanke 1986-01-01 (pdf) GENERATION OF DISTRIBUTED PROGRAMMING ENVIRONMENTS Gail E. Kaiser, Simon M. Kaplan, Josephine Micallef 1986-01-01 (pdf) MELD/FEATURES: AN OBJECT-ORIENTED APPROACH TO REUSABLE SOFTWARE Gail E. Kaiser 1986-01-01 (pdf) SMILE/MARVEL: TWO APPROACHES TO KNOWLEDGE-BASED PROGRAMMING ENVIRONMENTS Gail E. Kaiser, Peter H. Feiler 1986-01-01 (pdf) THE DO-LOOP CONSIDERED HARMFUL IN PRODUCTION SYSTEM PROGRAMMING Michael van Biema, Daniel P. Miranker, Salvatore J. Stolfo 1986-01-01 (pdf) PHYSICAL OBJECT REPRESENTATION AND GENERALIZATION: A SURVEY OF PROGRAMS FOR SEMANTIC-BASED NATURAL LANGUAGE PROCESSING Kenneth Wasserman 1986-01-01 (pdf) THESIS PROPOSAL: THE EXPECTED-OUTCOME MODEL OF TWO-PLAYER GAMES Bruce Abramson 1986-01-01 (pdf) DIRECTOR -- AN INTERPRETER FOR RULE-BASED PROGRAMS Galina Datskovsky Moerdler, J. Robert Ensor 1986-01-01 (pdf) THE OPS FAMILY OF PRODUCTION SYSTEM LANGUAGES Alexander Pasik 1986-01-01 (pdf) AVOIDING LATCH FORMATION IN REGULAR EXPRESSION RECOGNIZERS M. J. Foster 1986-01-01 (pdf) CONTROL STRATEGIES FOR TWO PLAYER GAMES Bruce Abramson 1986-01-01 (pdf) LANGUAGE GENERATION:  APPLICATIONS, ISSUES, AND APPROACHES Kathleen R. McKeown 1986-01-01 (pdf) REPLICATION AND NESTED TRANSACTIONS IN THE EDEN DISTRIBUTED SYSTEM Calton Pu 1986-01-01 (pdf) PROBABILISTIC SETTING OF INFORMATION-BASED COMPLEXITY H. Wozniakowski 1986-01-01 (pdf) COMPLEXITY OF INTEGRATION IN DIFFERENT SETTINGS H. Wozniakowski 1986-01-01 (pdf) SENSING AND DESCRIBING 3-D STRUCTURE Peter K. Allen 1986-01-01 (pdf) INTEGRATING VISION AND TOUCH FOR OBJECT RECOGNITION TASKS Peter K. Allen 1986-01-01 (pdf) APPROACHES TO DISTRIBUTED UNIX SYSTEMS Jonathan Smith 1986-01-01 (pdf) SUPERDATABASES FOR COMPOSITION OF HETEROGENEOUS DATABASES Calton Pu 1986-01-01 (pdf) SMOOTHNESS ASSUMPTIONS IN HUMAN AND MACHINE VISION, AND THEIR IMPLICATIONS FOR OPTIMAL SURFACE INTERPOLATION Terrance Boult 1986-01-01 (pdf) METHODS FOR PERFORMANCE EVALUATION OF PARALLEL COMPUTER SYSTEMS Yoram Eisenstadter 1986-01-01 (pdf) TRANSLATING BETWEEN PROGRAMMING LANGUAGES USING A CANONICAL REPRESENTATION AND ATTRIBUTE GRAMMAR INVERSION- EXTENDED ABSTRACT Rodney Farrow, Daniel Yellin 1986-01-01 (pdf) A METHODOLOGY FOR SPECIFICATION-BASED PERFORMANCE ANALYSIS OF PROTOCOLS - Ph.D. Dissertation Nihal Nounou 1986-01-01 (pdf) CANONICAL APPROXIMATION IN THE PERFORMANCE ANALYSIS OF DISTRIBUTED SYSTEMS -- PhD Dissertation Eugene Pinsky 1986-01-01 (pdf) AN INFORMATION-BASED APPROACH TO ILL-POSED PROBLEMS Arthur G. Werschulz 1986-01-01 (pdf) COMPLEITY OF DIFFERENTIAL AND INTEGRAL EQUATIONS Arthur G. Werschulz 1985-01-01 (pdf) ANALYZING USER PLANS TO PRODUCE INFORMATIVE RESPONSES BY A PROGRAMMER'S CONSULTANT Ursula Wolz 1985-01-01 (pdf) GENERATING ADMISSIBLE HEURISTICS BY CRITICIZING SOLUTIONS TO RELAXED MODELS Othar Hansson, Andrew E. Mayer, Mordechai M. Yung 1985-01-01 (pdf) OBJECT RECOGNITION USING VISION AND TOUCH Peter Kirby Allen 1985-01-01 (pdf) ON THE APPLICATION OF MASSIVELY PARALLEL SIMD TREE MACHINES TO CERTAIN INTERMEDIATE-LEVEL VISION TASKS Hussein A. H. Ibrahim, John R. Kender, David Elliot Shaw 1985-01-01 (pdf) A LISP COMPILER FOR THE DADO PARALLEL COMPUTER Mark D. Lerner, Michael van Biema, Gerald Q. Maguire Jr. 1985-01-01 (pdf) TAKING THE INITIATIVE FOR SYSTEM GOALS IN COOPERATIVE DIALOGUE Kevin Matthews 1985-01-01 (pdf) INITIATORY AND REACTIVE SYSTEM ROLES IN HUMAN COMPUTER DISCOURSE Kevin Matthews 1985-01-01 (pdf) A CURE FOR PATHOLOGICAL BEHAVIOR IN GAMES THAT USE MINIMAX Bruce Abramson 1985-01-01 (pdf) TOWARDS THE PARALLEL EXECUTION OF RULES IN PRODUCTION SYSTEM PROGRAMS Toru Ishida, Salvatore J. Stolfo 1985-01-01 (pdf) MACRO-OPERATORS:  A WEAK METHOD FOR LEARNING Richard E. Korf 1985-01-01 (pdf) SURFACE ORIENTATION AND SEGMENTATION FROM PERSPECTIVE VIEWS OF PARALLEL-LINE TEXTURES Mark L. Moerdler, John R. Kender 1985-01-01 (pdf) DEVELOPMENT TOOLS FOR COMMUNICATION PROTOCOLS Nihal Nounou, Yechiam Yemini 1985-01-01 (pdf) COMPLEXITY OF APPROXIMATELY SOLVED PROBLEMS J. F. Traub 1985-01-01 (pdf) ON THE COMPLEXITY OF COMPOSITION AND GENERALIZED COMPOSITION OF POWER SERIES R. P. Brent, J. F. Traub 1985-01-01 (pdf) A GENERIC FRAMEWORK FOR EXPERT DATA ANALYSIS SYSTEMS Luanne Burns, Alexander Pasik 1985-01-01 (pdf) EXPLANATION AND ACQUISITION IN EXPERT SYSTEMS USING SUPPORT KNOWLEDGE Alexander Pasik, Jens Christensen, Douglas Gordin,  Agata Stancato-Pasik, Salvatore Stolfo 1985-01-01 (pdf) A COMPARISON OF STORAGE OPTIMIZATIONS IN AUTOMATICALLY-GENERATED ATTRIBUTE EVALUATORS Rodney Farrow, Daniel Yellin 1985-01-01 (pdf) DESCRIPTION STRATEGIES FOR NAIVE AND EXPERT USERS Cecile L. Paris 1985-01-01 (pdf) NATURAL LANGUAGE INTERFACES TO EXPERT SYSTEMS Galina Datskovky 1985-01-01 (pdf) THE INFORMATION-CENTERED APPROACH TO OPTIMAL ALGORITHMS APPLIED TO THE 2-1/2 D SKETCH John R. Kender, David Lee, Terrance Boult 1985-01-01 (pdf) RESEARCHER:  AN EXPERIMENTAL INTELLIGENT INFORMATION SYSTEM Michael Lebowitz 1985-01-01 (pdf) TAILORING EXPLANATIONS FOR THE USER Kathleen R. McKeown, Myron Wish, Kevin Matthews 1985-01-01 (pdf) THE NEED FOR TEXT GENERATION Kathleen R. McKeown 1985-01-01 (pdf) A SIMPLE PREPROCESSING SCHEME TO EXTRACT AND BALANCE IMPLICIT PARALLELISM IN THE CONCURRENT MATCH OF PRODUCTION RULES Salvatore J. Stolfo, Daniel Miranker, Russell C. Mills 1985-01-01 (pdf) MORE RULES MAY MEAN FASTER PARALLEL EXECUTION Salvatore J. Stolfo, Daniel Miranker, Russell C. Mills 1985-01-01 (pdf) OPTIMAL ALGORITHM FOR LINEAR PROBLEMS WITH GAUSSIAN MEASURES G. W. Wasilkowski 1985-01-01 (pdf) UNIFYING REPRESENTATION AND GENERALIZATION: UNDERSTANDING HIERARCHICALLY STRUCTURED OBJECTS -- Ph.D. Dissertation Kenneth Hal Wasserman 1985-01-01 (pdf) A PRIVATE INTERACTIVE TEST OF A BOOLEAN PREDICATE AND MINIMUM-KN OWLEDGE PUBLIC-KEY CRYPTOSYSTEMS-EXTENDED ABSTRACT Zvi Galil, Stuart Haber, Moti Yung 1985-01-01 (pdf) DISTRIBUTED ALGORITHMS IN SYNCHRONOUS BROADCASTING NETWORKS Zvi Galil, Gad M. Landau, Moti Yung 1985-01-01 (pdf) A METHODOLOGY FOR SPECIFICATION-BASED PERFORMANCE ANALYSIS OF COMMUNICATION PROTOCOLS Nihal Nounou, Yechiam Yemini 1985-01-01 (pdf) COMPLEXITY OF COMPUTING TOPOLOGICAL DEGREE OF LIPSCHITZ FUNCTIONS IN N DIMENSIONS T. Boult, K. Sikorski 1985-01-01 (pdf) VISUAL SURFACE INTERPOLATION: A COMPARISON OF TWO METHODS Terrance E. Boult 1985-01-01 (pdf) GENERALIZING LOGIC CIRCUIT DESIGNS BY ANALYZING PROOFS OF CORRECTNESS Thomas Ellman 1985-01-01 (pdf) SYMMETRIC PUBLIC-KEY ENCRYPTION Zvi Galil, Stuart Haber, Moti Yung 1985-01-01 (pdf) FINDING A MAXIMUM-GENUS GRAPH IMBEDDING Merrick L. Furst, Jonathan L. Gross, Lyle A. McGeoch 1985-01-01 (pdf) GENUS DISTRIBUTION FOR TWO CLASSES OF GRAPHS Merrick L. Furst, Jonathan L. Gross, Richard Statman 1985-01-01 (pdf) HIERARCHY FOR IMBEDDING-DISTRIBUTION INVARIANTS OF A GRAPH Jonathan L. Gross, Merrick Furst 1985-01-01 (pdf) A KNOWLEDGE-BASED EXPERT SYSTEMS PRIMER AND CATALOG Bruce K. Hillyer 1985-01-01 (pdf) SPECIFICATION IF INTERPRETERS AND DEBUGGERS USING AN EXTENSION OF ATTRIBUTE GRAMMARS Gail E. Kaiser 1985-01-01 (pdf) DEPTH-FIRST ITERATIVE-DEEPENING: AN OPTIMAL ADMISSIBLE TREE SEARCH Richard E. Korf 1985-01-01 (pdf) IMPLEMENTATION OF THE GMR ALGORITHM FOR LARGE SYMMETRIC EIGENPROBLEMS Jacek Kuczynski 1985-01-01 (pdf) ON THE OPTIMAL SOLUTION OF LARGE EIGENPAIR PROBLEMS Jacek Kuczynski 1985-01-01 (pdf) THE USE OF MEMORY IN TEXT PROCESSING Michael Lebowitz 1985-01-01 (pdf) INTEGRATED LEARNING: CONTROLLING EXPLANATION Michael Lebowitz 1985-01-01 (pdf) STORY TELLING AS PLANNING AND LEARNING Michael Lebowitz 1985-01-01 (pdf) DISCOURSE STRATEGIES FOR GENERATING NATURAL-LANGUAGE TEXT Kathleen R. McKeown 1985-01-01 (pdf) AN AUTOMATED PERFORMANCE ANALYSIS OF A TWO PHASE LOCKING PROTOCOL Nihal Nounou, Yechiam Yemini 1985-01-01 (pdf) A METHODOLOGY FOR SPECIFICATION-BASED PERFORMANCE ANALYSIS OF COMMUNICATION PROTOCOLS Nihal Nounou, Yechiam Yemini 1985-01-01 (pdf) GUIDE TO THE UNIFICATION PROCESS AND ITS IMPLEMENTATION.  PROGRESS REPORT ON EXTENDING THE GRAMMAR. Cecile L. Paris, TjoeLiong Kwee 1985-01-01 (pdf) TOWARDS MORE GRACEFUL INTERACTION: A SURVEY OF QUESTION-ANSWERING PROGRAMS Cecile L. Paris 1985-01-01 (pdf) EQUIVALENT DESCRIPTIONS OF GENERALIZED CYLINDERS K. S. Roberts 1985-01-01 (pdf) DADO: A PARALLEL COMPUTER FOR ARTIFICIAL INTELLIGENCE Salvatore J. Stolfo 1985-01-01 (pdf) A SIMPLE SCHEME FOR A FAULT TOLERANT DADO MACHINE Salvatore J. Stolfo 1985-01-01 (pdf) CLOCKING SCHEMES FOR HIGH SPEED DIGITAL SYSTEMS Stephen H. Unger 1985-01-01 (pdf) NATURAL LANGUAGE FOR EXPERT SYSTEMS: COMPARISONS WITH DATABASE SYSTEMS Kathleen R. McKeown 1984-01-01 (pdf) ASYMPTOTIC OPTIMALITY OF THE BISECTION METHOD K. Sikorski, G. M. Trojan 1984-01-01 (pdf) MINIMAL NUMBER OF FUNCTION EVALUATIONS FOR COMPUTING TOPOLOGICAL DEGREE IN TWO DIMENSIONS K. Sikorski 1984-01-01 (pdf) LOGIC PROGRAMMING USING PARALLEL ASSOCIATIVE OPERATIONS S. Taylor, A. Lowry, G. Q. Maguire Jr., S. J. Stolfo 1984-01-01 (pdf) UNIFICATION IN A PARALLEL ENVIRONMENT Stephen Taylor, Daphne Tzoar, Salvatore J. Stolfo 1984-01-01 (pdf) PUTTING PIECES TOGETHER: UNDERSTANDING PATENT ABSTRACTS Michael Lebowitz 1984-01-01 (pdf) CREATING CHARACTERS IN A STORY-TELLING UNIVERSE Michael Lebowitz 1984-01-01 (pdf) AN O(EV LOG V) ALGORITHM FOR FINDING A MAXIMAL WEIGHTED MATCHING IN GENERAL GRAPHS Zvi Galil, Silvio Micali, Harold Gabow 1984-01-01 (pdf) ILL-FORMED TEXT AND CONCEPTUAL PROCESSING Michael Lebowitz 1984-01-01 (pdf) EXPERIENCE WITH A PRODUCTION COMPILER AUTOMATICALLY GENERATED FROM AN ATTRIBUTE GRAMMAR Rodney Farrow 1984-01-01 (pdf) SIMD AND MSIMD VARIANTS OF THE NON-VON SUPERCOMPUTER David Elliot Shaw 1984-01-01 (pdf) PPL/M: THE SYSTEM LEVEL LANGUAGE FOR PROGRAMMING THE @I(DADO) MACHINE Salvatore J. Stolfo, Gerald Q. Maguire Jr., Mark D. Lerner 1984-01-01 (pdf) @G(b)-TREES, @G(g)-SYSTEMS, AND A THEOREM ON F-HEAPS Zvi Galil, Thomas Spencer 1984-01-01 (pdf) LINEAR PROBLEMS (WITH EXTENDED RANGE) HAVE LINEAR OPTIMAL ALGORITHMS Edward W. Packel 1984-01-01 (pdf) ||PSL: A PARALLEL LISP FOR THE @I(DADO) MACHINE Michael K. Van Biema, Mark D. Lerner, Gerald Maguire,  Salvatore J. Stolfo 1984-01-01 (pdf) AN ANALYSIS OF ABSTRACTION IN PROBLEM SOLVING Richard E. Korf 1984-01-01 (pdf) SIMULTANEOUS FIRING OF PRODUCTION RULES ON TREE STRUCTURED MACHINES Toru Ishida, Salvatore J. Stolfo 1984-01-01 (pdf) NTEREST AND PREDICTABILITY: DECIDING WHAT TO LEARN, WHEN TO LEARN Michael Lebowitz 1984-01-01 (pdf) CONCEPT LEARNING IN A RICH INPUT DOMAIN: GENERALIZATION-BASED MEMORY Michael Lebowitz 1984-01-01 (pdf) LPS ALGORITHMS: A DETAILED EXAMINATION Andy Lowry, Stephen Taylor, Salvatore J. Stolfo 1984-01-01 (pdf) LPS ALGORITHMS: A CRITICAL ANALYSIS Andy Lowry, Stephen Taylor, Salvatore J. Stolfo 1984-01-01 (pdf) TAKING THE INITIATIVE IN PROBLEM-SOLVING DISCOURSE Kevin Matthews, Kathleen McKeown 1984-01-01 (pdf) DETERMINING THE LEVEL OF EXPERTISE OF A USER OF A QUESTION ANSWERING SYSTEM Cecile Paris 1984-01-01 (pdf) FIVE PARALLEL ALGORITHMS FOR PRODUCTION SYSTEM EXECUTION ON THE DADO MACHINE Salvatore J. Stolfo 1984-01-01 (pdf) PERFORMANCE ESTIMATES FOR THE DADO MACHINE: A COMPARISON OF @I(TREAT) AND @I(RETE) Daniel P. Miranker 1984-01-01 (pdf) TREE MACHINES: ARCHITECTURE AND ALGORITHMS -- A SURVEY PAPER Hussein A. H. Ibrahim 1984-01-01 (pdf) TIME-CONSTRAINED COMMUNICATION IN MULTIPLE ACCESS NETWORKS James Francis Kurose 1984-01-01 (pdf) USING MEMORY IN TEXT UNDERSTANDING Michael Lebowitz 1984-01-01 (pdf) DADO: A PARALLEL PROCESSOR FOR EXPERT SYSTEMS Salvatore J. Stolfo, Daniel P. Miranker 1984-01-01 (pdf) UNDERSTANDING HIERARCHICALLY STRUCTURED OBJECTS Kenneth Wasserman 1984-01-01 (pdf) THE CONNECTED COMPONENT ALGORITHM ON THE NON-VON SUPERCOMPUTER Hussein A.H. Ibrahim 1984-01-01 (pdf) IS CAD/CAM READY FOR AI? Salvatore J. Stolfo 1984-01-01 (pdf) A NOTE ON IMPLEMENTING OPS5 PRODUCTION SYSTEMS ON DADO Salvatore J. Stolfo 1984-01-01 (pdf) WHAT IS THE COMPLEXITY OF THE FREDHOLM PROBLEM OF THE SECOND KIND? Arthur G. Werschulz 1984-01-01 (pdf) WHAT IS THE COMPLEXITY OF ELLIPTIC SYSTEMS? Arthur G. Werschulz 1984-01-01 (pdf) AN EIGHT-PROCESSOR CHIP FOR A MASSIVELY PARALLEL MACHINE David Elliot Shaw, Theodore M. Sabety 1984-01-01 (pdf) A SURVEY OF TREE-WALK EVALUATION STRATEGIES FOR ATTRIBUTE GRAMMARS Daniel Yellin 1984-01-01 (pdf) THE AUTOMATIC INVERSION OF ATTRIBUTE GRAMMARS Daniel Yellin, Eva-Maria M. Mueckstein 1984-01-01 (pdf) A SECURE AND USEFUL 'KEYLESS CRYPTOSYSTEM Mordechai M. Yung 1984-01-01 (pdf) CRYPTOPROTOCOLS: SUBSCRIPTION TO A PUBLIC KEY, THE SECRET BLOCKING AND THE MULTI-PLAYER MENTAL POKER GAME (EXTENDED ABSTRACT) Mordechai Yung 1984-01-01 (pdf) DEVELOPMENT TOOLS FOR COMMUNICATION PROTOCOLS: AN OVERVIEW Nihal Nounou, Yechiam Yemini 1984-01-01 (pdf) ALGEBRAIC SPECIFICATION-BASED PERFORMANCE ANALYSIS OF COMMUNICATION PROTOCOLS Nihal Nounou, Yechiam Yemini 1984-01-01 (pdf) CAN WE APPROXIMATE ZEROS OF FUNCTIONS WITH NON-ZERO TOPOLOGICAL DEGREE? T. Boult, K. Sikorski 1984-01-01 (pdf) EXECUTION OF OPS5 PRODUCTION SYSTEMS ON A MASSIVELY PARALLEL MACHINE Bruce K. Hillyer, David Elliot Shaw 1984-01-01 (pdf) SOME NONLINEAR PROBLEMS ARE AS EASY AS THE APPROXIMATION PROBLEM G. W. Wasilkowski 1984-01-01 (pdf) WHEN IS NONADAPTIVE INFORMATION AS POWERFUL AS ADAPTIVE INFORMATION? J. F. Traub, G. W. Wasilkowski, H. Wozniakowski 1984-01-01 (pdf) APPROXIMATION OF LINEAR OPERATORS ON A WIENER SPACE D. Lee 1984-01-01 (pdf) A NOTE ON BIVARIATE BOX SPLINES ON A K-DIRECTION MESH D. Lee 1984-01-01 (pdf) AN OVERVIEW OF THE DADO PARALLEL COMPUTER Mark D. Lerner, Gerald Q. Maguire Jr., etc. 1984-01-01 (pdf) MENU INTERFACES TO EXPERT SYSTEMS:  OVERVIEW AND EVALUATION Galina Datskovky 1984-01-01 (pdf) FINITE ELEMENT METHODS ARE NOT ALWAYS OPTIMAL Arthur G. Wershulz 1984-01-01 (pdf) LPS ALGORITHMS Andy Lowry, Stephen Taylor, Salvatore J. Stolfo 1984-01-01 (pdf) THE DADO PRODUCTION SYSTEM MACHINE Salvatore J. Stolfo, Daniel P. Miranker 1984-01-01 (pdf) RECURSION IN TEXT AND ITS USE IN LANGUAGE GENERATION Kathleen R. McKeown 1983-01-01 (pdf) FOCUS CONSTRAINTS ON LANGUAGE GENERATION Kathleen R. McKeown 1983-01-01 (pdf) FOR WHICH ERROR CRITERIA CAN WE SOLVE NONLINEAR EQUATIONS? K. Sikorski, H. Wozniakowski 1983-01-01 (pdf) OPTIMAL SOLUTION OF NONLINEAR EQUATIONS SATISFYING A LIPSCHTIZ CONDITION K. Sikorski 1983-01-01 (pdf) ARCHITECTURE AND APPLICATIONS OF DADO: A LARGE-SCALE PARALLEL COMPUTER FOR ARTIFICIAL INTELLIGENCE Salvatore J. Stolfo, Daniel P. Miranker, David Elliot Shaw 1983-01-01 (pdf) PERFORMANCE EVALUATION OF A PACKETIZED VOICE SYSTEM -SIMULATION STUDY Tatsuya Suda, Hideo Miyahara, Toshiharu Hasegawa 1983-01-01 (pdf) PERFORMANCE EVALUATION OF AN INTEGRATED ACCESS SCHEME IN A SATELLITE COMMUNICATION CHANNEL Tatsuya Suda, Hideo Miyahara, Toshiharu Hasegawa 1983-01-01 (pdf) PROLOG ON THE DADO MACHINE: A PARALLEL SYSTEM FOR HIGH-SPEED LOGIC PROGRAMMING Stephen Taylor, Christopher Maio, Salvatore J. Stolfo, David E. Shaw 1983-01-01 (pdf) ENVIRONMENTAL RELATIONS IN IMAGE UNDERSTANDING: THE FORCE OF GRAVITY John R. Kender 1983-01-01 (pdf) EFFICIENT ALGORITHMS FOR FINDING MAXIMAL MATCHING IN GRAPHS Zvi Galil 1983-01-01 (pdf) EXERCISING THE NON-VON PRIMARY PROCESSING SUBSYSTEM Bruce K. Hillyer, David Elliot Shaw 1983-01-01 (pdf) COPING WITH COMPLEXITY J. F. Traub 1983-01-01 (pdf) IMPLEMENTING DESCRIPTIONS USING NON-VON NEUMANN PARALLELISM Michael Lebowitz 1983-01-01 (pdf) CLASSIFYING NUMERIC INFORMATION FOR GENERALIZATION Michael Lebowitz 1983-01-01 (pdf) RESEARCHER: AN OVERVIEW Michael Lebowitz 1983-01-01 (pdf) CREATING A STORY-TELLING UNIVERSE Michael Lebowitz 1983-01-01 (pdf) OPTIMAL INTEGRATION FOR FUNCTIONS OF BOUNDED VARIATION J. F. Traub, D. Lee 1983-01-01 (pdf) SOME NONLINEAR OPERATORS ARE AS EASY TO APPROXIMATE AS THE IDENTITY OPERATOR G. W. Wasilkowski 1983-01-01 (pdf) CUPID: A PROTOCOL DEVELOPMENT ENVIRONMENT Yechiam Yemini, Nihal Nounou 1983-01-01 (pdf) TWO NONLINEAR LOWER BOUNDS FOR ON-LINE COMPUTATIONS Pavol Duris, Zvi Galil, Wolfgang Paul, Ruediger Reischuk 1983-01-01 (pdf) THE NON-VON SUPERCOMPUTER PROJECT: CURRENT IDEOLOGY AND THREE-YEAR PLAN David Elliot Shaw 1983-01-01 (pdf) PHYSICAL OBJECT REPRESENTATION AND GENERALIZATION: A SURVEY OF NATURAL LANGUAGE PROCESSING PROGRAMS Kenneth Wasserman 1983-01-01 (pdf) THE DADO PARALLEL COMPUTER Salvatore J. Stolfo 1983-01-01 (pdf) INFORMATION AND COMPUTATION J. F. Traub, H. Wozniakowski 1983-01-01 (pdf) AVERAGE CASE @G(E)-COMPLEXITY IN COMPUTER SCIENCE: A BAYESIAN VIEW J. B. Kadane, G. W. Wasilkowski 1983-01-01 (pdf) WHAT IS THE COMPLEXITY OF RELATED ELLIPTIC, PARABOLIC AND HYPERBOLIC PROBLEMS Arthur G. Werschulz 1983-01-01 (pdf) COUNTEREXAMPLES IN OPTIMAL QUADRATURE Arthur G. Werschulz 1983-01-01 (pdf) COMPUTING D-OPTIMUM WEIGHING DESIGNS: WHERE STATISTICS, COMBINATORICS, AND COMPUTATION MEET Zvi Galil 1983-01-01 (pdf) TABLE-DRIVEN RULES IN EXPERT SYSTEMS Alexander Pasik, Marshall Schor 1983-01-01 (pdf) LOCAL AVERAGE ERROR G. W. Wasilkowski 1983-01-01 (pdf) COVERS OF ATTRIBUTE GRAMMARS AND SUB-PROTOCOL ATTRIBUTE EVALUATORS Rodney Farrow 1983-01-01 (pdf) PROTOCOL ARCHITECTURE OF A TREE NETWORK WITH COLLISION AVOIDANCE SWITCHES Tatsuya Suda 1983-01-01 (pdf) LOWER BOUNDS ON COMMUNICATION COMPLEXITY Pavol Duris, Zvi Galil, Georg Schnitger 1983-01-01 (pdf) OPTIMAL PARALLEL ALGORITHMS FOR STRING MATCHING Zvi Galil 1983-01-01 (pdf) CONTROLLING WINDOW PROTOCOLS FOR TIME-CONSTRAINED COMMUNICATION IN A MULTIPLE ACCESS ENVIRONMENT James F. Kurose, Mischa Schwartz, Yechiam Yemini 1983-01-01 (pdf) NATURAL LANGUAGE SYSTEMS:  HOW ARE THEY MEETING HUMAN NEEDS? Kathleen R. McKeown 1983-01-01 (pdf) ON THE DESIGN OF PARALLEL PRODUCTION SYSTEM MACHINES: WHAT'S IN A LIP? Salvatore J. Stolfo 1983-01-01 (pdf) NON-VON'S PERFORMANCE ON CERTAIN DATABASE BENCHMARKS Bruce K. Hillyer, David Elliot Shaw 1983-01-01 (pdf) EVOLUTION OF THE NON-VON SUPERCOMPUTER David Elliot Shaw 1983-01-01 (pdf) CAN ADAPTION HELP ON THE AVERAGE? G. W. Wasilkowski, H. Wozniakowski 1983-01-01 (pdf) IS GAUSS QUADRATURE OPTIMAL FOR ANALYTIC FUNCTIONS? M. A. Kowalski, A. G. Werschulz, H. Wozniakowski 1983-01-01 (pdf) THE ACCURATE SOLUTION OF CERTAIN CONTINUOUS PROBLEMS USING ONLY SINGLE PRECISION M. Jankowski, H. Wozniakowski 1983-01-01 (pdf) PERFORMANCE ANALYSIS OF TWO COMPETING DADO PE DESIGNS Daniel P. Miranker 1983-01-01 (pdf) COMPLEXITY OF INDEFINITE ELLIPTIC PROBLEMS Arthur G. Werschulz 1983-01-01 (pdf) USER-ORIENTED EXPLANATION FOR EXPERT SYSTEMS Kathleen McKeown 1983-01-01 (pdf) USING FOCUS TO GENERATE COMPLEX AND SIMPLE SENTENCES Marcia A. Derr, Kathleen R. McKeown 1983-01-01 (pdf) RESEARCH DIRECTIONS IN DISTRIBUTED COMPUTING AND COMMUNICATIONS Yechiam Yemini 1983-01-01 (pdf) A REAL-TIME TRANSPORT PROTOCOL Yechiam Yemini 1983-01-01 (pdf) DISTRIBUTED MULTIPLE ACCESS PROTOCOLS AND REAL-TIME COMMUNICATION James F. Kurose, Yechiam Yemini, Mischa Schwartz 1983-01-01 (pdf) A FAMILY OF WINDOW PROTOCOLS FOR TIME-CONSTRAINED APPLICATIONS IN CSMA NETWORKS James F. Kurose, Mischa Schwartz 1983-01-01 (pdf) ALLOCATION AND MANIPULATION OF RECORDS IN THE NON-VON SUPERCOMPUTER David Elliot Shaw, Bruce K. Hillyer 1983-01-01 (pdf) THE TEXT SYSTEM FOR NATURAL LANGUAGE GENERATION: AN OVERVIEW Kathleen R. McKeown 1982-01-01 (pdf) DADO: A TREE-STRUCTURED MACHINE ARCHITECTURE FOR PRODUCTION SYSTEMS Salvatore J. Stolfo, David Elliot Shaw 1982-01-01 (pdf) TOWARDS DISTRIBUTED SENSORS NETWORKS: AN EXTENDED ABSTRACT Y. Yemini, A. Lazar 1982-01-01 (pdf) TOWARDS THE UNIFICATION OF THE FUNCTIONAL AND PERFORMANCE ANALYSIS OF PROTOCOLS OR, IS THE ALTERNATING-BIT PROTOCOL REALLY CORRECT? Y. Yemini, J. F. Kurose 1982-01-01 (pdf) COMPLEXITY OF LINEAR PROGRAMMING J. F. Traub, H. Wozniakowski 1982-01-01 (pdf) A STATISTICAL MECHANICS OF DISTRIBUTED RESOURCE SHARING MECHANISMS Yechiam Yemini 1982-01-01 (pdf) THE NON-VON SUPER COMPUTER David Elliot Shaw 1982-01-01 (pdf) ON THE OPTIMAL SOLUTION OF LARGE LINEAR SYSTEMS J. F. Traub, H. Wozniakowski 1982-01-01 (pdf) AVERAGE CASE OPTIMALITY FOR LINEAR PROBLEMS J. F. Traub, G. Wasilkowski, H. Wozniakowski 1982-01-01 (pdf) AVERAGE CASE OPTIMAL ALGORITHMS IN HILBERT SPACES G. W. Wasilkowski, H. Wozniakowski 1982-01-01 (pdf) AN EXPERT SYSTEM SUPPORTING ANALYSIS AND MANAGEMENT DECISION MAKING Salvatore J. Stolfo, Gregg T. Vesonder 1982-01-01 (pdf) PROGRAMMING IN THE DADO MACHINE: AN INTRODUCTION TO PPL/M Salvatore J. Stolfo, Daniel Miranker, David Elliot Shaw 1982-01-01 (pdf) MEASURING UNCERTAINTY WITHOUT A NORM Arthur G. Werschulz 1982-01-01 (pdf) STATISTICAL SECURITY OF A STATISTICAL DATA BASE J. F. Traub, H. Wozniakowski, Y. Yemini 1982-01-01 (pdf) REPRESENTING COMPLEX PHYSICAL OBJECTS IN MEMORY Kenneth Wasserman, Michael Lebowitz 1982-01-01 (pdf) DOES INCREASED REGULARITY LOWER COMPLEXITY? Arthur G. Werschulz 1982-01-01 (pdf) AN INFORMATION-THEORETIC SCALE FOR CULTURAL RULE SYSTEMS Jonathan L. Gross 1981-01-01 (pdf) UNDERSTANDING: THE RELATION BETWEEN LANGUAGE AND MEMORY Michael Lebowitz 1981-01-01 (pdf) MEMORY-BASED PARSING Michael Lebowitz 1981-01-01 (pdf) REPRESENTING COMPLEX EVENTS SIMPLY Michael Lebowitz 1981-01-01 (pdf) DOUBLE-EDGE-TRIGGERED FLIP-FLOPS Stephen H. Unger 1981-01-01 (pdf) SPECIALIZED HARDWARE FOR PRODUCTION SYSTEMS Salvatore J. Stolfo, David Elliot Shaw 1981-01-01 (pdf) A HIGHLY PARALLEL VLSI-BASED SUBSYSTEM OF THE NON-VONDTABASE MACHINE\" David Elliot Shaw, Hussein Ibrahim, Gio Widerhold, Jim Andrews 1981-01-01 (pdf) NON-VON: A PARALLEL MACHINE ARCHITECTURE FOR KNOWLEDGE-BASED INFORMATION PROCESSING David Elliot Shaw 1981-01-01 (pdf) ON THE LONG-TERM IMPLICATIONS OF DATABASE MACHINE RESEARCH David Elliot Shaw 1981-01-01 (pdf) SELFISH OPTIMIZATION IN COMPUTER NETWORKS Yechiam Yemini 1981-01-01 (pdf) PARALLEL KNOWLEDGE-BASED INFORMATION RETRIEVAL ON THE NON-VON MACHINE David Elliot Shaw 1981-01-01 (pdf) THE NON-VON DATABASE MACHINE: AN OVERVIEW David Elliot Shaw, Salvatore J. Stolfo, Hussein 1981-01-01 (pdf) SOME PROBLEMS IN TOPOLOGICAL GRAPH THEORY Jonathan L. Gross 1980-01-01 (pdf) COMPUTATIONAL COMPLEXITY J. F. Traub 1980-01-01 (pdf) KNOWLEDGE-BASED RETRIEVAL ON A RATIONAL DATABASE MACHINE David Elliot Shaw 1980-01-01 (pdf) LEARNING META-RULE CONTROL OF PRODUCTION SYSTEMS FROM EXECUTION TRACES Malcolm C. Harrison, Salvatore J. Stolfo 1980-01-01 (pdf) CAN ANY STATIONARY ITERATION USING LINEAR INFORMATION BE GLOBALLY CONVERGENT? G. W. Wasilkowski 1980-01-01 (pdf) THE STRENGTH OF NONSTATIONARY ITERATION G. W. Wasilkowski 1979-01-01 (pdf) A RELATIONAL DATABASE MACHINE ARCHITECTURE David Elliot Shaw 1979-01-01 (pdf) A HIERARCHIAL ASSOCIATIVE ARCHITECTURE FOR THE PARALLEL EVALUATION OF RELATIONAL ALGEBRAIC DATABASE PRIMITIVES David Elliot Shaw 1979-01-01 (pdf) LEARNING CONTROL OF PRODUCTION SYSTEMS Salvatore J. Stolfo 1979-01-01 (pdf) ANY ITERATION FOR POLYNOMIAL EQUATIONS USING LINEAR INFORMATION HAS INFINITE COMPLEXITY G. W. Wasilkowski 1979-01-01 (pdf) STRUCTURE AND ABSTRACTION IN A SYSTEM FOR CONCEPTUAL METHODS David Elliot Shaw 1977-01-01 (pdf) INFERRING LISP PROGRAMS FROM EXAMPLES David Elliot Shaw, William R. Swartout, C. Cordell Green 1975-01-01 (pdf) Columbia University in the City of New York School of Engineering and Applied Science SEAS Undergraduate Admissions SEAS M.S./Ph.D. Application Data Science Institute CS Advising MICE CRF Resources for Faculty and Staff Copyright FAQ Computer Science Department 500 West 120 Street, Room 450 MC0401 New York, New York 10027 Main Office: +1-212-853-8400 Directions Map Directory © Columbia Webmaster Privacy Policy Dean Boyce's statement on amicus brief filed by President Bollinger President Bollinger announced that Columbia University along with many other academic institutions (sixteen, including all Ivy League universities) filed an amicus brief in the U.S. District Court for the Eastern District of New York challenging the Executive Order regarding immigrants from seven designated countries and refugees. Among other things, the brief asserts that “safety and security concerns can be addressed in a manner that is consistent with the values America has always stood for, including the free flow of ideas and people across borders and the welcoming of immigrants to our universities.” This recent action provides a moment for us to collectively reflect on our community within Columbia Engineering and the importance of our commitment to maintaining an open and welcoming community for all students, faculty, researchers and administrative staff. As a School of Engineering and Applied Science, we are fortunate to attract students and faculty from diverse backgrounds, from across the country, and from around the world. It is a great benefit to be able to gather engineers and scientists of so many different perspectives and talents – all with a commitment to learning, a focus on pushing the frontiers of knowledge and discovery, and with a passion for translating our work to impact humanity. I am proud of our community, and wish to take this opportunity to reinforce our collective commitment to maintaining an open and collegial environment. We are fortunate to have the privilege to learn from one another, and to study, work, and live together in such a dynamic and vibrant place as Columbia. Sincerely, Mary C. Boyce Dean of Engineering Morris A. and Alma Schapiro Professor      -   Location:  \n                 Speaker: },  \n                 Series:   }  Bio }         Interests:    }  Courses This Semester   ( -)   } Close"
}